{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f846671c",
   "metadata": {},
   "source": [
    "# Ejercicios acerca del determinante\n",
    "## Desarrollo de Laplace.\n",
    "### Deducir de la definici√≥n 4 el determinante en dimensi√≥n 0, 1 y 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9eec5",
   "metadata": {},
   "source": [
    "#### Dimensi√≥n 0\n",
    "\n",
    "En dimensi√≥n 0, una matriz es simplemente un escalar, y su determinante es el propio escalar:\n",
    "\n",
    "\\begin{equation}\n",
    "det(a) = a\n",
    "\\end{equation}\n",
    "\n",
    "#### Dimensi√≥n 1\n",
    "\n",
    "Una matriz es un solo vector (o escalar). Para un vector $\\mathbb{v_1}$ en $\\mathbb{R}$, el determinante es simplemente el valor absoluto del vector, ya que la √∫nica funci√≥n lineal antisim√©trica es la identidad.\n",
    "\n",
    "\\begin{equation}\n",
    "\\det([v_1]) = v_1\n",
    "\\end{equation}\n",
    "\n",
    "#### Dimensi√≥n 2\n",
    "En dimensi√≥n 2 tenemos una matriz 2x2. Sea $\\mathbb{A}$ una matriz de $\\mathbb{R}^2$:\n",
    "\n",
    "\\begin{equation}\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "La funci√≥n lineal antisimetrica que cumple la definici√≥n 4 seria:\n",
    "\n",
    "\\begin{equation}\n",
    "det(A) =ad-cd\n",
    "\\end{equation}\n",
    "\n",
    "La matriz identidad de una matriz 2x2 es:\n",
    "\n",
    "\\begin{equation}\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Por lo que tenemos que el resultado del determinate sera dado por:\n",
    "\n",
    "\\begin{equation}\n",
    "det(A) = 1 \\cdot 1-0 \\cdot 0 = 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e4d97d",
   "metadata": {},
   "source": [
    "\n",
    "### A partir de la definici√≥n 4, expresar el determinante de una matriz cuadrada recursivamente en funci√≥n de determinantes la matrices cuadradas de dimensi√≥n inferior.\n",
    "Indicaci√≥n: para cada $n ‚àà N$, distribuir (por linealidad en las columnas) sobre la descomposici√≥n\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "\\lambda & \\omega \\\\\n",
    "v & A\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\lambda \\cdot 1 + 0 & \\omega \\\\\n",
    "\\lambda \\cdot 0 + v & A\n",
    "\\end{bmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "de una matriz cuadrada de dimensi√≥n ùëõ + 1, siendo $n ‚àà N$ y \\\n",
    "- $ùúÜ$ un coeficiente real, \\\n",
    "- $ùë£$ un vector de dimensi√≥n ùëõ (una columna de ùëõ coeficientes reales), \\\n",
    "- $ùúî$ un covector de la misma dimensi√≥n (una fila de ùëõ coeficientes), \\\n",
    "- $ùê¥$ una matriz cuadrada de la misma dimensi√≥n (con ùëõ2 coeficientes), \\\n",
    "\n",
    "luego proceder del mismo modo con\\\n",
    "- los dem√°s coeficientes de esa primera columna,\\\n",
    "- con cada columna.\n",
    "\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068ec292",
   "metadata": {},
   "source": [
    "**Resoluci√≥n ejercicio :** \\\n",
    "\n",
    "Diremos que n = 2 por lo que tendremos las siguientes igualdades :\n",
    "\n",
    "\\begin{equation}\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "v =\n",
    "\\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w =\n",
    "\\begin{bmatrix}\n",
    "w_1 &&\n",
    "w_2\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Usaremos una matriz M de dimensi√≥n 3√ó3 como ejemplo concreto donde estaran los valores $ùúÜ, ùë£,  ùúî$ y los valores de A\n",
    "\n",
    "\\begin{equation}\n",
    "M =\n",
    "\\begin{bmatrix}\n",
    "\\lambda & w_1 & w_2 \\\\\n",
    "v_1 & a_{11} & a_{12} \\\\\n",
    "v_2 & a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb5a24-1402-44a2-bed9-eab21c88a280",
   "metadata": {},
   "source": [
    "**Paso 1: Expansi√≥n por Cofactores**\n",
    "\n",
    "Para calcular el determinante de M, vamos a usar cofactores. Esto significa que vamos a descomponer el determinante de la matriz grande en t√©rminos de los determinantes de matrices m√°s peque√±as.\n",
    "\n",
    "\\begin{equation}\n",
    "det(M) =\n",
    "\\lambda \\cdot\n",
    "det\n",
    " \\Biggl(\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix} \n",
    " \\Biggl)\n",
    "-\n",
    "w_1 \\cdot\n",
    "det\n",
    "\\Biggl(\n",
    "\\begin{bmatrix}\n",
    "v_1 & a_{12} \\\\\n",
    "v_2 & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\Biggl)\n",
    "+\n",
    "w_2 \\cdot \n",
    "det\n",
    "\\Biggl(\n",
    "\\begin{bmatrix}\n",
    "v_1 & a_{11} \\\\\n",
    "v_2 & a_{21} \\\\\n",
    "\\end{bmatrix} \n",
    "\\Biggl)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2029d-2bd9-4631-9146-752f2ea03a3c",
   "metadata": {},
   "source": [
    "Entonces tenemos 3 sub matrices :\n",
    "\n",
    "\\begin{equation}\n",
    "M_1 =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "M_2 =\n",
    "\\begin{bmatrix}\n",
    "v_1 & a_{12} \\\\\n",
    "v_2 & a_{22} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "M_3 =\n",
    "\\begin{bmatrix}\n",
    "v_1 & a_{11} \\\\\n",
    "v_2 & a_{21} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4de306-dbc2-4386-a540-3fa12f9744a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Si simplificamos la ecuaci√≥n tendr√≠amos que: \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "det(M) =\n",
    "\\lambda \\cdot\n",
    "det(A)\n",
    "-\n",
    "w_1 \\cdot det(M_2)\n",
    "+\n",
    "w_2 \\cdot det(M_2)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d713debf",
   "metadata": {},
   "source": [
    "Por tanto tenemos que en n:\n",
    "\n",
    "\\begin{equation}\n",
    "M =\n",
    "\\begin{pmatrix}\n",
    "\\lambda & \\omega_1 & \\omega_2 & \\cdots & \\omega_n \\\\\n",
    "v_1 & a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "v_2 & a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_n & a_{n1} & a_{n2} & \\cdots & a_{nn}\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "El determinante esta dado por:\n",
    "\n",
    "\\begin{equation}\n",
    "det(M) =\n",
    "\\lambda \\cdot\n",
    "det(A)\n",
    "-\n",
    "w_1 \\cdot det(M_1)\n",
    "+\n",
    "w_2 \\cdot det(M_2) ... + (-1)^{1+i} \\cdot w_i \\cdot det(A_i)\n",
    "\\end{equation}\n",
    "\n",
    "Que es similar a decir que:\n",
    "\n",
    "\\begin{equation}\n",
    "\\det\\begin{pmatrix}\n",
    "\\lambda & \\omega \\\\\n",
    "v & A\n",
    "\\end{pmatrix}\n",
    "= \\lambda \\cdot \\det(A) \n",
    "+\n",
    "\\sum_{i=1}^n \\omega_i \\cdot (-1)^{1+i} \\cdot \\det(A_i)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar en Python la definici√≥n as√≠ obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62af25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 5\n",
      "lambda: 0\n",
      "v: [ 5 -9 -6  7  0]\n",
      "omega: [-5  6 -6 11  1]\n",
      "A:\n",
      "[[  3   6  13   2  -9]\n",
      " [  1  -4   4   1  -5]\n",
      " [ -7   3   1   8   5]\n",
      " [ -2  14   5   1  -3]\n",
      " [ -2 -11   4  12  13]]\n",
      "Matriz compuesta:\n",
      "[[  0  -5   6  -6  11   1]\n",
      " [  5   3   6  13   2  -9]\n",
      " [ -9   1  -4   4   1  -5]\n",
      " [ -6  -7   3   1   8   5]\n",
      " [  7  -2  14   5   1  -3]\n",
      " [  0  -2 -11   4  12  13]]\n",
      "Cofactores:\n",
      "F√≥rmula simb√≥lica:  0 * det(A) + -5 * (-1)^1 * det(A_1) + 6 * (-1)^2 * det(A_2) + -6 * (-1)^3 * det(A_3) + 11 * (-1)^4 * det(A_4) + 1 * (-1)^5 * det(A_5)\n",
      "F√≥rmula num√©rica:  0 * 690.0000000000016 + 140600.00000000006 + -46212.00000000012 + -131280.0000000005 + -336688.00000000093 + 10678.000000000053\n",
      "Determinante: -362902.00000000076\n",
      "Verificacion: -362902.00000000146\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcular_determinante_combinado(lambda_val, v, omega, A):\n",
    "    # Convertir las entradas a matrices numpy\n",
    "    v = np.array(v).reshape(-1, 1)\n",
    "    omega = np.array(omega).reshape(1, -1)\n",
    "    A = np.array(A)\n",
    "    \n",
    "    # Construir la matriz combinada\n",
    "    primera_fila = np.hstack(([lambda_val], omega[0]))\n",
    "    resto_filas = np.hstack((v, A))\n",
    "    matriz_combinada = np.vstack((primera_fila, resto_filas))\n",
    "    \n",
    "    print(\"Matriz compuesta:\")\n",
    "    print(matriz_combinada)\n",
    "    \n",
    "    # Calcular el determinante de la matriz combinada\n",
    "    determinante_combinado = np.linalg.det(matriz_combinada)\n",
    "    \n",
    "    # Verificaci√≥n del determinante seg√∫n la f√≥rmula\n",
    "    determinante_A = np.linalg.det(A)\n",
    "    suma_terminos = 0\n",
    "    cofactores = []\n",
    "    for i in range(len(omega[0])):\n",
    "        # Crear una submatriz A_i eliminando la primera fila y la columna i\n",
    "        columna_a_eliminar = i + 1;\n",
    "        num_filas, num_columnas = matriz_combinada.shape\n",
    "        A_i = np.delete(matriz_combinada, 0, axis=0)  # Eliminar la primera fila\n",
    "        A_i = np.delete(A_i, columna_a_eliminar, axis=1)  # Eliminar la columna i\n",
    "        #print(f\"(A_{i+1}\")\n",
    "        #print(A_i)\n",
    "        #print(f\"w_{i+1}\", omega[0][i])\n",
    "        cofactor = omega[0][i] * (-1)**(1+i) * np.linalg.det(A_i)\n",
    "        cofactores.append(cofactor)\n",
    "        suma_terminos += cofactor\n",
    "        #print(f\"Cofactor {i+1}: omega[{i}] * (-1)^{1+i} * det(A_{i+1}) = {omega[0][i]} * (-1)^{1+i} * {np.linalg.det(A_i)} = {cofactor}\")\n",
    "    \n",
    "    determinante_verificado = lambda_val * determinante_A + suma_terminos\n",
    "    \n",
    "    # Mostrar cada uno de los cofactores y sus resultados\n",
    "    print(\"Cofactores:\")\n",
    "    \n",
    "    # Mostrar la f√≥rmula completa en n√∫meros\n",
    "    formula = f\"{lambda_val} * det(A) + \" + \" + \".join([f\"{omega[0][i]} * (-1)^{1+i} * det(A_{i+1})\" for i in range(len(omega[0]))])\n",
    "    formula_numerica = f\"{lambda_val} * {determinante_A} + \" + \" + \".join([str(cofactor) for cofactor in cofactores])\n",
    "    \n",
    "    print(\"F√≥rmula simb√≥lica: \", formula)\n",
    "    print(\"F√≥rmula num√©rica: \", formula_numerica)\n",
    "    \n",
    "    return matriz_combinada, determinante_combinado, determinante_verificado\n",
    "\n",
    "# Ejemplo de uso\n",
    "def generate_random_parameters(inf_lim, sup_lim):\n",
    "\n",
    "    # 1. Generar n como un n√∫mero entero aleatorio entre 2 y 5\n",
    "    n = np.random.randint(2, 15)\n",
    "    \n",
    "    # 2. Generar lambda como un n√∫mero entero aleatorio entre -10 y 10\n",
    "    lambda_ = np.random.randint(inf_lim, sup_lim)\n",
    "    \n",
    "    # 3. Generar un vector columna v de dimensi√≥n n con valores enteros\n",
    "    v = np.random.randint(inf_lim, sup_lim, n)\n",
    "    \n",
    "    # 4. Generar un covector omega de dimensi√≥n n con valores enteros\n",
    "    omega = np.random.randint(inf_lim, sup_lim, n)\n",
    "    \n",
    "    # 5. Generar una matriz cuadrada A de dimensi√≥n n x n con valores enteros\n",
    "    A = np.random.randint(inf_lim, sup_lim, (n, n))\n",
    "    \n",
    "    return n, lambda_, v, omega, A\n",
    "\n",
    "\n",
    "\n",
    "n, lambda_, v, omega, A = generate_random_parameters(-11,15)\n",
    "\n",
    "print(\"n:\", n)\n",
    "print(\"lambda:\", lambda_)\n",
    "print(\"v:\", v)\n",
    "print(\"omega:\", omega)\n",
    "print(\"A:\")\n",
    "print(A)\n",
    "\n",
    "matriz_combinada, determinante_combinado, determinante_verificado = calcular_determinante_combinado(lambda_, v, omega, A)\n",
    "\n",
    "print(\"Determinante:\", determinante_combinado)\n",
    "print(\"Verificacion:\", determinante_verificado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 : Eliminaci√≥n de Gauss‚ÄìJordan.\n",
    "\n",
    "### Deducir de la definici√≥n 4 el efecto que tiene en el determinante de una matriz sumar a una de sus columnas una combinaci√≥n lineal de las dem√°s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a78c6e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A partir de la definici√≥n 4, proponer una estrategia para triangularizar una matriz sin cambiar su determinante e implementar en Python una definici√≥n alternativa del determinante. Indicaci√≥n: descomponer similarmente al ejercicio anterior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470d6b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementar en Python la definici√≥n as√≠ obtenida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef93d7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Comparaci√≥n.\n",
    "\n",
    "### Obtener la complejidad computacional de cada una de estas dos implementaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a01fad",
   "metadata": {},
   "source": [
    "- Calcular complejidad computacional ejercicio: Determinante Matriz Combinada \n",
    "   \n",
    "   - Funci√≥n **generate_random_parameters(inf_lim, sup_lim)**:\\\n",
    "      Tiene complejidad computacional **O(n¬≤)** dado que las l√≠neas que generan vectores y matrices (puntos 3, 4 y 5) tienen complejidades O(n) y O(n¬≤), respectivamente, y el resto de las l√≠neas O(1), la complejidad total del c√≥digo est√° dominada por la generaci√≥n de la matriz 'A', es decir **O(n¬≤)**.\n",
    "      \n",
    "\n",
    "   - Funci√≥n **calcular_determinante_combinado(lambda_val, v, omega, A)**:\\\n",
    "      La funci√≥n tiene la siguiente estructura:\n",
    "      - Construcci√≥n de la matriz combinada:\n",
    "      Tiene un costo de O(n), donde n es el tama√±o del vector v y la matriz A (asumiendo que omega tiene la misma longitud que v).\n",
    "      - C√°lculo del determinante de la matriz combinada:\n",
    "      Utilizando np.linalg.det, el costo es O(n^3) en el peor de los casos para matrices cuadradas.\n",
    "      - C√°lculo de los cofactores:\n",
    "      Se realiza un ciclo for de tama√±o n (el tama√±o de omega).\n",
    "      Dentro del ciclo, se calcula el determinante de una submatriz de tama√±o (n-1) x (n-1), con un costo de O((n-1)^3).\n",
    "      \n",
    "      Por lo tanto, el costo de calcular los cofactores es O(n * (n-1)^3), que es equivalente a O(n^4).\n",
    "      Verificaci√≥n del determinante:\n",
    "      Tiene un costo de O(n), ya que se realiza una suma de n t√©rminos.\n",
    "   \n",
    "   >_\"La complejidad computacional total del c√≥digo es O(n^4), lo que significa que el tiempo de ejecuci√≥n crece con la cuarta potencia del tama√±o de la matriz. Este es un tiempo de ejecuci√≥n relativamente alto, especialmente para matrices grandes.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generar matrices aleatoriamente en dimensi√≥n $n ‚àà$ { 2, 3, ¬∑ ¬∑ , 9, 10 } y comparar el tiempo de ejecuci√≥n de cada una de estas dos implementaciones con la funci√≥n numpy.linalg.det (la funci√≥n determinante de la extensi√≥n num√©rica de Python al √°lgebra lineal). Indicaci√≥n: se puede utilizar la funci√≥n numpy.random.rand para generar los coeficientes aleatorios de sus matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557d1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d692ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def det_laplace(matriz: np.ndarray) -> float:\n",
    "  \"\"\"Calcula el determinante de una matriz usando la regla de Laplace.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        matriz: Es la matriz n x n para la cual se calcula el determinante\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        El determinante de la matriz\n",
    "  \"\"\"\n",
    "\n",
    "  n = len(matriz)\n",
    "  \n",
    "  if n == 1:\n",
    "    return matriz[0][0]\n",
    "  elif n == 2:\n",
    "    return matriz[0][0] * matriz[1][1] - matriz[0][1] * matriz[1][0]\n",
    "  else:\n",
    "    det = 0\n",
    "    for j in range(n):\n",
    "      submatriz = np.delete(np.delete(matriz, 0, axis=0), j, axis=1)\n",
    "      det += (-1) ** j * matriz[0][j] * det_laplace(submatriz)\n",
    "    return det\n",
    "\n",
    "\n",
    "def det_gauss_jordan(matriz: np.ndarray) -> float:\n",
    "  \"\"\"Calcula el determinante de una matriz usando el m√©todo de eliminaci√≥n de Gauss-Jordan.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        matriz: Es la matriz n x n para la cual se calcula el determinante\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        El determinante de la matriz\n",
    "  \"\"\"\n",
    "\n",
    "  n = len(matriz)\n",
    "  det = 1\n",
    "\n",
    "  for i in range(n):\n",
    "    pivot = matriz[i][i]\n",
    "    if pivot == 0:\n",
    "      return 0\n",
    "    \n",
    "    det  *= pivot\n",
    "    matriz[i] /= pivot\n",
    "\n",
    "    for j in range(i+1, n):\n",
    "      factor = matriz[j][i]\n",
    "      matriz[j]  -= factor * matriz[i]\n",
    "  return det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a153eb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi√≥n | T.Laplace (ms) | T.Gauss-Jordan (ms) | T.numpy.linalg.det (ms)\n",
      "---------------------------------------------------------------------------\n",
      "2         | 0.009608       | 0.046513            | 0.035703\n",
      "3         | 0.410473       | 0.042401            | 0.024766\n",
      "4         | 0.830461       | 0.063236            | 0.026046\n",
      "5         | 2.359853       | 0.090583            | 0.040796\n",
      "6         | 13.553588      | 0.130150            | 0.036440\n",
      "7         | 80.875877      | 0.162143            | 0.046795\n",
      "8         | 643.808716     | 0.210978            | 0.045501\n",
      "9         | 5897.031685    | 0.237919            | 0.040963\n",
      "10        | 58559.983033   | 0.291078            | 0.039791\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensi√≥n\".ljust(10) + \"| T.Laplace (ms)\".ljust(17) + \"| T.Gauss-Jordan (ms)\".ljust(22) + \"| T.numpy.linalg.det (ms)\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "# Genera matrices aleatorias de tama√±o n x n\n",
    "for n in range(2, 11):\n",
    "  A = np.random.rand(n, n)\n",
    "\n",
    "  # Calcula el tiempo de ejecuci√≥n para cada algoritmo\n",
    "  laplace_time = timeit.timeit(lambda: det_laplace(A), number=1) * 1000\n",
    "  gauss_jordan_time = timeit.timeit(lambda: det_gauss_jordan(A), number=1)  * 1000\n",
    "  numpy_time = timeit.timeit(lambda: np.linalg.det(A), number=1)  * 1000\n",
    "\n",
    "  print(f\"{str(n).ljust(9)} | {format(laplace_time, '.6f').ljust(14)} | {format(gauss_jordan_time, '.6f').ljust(19)} | {format(numpy_time, '.6f')}\")\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios acerca del gradiente\n",
    "\n",
    "## Ejercicio 4 : M√©todo descenso del gradiente\n",
    "Con el prop√≥sito de aproximar un m√≠nimo local de una funci√≥n real de varias variables reales, el m√©todo de descenso de gradiente consiste en iterar una marcha (positivamente) proporcional al (opuesto del) gradiente desde un valor inicial, con la intuici√≥n de ‚Äòseguir el agua‚Äô hasta dar con el valle.\n",
    "\n",
    "### Implementar en Python un algoritmo de descenso del gradiente (con un m√°ximo de m = $10‚Åµ$ iteraciones) a partir de los siguientes argumentos tomados en ese orden:\n",
    "\n",
    "- la funci√≥n f cuyo m√≠nimo local se propone aproximar,\\\n",
    "- el valor inicial x desde el que empieza la marcha,\\\n",
    "- la raz√≥n geom√©trica o coeficiente de proporcionalidad y,\\\n",
    "- el par√°metro de tolerancia z para finalizar cuando el gradiente de la funci√≥n f caiga dentro de esa tolerancia.\\\n",
    "\n",
    "Indicaci√≥n: empezar por implementar el gradiente grad(f) de la funci√≥n f.\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a5bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradiente(f, x, h=1e-8):\n",
    "    \"\"\"\n",
    "    Aproximaci√≥n del gradiente de f en el punto x usando diferencias finitas.\n",
    "\n",
    "    f: La funci√≥n de la cual se va a calcular el gradiente.\n",
    "    x: El punto en el cual se va a calcular el gradiente.\n",
    "    h: Un peque√±o incremento para calcular las diferencias finitas.\n",
    "    return: El gradiente de f en x.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    gradiente = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x0 = np.copy(x)\n",
    "        x1 = np.copy(x)\n",
    "        x1[i] += h\n",
    "        gradiente[i] = (f(x1) - f(x0)) / h\n",
    "    return gradiente\n",
    "\n",
    "def descenso_gradiente(f, x0, tasa_aprendizaje, tolerancia, max_iter=10**5):\n",
    "    \"\"\"\n",
    "    Algoritmo de descenso de gradiente para encontrar el m√≠nimo de una funci√≥n f.\n",
    "\n",
    "    :param f: La funci√≥n cuyo m√≠nimo local se desea encontrar.\n",
    "    :param x0: El punto inicial desde donde comienza la b√∫squeda.\n",
    "    :param tasa_aprendizaje: La raz√≥n geom√©trica o coeficiente de proporcionalidad.\n",
    "    :param tolerancia: El par√°metro de tolerancia para finalizar cuando el gradiente de f est√© dentro de esa tolerancia.\n",
    "    :param max_iter: El n√∫mero m√°ximo de iteraciones.\n",
    "    :return: El punto que minimiza la funci√≥n f.\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    for i in range(max_iter):\n",
    "        grad = gradiente(f, x)\n",
    "        if np.linalg.norm(grad) < tolerancia:\n",
    "            print(f\"Convergi√≥ despu√©s de {i} iteraciones.\")\n",
    "            return x\n",
    "        x = x - tasa_aprendizaje * grad\n",
    "    print(f\"Alcanz√≥ el m√°ximo de iteraciones ({max_iter}) sin convergencia.\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3bca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Punto m√≠nimo encontrado: [10 10]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso:\n",
    "# Definimos una funci√≥n cuadr√°tica simple para demostrar el descenso de gradiente.\n",
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# Punto inicial\n",
    "x0 = np.array([10, 10])\n",
    "# Raz√≥n geom√©trica o coeficiente de proporcionalidad\n",
    "tasa_aprendizaje = 0.1\n",
    "# Tolerancia\n",
    "tolerancia = 1e-6\n",
    "\n",
    "# Ejecutar el descenso de gradiente\n",
    "punto_minimo = descenso_gradiente(f, x0, tasa_aprendizaje, tolerancia)\n",
    "print(\"Punto m√≠nimo encontrado:\", punto_minimo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular formalmente $\\{ ùë° ‚àà R.  ùëì ‚Ä≤(ùë°) = 0 \\}$ para $ùëì : ùë° ‚Ü¶ 3ùë°‚Å¥+4ùë°¬≥‚àí12ùë°¬≤+7$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de12ea8",
   "metadata": {},
   "source": [
    "Para resolver este problema, primero necesitamos calcular la derivada de la funci√≥n $( f(t) = 3t^4 + 4t^3 - 12t^2 + 7 )$ y luego encontrar los puntos donde la derivada es igual a cero. Estos puntos son los candidatos para los m√≠nimos y m√°ximos locales de la funci√≥n.\n",
    "\n",
    "Vamos a proceder paso a paso:\n",
    "\n",
    "1. Calcular la derivada de \\( f(t) \\).\n",
    "2. Encontrar los puntos donde la derivada es igual a cero.\n",
    "3. Utilizar la funci√≥n de descenso de gradiente modificada para encontrar estos puntos.\n",
    "\n",
    "<br>\n",
    "\n",
    "**- Calcular la derivada de $( f(t) )$**\n",
    "\n",
    "La derivada de $( f(t) )$ es:\n",
    "\n",
    "$[ f'(t) = \\frac{d}{dt}(3t^4 + 4t^3 - 12t^2 + 7) = 12t^3 + 12t^2 - 24t ]$\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed87923",
   "metadata": {},
   "source": [
    "**- Encontrar los puntos donde la derivada es igual a cero**\n",
    "\n",
    "Queremos encontrar los puntos $( t )$ donde $( f'(t) = 0 )$. Esto se traduce en resolver la ecuaci√≥n:\n",
    "\n",
    "$[ 12t^3 + 12t^2 - 24t = 0 ]$\n",
    "\n",
    "Podemos factorizar esta ecuaci√≥n:\n",
    "\n",
    "$[ 12t(t^2 + t - 2) = 0 ]$\n",
    "\n",
    "Esta factorizaci√≥n da tres posibles soluciones:\n",
    "\n",
    "$[ 12t = 0 \\quad \\text{o} \\quad t^2 + t - 2 = 0 ]$\n",
    "\n",
    "Resolviendo estas ecuaciones:\n",
    "\n",
    "1. $( 12t = 0 )$ nos da $( t = 0 )$.\n",
    "2. $( t^2 + t - 2 = 0 )$ se puede resolver usando la f√≥rmula cuadr√°tica:\n",
    "\n",
    "$[ t = \\frac{-1 \\pm \\sqrt{1^2 - 4 \\cdot 1 \\cdot (-2)}}{2 \\cdot 1} = \\frac{-1 \\pm \\sqrt{1 + 8}}{2} = \\frac{-1 \\pm 3}{2} ]$\n",
    "\n",
    "Esto nos da dos soluciones:\n",
    "\n",
    "$[ t = 1 \\quad \\text{y} \\quad t = -2 ]$\n",
    "\n",
    "Por lo tanto, los puntos donde $( f'(t) = 0 )$ son $( t = 0 ), ( t = 1 )$, y $( t = -2 )$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700cb0a4",
   "metadata": {},
   "source": [
    "**- Implementaci√≥n en Python de la funci√≥n descenso de gradiente modificada para encontrar estos puntos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93b1540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Ra√≠z encontrada: 0\n",
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Ra√≠z encontrada: 1\n",
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Ra√≠z encontrada: -2\n",
      "Todas las ra√≠ces: [0, 1, -2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Derivada de la funci√≥n f(t)\n",
    "def f_prima(t):\n",
    "    return 12*t**3 + 12*t**2 - 24*t\n",
    "\n",
    "# Descenso de gradiente adaptado para encontrar ra√≠ces de la derivada\n",
    "def descenso_gradiente_raices(f_prima, x0, tasa_aprendizaje, tolerancia, max_iter=10**5):\n",
    "    x = x0\n",
    "    for i in range(max_iter):\n",
    "        gradiente = f_prima(x)\n",
    "        if abs(gradiente) < tolerancia:\n",
    "            print(f\"Convergi√≥ despu√©s de {i} iteraciones.\")\n",
    "            return x\n",
    "        x = x - tasa_aprendizaje * gradiente\n",
    "    print(f\"Alcanz√≥ el m√°ximo de iteraciones ({max_iter}) sin convergencia.\")\n",
    "    return x\n",
    "\n",
    "# Par√°metros para el descenso de gradiente\n",
    "tasa_aprendizaje = 0.01\n",
    "tolerancia = 1e-6\n",
    "\n",
    "# Puntos iniciales para encontrar las ra√≠ces de la derivada\n",
    "puntos_iniciales = [0, 1, -2]\n",
    "\n",
    "# Ejecutar el descenso de gradiente para cada punto inicial\n",
    "raices = []\n",
    "for x0 in puntos_iniciales:\n",
    "    raiz = descenso_gradiente_raices(f_prima, x0, tasa_aprendizaje, tolerancia)\n",
    "    raices.append(raiz)\n",
    "    print(f\"Ra√≠z encontrada: {raiz}\")\n",
    "\n",
    "print(\"Todas las ra√≠ces:\", raices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0007e01",
   "metadata": {},
   "source": [
    "1. **Funci√≥n `f_prima`**: Calcula la derivada de \\( f(t) \\).\n",
    "2. **Funci√≥n `descenso_gradiente_raices`**: Aplica el descenso de gradiente para encontrar las ra√≠ces de la derivada de la funci√≥n.\n",
    "3. **Par√°metros para el descenso de gradiente**: Definimos la tasa de aprendizaje y la tolerancia.\n",
    "4. **Puntos iniciales**: Usamos los puntos \\( t = 0 \\), \\( t = 1 \\), y \\( t = -2 \\) como puntos de partida para verificar las ra√≠ces.\n",
    "5. **Ejecutar el descenso de gradiente**: Para cada punto inicial, ejecutamos el descenso de gradiente y almacenamos los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con una tolerancia $z = 10‚Åª¬π¬≤$ y un valor inicial de $x = 3$ aplicar su algoritmo con raz√≥n $y = 10‚Åª¬π$, $10‚Åª¬≤$, $10‚Åª¬≥$ luego hacer lo mismo con $x = 0$. Interpretar el resultado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daef730",
   "metadata": {},
   "source": [
    "Para abordar este ejercicio, vamos a aplicar el algoritmo de descenso de gradiente a la funci√≥n $( f(t) = 3t^4 + 4t^3 - 12t^2 + 7 )$ utilizando diferentes tasas de aprendizaje $(\\text{learning rates})$ y dos puntos iniciales: $( x = 3 )$ y $( x = 0 )$. Utilizaremos una tolerancia de $( z = 10^{-12} )$.\n",
    "\n",
    "Primero, recordemos la derivada de la funci√≥n:\n",
    "\n",
    "$[ f'(t) = 12t^3 + 12t^2 - 24t. ]$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91dc70",
   "metadata": {},
   "source": [
    "**2.1.3.1 Implementaci√≥n del Algoritmo**\n",
    "\n",
    "Vamos a implementar el descenso de gradiente para encontrar las ra√≠ces de $( f'(t) )$ utilizando las tasas de aprendizaje dadas. Luego, interpretaremos los resultados para $( x = 3 )$ y $( x = 0 )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "819f835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Derivada de la funci√≥n f(t)\n",
    "def f_prime(t):\n",
    "    return 12*t**3 + 12*t**2 - 24*t\n",
    "\n",
    "# Descenso de gradiente adaptado para encontrar ra√≠ces de la derivada\n",
    "def descenso_gradiente_raices(f_prime, x0, tasa_aprendizaje, tolerancia, max_iter=10**5):\n",
    "    x = x0\n",
    "    for i in range(max_iter):\n",
    "        gradiente = f_prime(x)\n",
    "        if abs(gradiente) < tolerancia:\n",
    "            print(f\"Convergi√≥ despu√©s de {i} iteraciones.\")\n",
    "            return x\n",
    "        x = x - tasa_aprendizaje * gradiente\n",
    "        \n",
    "        # Verificaci√≥n de l√≠mites para prevenir overflow\n",
    "        if abs(x) > 1e10:\n",
    "            print(f\"El valor de x = {x} se volvi√≥ demasiado grande en la iteraci√≥n {i}.\")\n",
    "            return None\n",
    "        \n",
    "    print(f\"Alcanz√≥ el m√°ximo de iteraciones ({max_iter}) sin convergencia.\")\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f74626e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**2.1.3.2 Aplicaci√≥n del Algoritmo**\n",
    "\n",
    "- **Valor inicial** $( x = 3 )$\n",
    "- **Tasa de aprendizaje** $( y = 10^{-1} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5329cd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = 3 con tasa de aprendizaje = 0.1\n",
      "El valor de x = -87049951065956.78 se volvi√≥ demasiado grande en la iteraci√≥n 2.\n",
      "No se encontr√≥ una ra√≠z dentro de los l√≠mites permitidos.\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tolerancia = 1e-12\n",
    "punto_inicial = 3\n",
    "tasa_aprendizaje = 0.1\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(f_prime, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "if raiz is not None:\n",
    "    print(f\"Ra√≠z encontrada: {raiz}\\n\")\n",
    "else:\n",
    "    print(\"No se encontr√≥ una ra√≠z dentro de los l√≠mites permitidos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a126d",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "La tasa de aprendizaje $( y = 10^{-1} )$ es demasiado alta. Esto hace que las actualizaciones en $ x $ sean muy grandes, causando que los valores se vuelvan extremadamente grandes en poco tiempo, lo que lleva a un desbordamiento num√©rico. Este es un claro ejemplo de c√≥mo una tasa de aprendizaje demasiado grande puede desestabilizar el algoritmo de descenso de gradiente.\n",
    "<br><br>\n",
    "\n",
    "- **Valor inicial** $( x = 3 )$\n",
    "- **Tasa de aprendizaje** $y=10^{-2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "661a9769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = 3 con tasa de aprendizaje = 0.01\n",
      "Convergi√≥ despu√©s de 31 iteraciones.\n",
      "Ra√≠z encontrada: -1.9999999999999882\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tasa_aprendizaje = 0.01\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(f_prime, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe94848",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "Con una tasa de aprendizaje de $( y = 10^{-2} )$, el algoritmo es m√°s estable y converge r√°pidamente a una ra√≠z cercana a $-2$. Este valor es uno de los puntos donde la derivada de la funci√≥n original es cero, lo que indica un m√≠nimo o m√°ximo local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbac80a",
   "metadata": {},
   "source": [
    "- **Valor inicial** $( x = 3 )$\n",
    "- **Tasa de aprendizaje** $y=10^{-3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b4bb3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = 3 con tasa de aprendizaje = 0.001\n",
      "Convergi√≥ despu√©s de 831 iteraciones.\n",
      "Ra√≠z encontrada: 1.0000000000000275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tasa_aprendizaje = 0.001\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(f_prime, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647622e",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "Con una tasa de aprendizaje a√∫n m√°s peque√±a, $( y = 10^{-3} )$, el algoritmo converge de manera m√°s lenta (requiriendo $831$ iteraciones). Sin embargo, alcanza un punto cercano a $1$, que es otro punto donde la derivada de la funci√≥n original es cero. Esto demuestra que una tasa de aprendizaje m√°s peque√±a puede llevar a una mayor precisi√≥n, aunque a costa de m√°s iteraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748b9de",
   "metadata": {},
   "source": [
    "- **Valor inicial** $x = 0$\n",
    "- **Tasa de aprendizaje** $y = 10^{-2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f5c0a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = 0 con tasa de aprendizaje = 0.1\n",
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Ra√≠z encontrada: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "punto_inicial = 0\n",
    "tasa_aprendizaje = 0.1\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(f_prime, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ae077",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "Al comenzar en $( x = 0 )$, el valor inicial ya es un punto donde la derivada de la funci√≥n es cero. No se necesitan iteraciones adicionales porque 0 es una ra√≠z de la derivada. Esto muestra que el algoritmo detecta correctamente que ya est√° en un punto estacionario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677ac94",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Tasa de aprendizaje** $y=10^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dbe36b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = 0 con tasa de aprendizaje = 0.01\n",
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Ra√≠z encontrada: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tasa_aprendizaje = 0.01\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(f_prime, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b8668",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "Igual que con $( y = 10^{-1} )$, el algoritmo reconoce que el punto inicial $x = 0 $ ya es una ra√≠z de la derivada. No se requieren actualizaciones adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241fd547",
   "metadata": {},
   "source": [
    "**Tasa de aprendizaje** $y=10^{-3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e800de37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = 0 con tasa de aprendizaje = 0.001\n",
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Ra√≠z encontrada: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tasa_aprendizaje = 0.001\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(f_prime, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a5520",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "Nuevamente, al comenzar en $x = 0$, el valor inicial ya es una ra√≠z de la derivada. La tasa de aprendizaje no afecta el resultado en este caso porque no se necesitan iteraciones adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f97a538",
   "metadata": {},
   "source": [
    "**2.1.3.3 Conclusi√≥n General**\n",
    "\n",
    "- **Tasa de aprendizaje grande**: Puede llevar a desbordamientos num√©ricos o a oscilaciones alrededor de la ra√≠z, como se observa con $( y = 10^{-1} )$ cuando el valor inicial es $( x = 3 )$.\n",
    "- **Tasa de aprendizaje moderada**: Proporciona un equilibrio entre velocidad y estabilidad, permitiendo una convergencia r√°pida y precisa, como se observa con $( y = 10^{-2} )$ cuando el valor inicial es $( x = 3 )$.\n",
    "- **Tasa de aprendizaje peque√±a**: Garantiza una alta precisi√≥n, aunque a costa de un mayor n√∫mero de iteraciones, como se observa con $( y = 10^{-3} )$ cuando el valor inicial es $( x = 3 )$.\n",
    "- **Valor inicial en la ra√≠z**: Si el valor inicial es ya una ra√≠z como $( x = 0 )$, el algoritmo converge instant√°neamente sin necesidad de iteraciones adicionales.\n",
    "\n",
    "Estos resultados ilustran c√≥mo la elecci√≥n de la tasa de aprendizaje y el punto inicial pueden influir significativamente en el comportamiento y eficiencia del algoritmo de descenso de gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Repetir estos dos √∫ltimos apartados con $ùëì : (ùë†, ùë°) ‚Ü¶ ùë†¬≤ + 3ùë†ùë° + ùë°¬≥ + 1$ y los valores iniciales x = [-1,1], [0,0].](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d728aa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**2.1.4.1: Calcular formalmente ${ (s, t) ‚àà R^{2} | f(s, t) = 0 }$**\n",
    "<br>\n",
    "\n",
    "La funci√≥n dada es:\n",
    "$[ f(s, t) = s^2 + 3st + t^3 + 1 ]$\n",
    "\n",
    "El gradiente de $( f(s, t) )$ es:\n",
    "$[ \\nabla f(s, t) = \\left( \\frac{\\partial f}{\\partial s}, \\frac{\\partial f}{\\partial t} \\right) ]$\n",
    "\n",
    "Calculando las derivadas parciales:\n",
    "\n",
    "$[ \\frac{\\partial f}{\\partial s} = 2s + 3t ]$\n",
    "$[ \\frac{\\partial f}{\\partial t} = 3s + 3t^2 ]$\n",
    "\n",
    "Queremos encontrar los puntos donde el gradiente es cero:\n",
    "\n",
    "$[ 2s + 3t = 0 ]$\n",
    "\n",
    "$[ 3s + 3t^2 = 0 ]$\n",
    "\n",
    "Resolviendo estas ecuaciones simult√°neamente:\n",
    "\n",
    "1. De la primera ecuaci√≥n:\n",
    "$[ 2s + 3t = 0 \\implies s = -\\frac{3}{2}t ]$\n",
    "\n",
    "2. Sustituyendo $( s = -\\frac{3}{2}t )$ en la segunda ecuaci√≥n:\n",
    "\n",
    "- $[ 3\\left(-\\frac{3}{2}t\\right) + 3t^2 = 0 ]$\n",
    "- $[ -\\frac{9}{2}t + 3t^2 = 0 ]$\n",
    "- $[ t(3t - \\frac{9}{2}) = 0 ]$\n",
    "- $[ t = 0 \\text{ o } t = \\frac{3}{2} ]$\n",
    "\n",
    "<br>\n",
    "\n",
    "Para $( t = 0 )$:\n",
    "$[ s = 0 ]$\n",
    "\n",
    "Para $( t = \\frac{3}{2} )$:\n",
    "$[ s = -\\frac{3}{2} \\left(\\frac{3}{2}\\right) = -\\frac{9}{4} ]$\n",
    "\n",
    "<br>\n",
    "\n",
    "Entonces, los puntos cr√≠ticos son:\n",
    "$[ (0, 0) ]$\n",
    "$[ \\left(-\\frac{9}{4}, \\frac{3}{2}\\right) ]$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08121ab",
   "metadata": {},
   "source": [
    "**2.1.4.2 Aplicar el algoritmo de descenso de gradiente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d147bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gradiente de la funci√≥n f(s, t)\n",
    "def gradiente_f(st):\n",
    "    s, t = st\n",
    "    df_ds = 2 * s + 3 * t\n",
    "    df_dt = 3 * s + 3 * t**2\n",
    "    return np.array([df_ds, df_dt])\n",
    "\n",
    "# Descenso de gradiente adaptado para encontrar ra√≠ces del gradiente\n",
    "def descenso_gradiente_raices(grad_f, x0, tasa_aprendizaje, tolerancia, max_iter=10**5):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        if np.linalg.norm(grad) < tolerancia:\n",
    "            print(f\"Convergi√≥ despu√©s de {i} iteraciones.\")\n",
    "            return x\n",
    "        x = x - tasa_aprendizaje * grad\n",
    "    print(f\"Alcanz√≥ el m√°ximo de iteraciones ({max_iter}) sin convergencia.\")\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c3b2a",
   "metadata": {},
   "source": [
    "- **Valor inicial $[-1, 1]$**\n",
    "- **Tasa de aprendizaje $( y = 10^{-1} )$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1819ea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.1\n",
      "Convergi√≥ despu√©s de 302 iteraciones.\n",
      "Ra√≠z encontrada: [-2.25  1.5 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tolerancia = 1e-12\n",
    "punto_inicial = [-1, 1]\n",
    "tasa_aprendizaje = 0.1\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(gradiente_f, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81801f",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "La tasa de aprendizaje de $0.1$ es moderada, permitiendo que el algoritmo converja razonablemente r√°pido a la ra√≠z $([-2.25, 1.5])$. Este punto es una de las soluciones donde el gradiente de la funci√≥n es cero. El n√∫mero de iteraciones es relativamente bajo, lo que indica una buena convergencia con esta tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ce262",
   "metadata": {},
   "source": [
    "- **Valor inicial $[-1, 1]$**\n",
    "- **Tasa de aprendizaje $( y = 10^{-2} )$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd50f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.01\n",
      "Convergi√≥ despu√©s de 3139 iteraciones.\n",
      "Ra√≠z encontrada: [-2.25  1.5 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tasa_aprendizaje = 0.01\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(gradiente_f, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b86d0",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "Con una tasa de aprendizaje de $0.01$, el algoritmo converge m√°s lentamente que con una tasa de 0.1, pero a√∫n llega a la misma ra√≠z $([-2.25, 1.5])$. El n√∫mero de iteraciones es significativamente mayor debido a la menor tasa de aprendizaje, lo que resulta en pasos m√°s peque√±os hacia la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7ab41",
   "metadata": {},
   "source": [
    "- **Valor inicial $[-1, 1]$**\n",
    "- **Tasa de aprendizaje** $( y = 10^{-3} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96ee4212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.001\n",
      "Convergi√≥ despu√©s de 31558 iteraciones.\n",
      "Ra√≠z encontrada: [-2.25  1.5 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tasa_aprendizaje = 0.001\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(gradiente_f, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6437f0",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "Con una tasa de aprendizaje a√∫n m√°s peque√±a de $0.001$, el algoritmo requiere muchas m√°s iteraciones ($31,559$) para converger a la misma ra√≠z $([-2.25, 1.5])$. Esto demuestra que una tasa de aprendizaje muy baja resulta en una convergencia muy lenta, aunque sigue siendo precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1493c0f",
   "metadata": {},
   "source": [
    "- **Valor inicial** $([0, 0])$\n",
    "- **Tasa de aprendizaje** $( y = 10^{-1} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38e109bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = [0, 0] con tasa de aprendizaje = 0.1\n",
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Ra√≠z encontrada: [0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "punto_inicial = [0, 0]\n",
    "tasa_aprendizaje = 0.1\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(gradiente_f, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042f2ab",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "El valor inicial $([0, 0])$ ya es un punto donde el gradiente de la funci√≥n es cero. Por lo tanto, el algoritmo no necesita realizar ninguna iteraci√≥n adicional para encontrar la ra√≠z. Esto muestra que el punto inicial ya es una soluci√≥n, independientemente de la tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f5fc0",
   "metadata": {},
   "source": [
    "- **Valor inicial** $([0, 0])$\n",
    "- **Tasa de aprendizaje** $( y = 10^{-2} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "887a9426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = [0, 0] con tasa de aprendizaje = 0.01\n",
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Ra√≠z encontrada: [0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tasa_aprendizaje = 0.01\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(gradiente_f, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9c8d6",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "Igual que con la tasa de $0.1$, el algoritmo reconoce que el punto inicial $[0, 0])$ ya es una ra√≠z de la funci√≥n, por lo que no se requieren iteraciones adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f4f816",
   "metadata": {},
   "source": [
    "- **Valor inicial** $([0, 0])$\n",
    "- **Tasa de aprendizaje** $( y = 10^{-3} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66d15c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando en x = [0, 0] con tasa de aprendizaje = 0.001\n",
      "Convergi√≥ despu√©s de 0 iteraciones.\n",
      "Ra√≠z encontrada: [0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros\n",
    "tasa_aprendizaje = 0.001\n",
    "\n",
    "print(f\"Comenzando en x = {punto_inicial} con tasa de aprendizaje = {tasa_aprendizaje}\")\n",
    "raiz = descenso_gradiente_raices(gradiente_f, punto_inicial, tasa_aprendizaje, tolerancia)\n",
    "print(f\"Ra√≠z encontrada: {raiz}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e9d61",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n**:\n",
    "\n",
    "Al igual que con las tasas de aprendizaje mayores, el punto inicial $([0, 0])$ ya es una ra√≠z, y el algoritmo no necesita realizar iteraciones adicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac13868",
   "metadata": {},
   "source": [
    "**2.1.4.3 Conclusi√≥n General**\n",
    "\n",
    "- **Tasa de aprendizaje y valor inicial $[-1, 1]$**:\n",
    "  - Una tasa de aprendizaje mayor (0.1) permite una convergencia m√°s r√°pida con menos iteraciones.\n",
    "  - Una tasa de aprendizaje moderada (0.01) resulta en una convergencia m√°s lenta pero estable.\n",
    "  - Una tasa de aprendizaje muy peque√±a (0.001) lleva a una convergencia extremadamente lenta, aunque precisa.\n",
    "  - Todos convergen a la misma ra√≠z \\([-2.25, 1.5]\\), que es un punto cr√≠tico de la funci√≥n.\n",
    "\n",
    "- **Tasa de aprendizaje y valor inicial $[0, 0]$**:\n",
    "  - El valor inicial \\([0, 0]\\) ya es un punto cr√≠tico donde el gradiente es cero.\n",
    "  - Independientemente de la tasa de aprendizaje, el algoritmo reconoce inmediatamente que est√° en la ra√≠z y no realiza iteraciones adicionales.\n",
    "  - Esto muestra que cuando el punto inicial es ya un punto cr√≠tico, la tasa de aprendizaje no influye en el resultado.\n",
    "\n",
    "Estos resultados ilustran c√≥mo la elecci√≥n de la tasa de aprendizaje afecta la velocidad de convergencia del algoritmo de descenso de gradiente y c√≥mo los puntos cr√≠ticos iniciales pueden simplificar la convergencia."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Jonathan Mora"
   },
   {
    "name": "Luis Jama Tello"
   },
   {
    "name": "Blanca Santos Fern√°ndez"
   },
   {
    "name": "Laura Betancourt Leal"
   }
  ],
  "date": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "title": ""
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
