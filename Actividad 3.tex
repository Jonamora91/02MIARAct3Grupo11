\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    %\let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    \usepackage{tikz}

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    % Crear la portada sin aÃ±adir una pÃ¡gina en blanco adicional


% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=black,
      citecolor=citecolor,
      }
    
    % Slightly bigger margins than the latex defaults    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}

    \renewcommand{\contentsname}{Contenido}
\begin{document}
    % Portada
    \begin{titlepage}
        \centering
        \newgeometry{margin=0cm}
        % Ajustar la imagen para que ocupe todo el ancho y alto de la pÃ¡gina
        \begin{tikzpicture}[remember picture,overlay]
            \node at (current page.center) {\includegraphics[width=\paperwidth,height=\paperheight,keepaspectratio]{resources/02MIAR_Actividad3.png}};
        \end{tikzpicture}

    \end{titlepage}

    % Generar tabla de contenido
    \tableofcontents
    \clearpage

    % NumeraciÃ³n de pÃ¡ginas en arÃ¡bigo
    \pagenumbering{arabic}
    \setcounter{page}{1} 

    % Comienzo del contenido
    \section{Ejercicios acerca del
determinante}\label{ejercicios-acerca-del-determinante}

\subsection{Desarrollo de Laplace.}\label{desarrollo-de-laplace.}

\subsubsection{Deducir de la definiciÃ³n 4 el determinante en dimensiÃ³n
0, 1 y
2.}\label{deducir-de-la-definiciuxf3n-4-el-determinante-en-dimensiuxf3n-0-1-y-2.}

    \paragraph{DimensiÃ³n 0}\label{dimensiuxf3n-0}

En dimensiÃ³n 0, una matriz es simplemente un escalar, y su determinante
es el propio escalar:

\begin{equation}
det(a) = a
\end{equation}

\paragraph{DimensiÃ³n 1}\label{dimensiuxf3n-1}

Una matriz es un solo vector (o escalar). Para un vector
\(\mathbb{v_1}\) en \(\mathbb{R}\), el determinante es simplemente el
valor absoluto del vector, ya que la Ãºnica funciÃ³n lineal antisimÃ©trica
es la identidad.

\begin{equation}
\det([v_1]) = v_1
\end{equation}

\paragraph{DimensiÃ³n 2}\label{dimensiuxf3n-2}

En dimensiÃ³n 2 tenemos una matriz 2x2. Sea \(\mathbb{A}\) una matriz de
\(\mathbb{R}^2\):

\begin{equation}
A =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\end{equation}

La funciÃ³n lineal antisimetrica que cumple la definiciÃ³n 4 seria:

\begin{equation}
det(A) =ad-cd
\end{equation}

La matriz identidad de una matriz 2x2 es:

\begin{equation}
A =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\end{equation}

Por lo que tenemos que el resultado del determinate sera dado por:

\begin{equation}
det(A) = 1 \cdot 1-0 \cdot 0 = 1
\end{equation}

    \subsubsection{A partir de la definiciÃ³n 4, expresar el determinante de
una matriz cuadrada recursivamente en funciÃ³n de determinantes la
matrices cuadradas de dimensiÃ³n
inferior.}\label{a-partir-de-la-definiciuxf3n-4-expresar-el-determinante-de-una-matriz-cuadrada-recursivamente-en-funciuxf3n-de-determinantes-la-matrices-cuadradas-de-dimensiuxf3n-inferior.}

IndicaciÃ³n: para cada \(n â N\), distribuir (por linealidad en las
columnas) sobre la descomposiciÃ³n

\begin{equation}
\begin{bmatrix}
\lambda & \omega \\
v & A
\end{bmatrix} = \begin{bmatrix}
\lambda \cdot 1 + 0 & \omega \\
\lambda \cdot 0 + v & A
\end{bmatrix}.
\end{equation}

de una matriz cuadrada de dimensiÃ³n ğ + 1, siendo \(n â N\) y\\
- \(ğ\) un coeficiente real,\\
- \(ğ£\) un vector de dimensiÃ³n ğ (una columna de ğ coeficientes
reales),\\
- \(ğ\) un covector de la misma dimensiÃ³n (una fila de ğ
coeficientes),\\
- \(ğ´\) una matriz cuadrada de la misma dimensiÃ³n (con ğ2
coeficientes),\\
luego proceder del mismo modo con\\
- los demÃ¡s coeficientes de esa primera columna,\\
- con cada columna.\\

    \textbf{ResoluciÃ³n ejercicio :}\\
Diremos que n = 2 por lo que tendremos las siguientes igualdades :

\begin{equation}
A =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix}
\end{equation}

\begin{equation}
v =
\begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}
\end{equation}

\begin{equation}
w =
\begin{bmatrix}
w_1 &&
w_2
\end{bmatrix}
\end{equation}

Usaremos una matriz M de dimensiÃ³n 3Ã3 como ejemplo concreto donde
estaran los valores \(ğ, ğ£, ğ\) y los valores de A

\begin{equation}
M =
\begin{bmatrix}
\lambda & w_1 & w_2 \\
v_1 & a_{11} & a_{12} \\
v_2 & a_{21} & a_{22} \\
\end{bmatrix}
\end{equation}

    \textbf{Paso 1: ExpansiÃ³n por Cofactores}

Para calcular el determinante de M, vamos a usar cofactores. Esto
significa que vamos a descomponer el determinante de la matriz grande en
tÃ©rminos de los determinantes de matrices mÃ¡s pequeÃ±as.

\begin{equation}
det(M) =
\lambda \cdot
det
 \Biggl(
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix} 
 \Biggl)
-
w_1 \cdot
det
\Biggl(
\begin{bmatrix}
v_1 & a_{12} \\
v_2 & a_{22} \\
\end{bmatrix}
\Biggl)
+
w_2 \cdot 
det
\Biggl(
\begin{bmatrix}
v_1 & a_{11} \\
v_2 & a_{21} \\
\end{bmatrix} 
\Biggl)
\end{equation}

    Entonces tenemos 3 sub matrices :

\begin{equation}
M_1 =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix}
\end{equation}

\begin{equation}
M_2 =
\begin{bmatrix}
v_1 & a_{12} \\
v_2 & a_{22} \\
\end{bmatrix}
\end{equation}

\begin{equation}
M_3 =
\begin{bmatrix}
v_1 & a_{11} \\
v_2 & a_{21} \\
\end{bmatrix}
\end{equation}

    Si simplificamos la ecuaciÃ³n tendrÃ­amos que:

\begin{equation}
det(M) =
\lambda \cdot
det(A)
-
w_1 \cdot det(M_2)
+
w_2 \cdot det(M_2)
\end{equation}

    Por tanto tenemos que en n:

\begin{equation}
M =
\begin{pmatrix}
\lambda & \omega_1 & \omega_2 & \cdots & \omega_n \\
v_1 & a_{11} & a_{12} & \cdots & a_{1n} \\
v_2 & a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
v_n & a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}
\end{equation}

El determinante esta dado por:

\begin{equation}
det(M) =
\lambda \cdot
det(A)
-
w_1 \cdot det(M_1)
+
w_2 \cdot det(M_2) ... + (-1)^{1+i} \cdot w_i \cdot det(A_i)
\end{equation}

Que es similar a decir que:

\begin{equation}
\det\begin{pmatrix}
\lambda & \omega \\
v & A
\end{pmatrix}
= \lambda \cdot \det(A) 
+
\sum_{i=1}^n \omega_i \cdot (-1)^{1+i} \cdot \det(A_i)
\end{equation}

    \subsubsection{Implementar en Python la definiciÃ³n asÃ­
obtenida.}\label{implementar-en-python-la-definiciuxf3n-asuxed-obtenida.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k}{def} \PY{n+nf}{calcular\PYZus{}determinante\PYZus{}combinado}\PY{p}{(}\PY{n}{lambda\PYZus{}val}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Convertir las entradas a matrices numpy}
    \PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{v}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{omega} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{omega}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{A}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Construir la matriz combinada}
    \PY{n}{primera\PYZus{}fila} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{p}{[}\PY{n}{lambda\PYZus{}val}\PY{p}{]}\PY{p}{,} \PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    \PY{n}{resto\PYZus{}filas} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{v}\PY{p}{,} \PY{n}{A}\PY{p}{)}\PY{p}{)}
    \PY{n}{matriz\PYZus{}combinada} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{(}\PY{n}{primera\PYZus{}fila}\PY{p}{,} \PY{n}{resto\PYZus{}filas}\PY{p}{)}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Matriz compuesta:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{matriz\PYZus{}combinada}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Calcular el determinante de la matriz combinada}
    \PY{n}{determinante\PYZus{}combinado} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{matriz\PYZus{}combinada}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} VerificaciÃ³n del determinante segÃºn la fÃ³rmula}
    \PY{n}{determinante\PYZus{}A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A}\PY{p}{)}
    \PY{n}{suma\PYZus{}terminos} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{cofactores} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Crear una submatriz A\PYZus{}i eliminando la primera fila y la columna i}
        \PY{n}{columna\PYZus{}a\PYZus{}eliminar} \PY{o}{=} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{;}
        \PY{n}{num\PYZus{}filas}\PY{p}{,} \PY{n}{num\PYZus{}columnas} \PY{o}{=} \PY{n}{matriz\PYZus{}combinada}\PY{o}{.}\PY{n}{shape}
        \PY{n}{A\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{matriz\PYZus{}combinada}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Eliminar la primera fila}
        \PY{n}{A\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{A\PYZus{}i}\PY{p}{,} \PY{n}{columna\PYZus{}a\PYZus{}eliminar}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Eliminar la columna i}
        \PY{c+c1}{\PYZsh{}print(f\PYZdq{}(A\PYZus{}\PYZob{}i+1\PYZcb{}\PYZdq{})}
        \PY{c+c1}{\PYZsh{}print(A\PYZus{}i)}
        \PY{c+c1}{\PYZsh{}print(f\PYZdq{}w\PYZus{}\PYZob{}i+1\PYZcb{}\PYZdq{}, omega[0][i])}
        \PY{n}{cofactor} \PY{o}{=} \PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{i}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A\PYZus{}i}\PY{p}{)}
        \PY{n}{cofactores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cofactor}\PY{p}{)}
        \PY{n}{suma\PYZus{}terminos} \PY{o}{+}\PY{o}{=} \PY{n}{cofactor}
        \PY{c+c1}{\PYZsh{}print(f\PYZdq{}Cofactor \PYZob{}i+1\PYZcb{}: omega[\PYZob{}i\PYZcb{}] * (\PYZhy{}1)\PYZca{}\PYZob{}1+i\PYZcb{} * det(A\PYZus{}\PYZob{}i+1\PYZcb{}) = \PYZob{}omega[0][i]\PYZcb{} * (\PYZhy{}1)\PYZca{}\PYZob{}1+i\PYZcb{} * \PYZob{}np.linalg.det(A\PYZus{}i)\PYZcb{} = \PYZob{}cofactor\PYZcb{}\PYZdq{})}
    
    \PY{n}{determinante\PYZus{}verificado} \PY{o}{=} \PY{n}{lambda\PYZus{}val} \PY{o}{*} \PY{n}{determinante\PYZus{}A} \PY{o}{+} \PY{n}{suma\PYZus{}terminos}
    
    \PY{c+c1}{\PYZsh{} Mostrar cada uno de los cofactores y sus resultados}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cofactores:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Mostrar la fÃ³rmula completa en nÃºmeros}
    \PY{n}{formula} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{lambda\PYZus{}val}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * det(A) + }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * (\PYZhy{}1)\PYZca{}}\PY{l+s+si}{\PYZob{}}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * det(A\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
    \PY{n}{formula\PYZus{}numerica} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{lambda\PYZus{}val}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * }\PY{l+s+si}{\PYZob{}}\PY{n}{determinante\PYZus{}A}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{n+nb}{str}\PY{p}{(}\PY{n}{cofactor}\PY{p}{)} \PY{k}{for} \PY{n}{cofactor} \PY{o+ow}{in} \PY{n}{cofactores}\PY{p}{]}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FÃ³rmula simbÃ³lica: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{formula}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{FÃ³rmula numÃ©rica: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{formula\PYZus{}numerica}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{matriz\PYZus{}combinada}\PY{p}{,} \PY{n}{determinante\PYZus{}combinado}\PY{p}{,} \PY{n}{determinante\PYZus{}verificado}

\PY{c+c1}{\PYZsh{} Ejemplo de uso}
\PY{k}{def} \PY{n+nf}{generate\PYZus{}random\PYZus{}parameters}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{)}\PY{p}{:}

    \PY{c+c1}{\PYZsh{} 1. Generar n como un nÃºmero entero aleatorio entre 2 y 5}
    \PY{n}{n} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 2. Generar lambda como un nÃºmero entero aleatorio entre \PYZhy{}10 y 10}
    \PY{n}{lambda\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 3. Generar un vector columna v de dimensiÃ³n n con valores enteros}
    \PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{,} \PY{n}{n}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 4. Generar un covector omega de dimensiÃ³n n con valores enteros}
    \PY{n}{omega} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{,} \PY{n}{n}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 5. Generar una matriz cuadrada A de dimensiÃ³n n x n con valores enteros}
    \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{,} \PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{n}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A}

\PY{n}{n}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A} \PY{o}{=} \PY{n}{generate\PYZus{}random\PYZus{}parameters}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{11}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{n}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lambda:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{v:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{v}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{omega:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{omega}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{A}\PY{p}{)}

\PY{n}{matriz\PYZus{}combinada}\PY{p}{,} \PY{n}{determinante\PYZus{}combinado}\PY{p}{,} \PY{n}{determinante\PYZus{}verificado} \PY{o}{=} \PY{n}{calcular\PYZus{}determinante\PYZus{}combinado}\PY{p}{(}\PY{n}{lambda\PYZus{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Determinante:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{determinante\PYZus{}combinado}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Verificacion:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{determinante\PYZus{}verificado}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
n: 5
lambda: 0
v: [ 5 -9 -6  7  0]
omega: [-5  6 -6 11  1]
A:
[[  3   6  13   2  -9]
 [  1  -4   4   1  -5]
 [ -7   3   1   8   5]
 [ -2  14   5   1  -3]
 [ -2 -11   4  12  13]]
Matriz compuesta:
[[  0  -5   6  -6  11   1]
 [  5   3   6  13   2  -9]
 [ -9   1  -4   4   1  -5]
 [ -6  -7   3   1   8   5]
 [  7  -2  14   5   1  -3]
 [  0  -2 -11   4  12  13]]
Cofactores:
FÃ³rmula simbÃ³lica:  0 * det(A) + -5 * (-1)\^{}1 * det(A\_1) + 6 * (-1)\^{}2 * det(A\_2)
+ -6 * (-1)\^{}3 * det(A\_3) + 11 * (-1)\^{}4 * det(A\_4) + 1 * (-1)\^{}5 * det(A\_5)
FÃ³rmula numÃ©rica:  0 * 690.0000000000016 + 140600.00000000006 +
-46212.00000000012 + -131280.0000000005 + -336688.00000000093 +
10678.000000000053
Determinante: -362902.00000000076
Verificacion: -362902.00000000146
    \end{Verbatim}

    \subsection{Ejercicio 2 : EliminaciÃ³n de
Gauss--Jordan.}\label{ejercicio-2-eliminaciuxf3n-de-gaussjordan.}

\subsubsection{Deducir de la definiciÃ³n 4 el efecto que tiene en el
determinante de una matriz sumar a una de sus columnas una combinaciÃ³n
lineal de las
demÃ¡s.}\label{deducir-de-la-definiciuxf3n-4-el-efecto-que-tiene-en-el-determinante-de-una-matriz-sumar-a-una-de-sus-columnas-una-combinaciuxf3n-lineal-de-las-demuxe1s.}

    Para la siguiente deducciÃ³n, tomamos una matriz A genÃ©rica de dimensiÃ³n
3x3:

    \begin{equation}
A =
    \begin{pmatrix}
        a_{11} & a_{12} & a_{13}\\
        b_{11} & b_{12} & b_{13}\\
        c_{11} & c_{12} & c_{13}
    \end{pmatrix}
\end{equation}

    Calculamos el determinante de A aplicando la regla de Sarrus:

    det(A) = \(\bigl|\begin{matrix}A\end{matrix}\bigr|\) =
\(\bigl(\begin{matrix}a_{11} \cdot b_{12} \cdot c_{13}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}a_{12} \cdot b_{13} \cdot c_{11}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}a_{13} \cdot b_{11} \cdot c_{12}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)

    Ahora construimos una matriz B sumando a la tercera lÃ­nea una
combinaciÃ³n lineal de las otras dos:

    \begin{equation}
B =
    \begin{pmatrix}
        a_{11} & a_{12} & a_{13}\\
        b_{11} & b_{12} & b_{13}\\
        2a_{11}-b_{11}+c_{11} & 2a_{12}-b_{12}+c_{12} & 2a_{13}-b_{13}+c_{13}
    \end{pmatrix}
\end{equation}

    El determinante de B aplicando la regla de Sarrus es:

    det(B) = \(\bigl|\begin{matrix}B\end{matrix}\bigr|\) =
\(\bigl[\begin{matrix}a_{11} \cdot b_{12} \cdot (2a_{13}-b_{13}+c_{13})\end{matrix}\bigr]\)+\(\bigl[\begin{matrix}a_{12} \cdot b_{13} \cdot (2a_{11}-b_{11}+c_{11})\end{matrix}\bigr]\)+\(\bigl[\begin{matrix}a_{13} \cdot b_{11} \cdot (2a_{12}-b_{12}+c_{12})\end{matrix}\bigr]\)-\(\bigl[\begin{matrix}(2a_{11}-b_{11}+c_{11}) \cdot b_{12} \cdot a_{13}\end{matrix}\bigr]\)-\(\bigl[\begin{matrix}(2a_{12}-b_{12}+c_{12}) \cdot b_{13} \cdot a_{11}\end{matrix}\bigr]\)-\(\bigl[\begin{matrix}(2a_{13}-b_{13}+c_{13}) \cdot b_{11} \cdot a_{12}\end{matrix}\bigr]\)

    Operando queda:

    \(\bigl|\begin{matrix}B\end{matrix}\bigr|\) =
\(\bigl(\begin{matrix}a_{11} & b_{12} \cdot 2a_{13}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}a_{11} \cdot b_{12} \cdot b_{13}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}a_{11} \cdot b_{12} \cdot c_{13}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}a_{12} \cdot b_{13} \cdot 2a_{11}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}a_{12} \cdot b_{13} \cdot b_{11}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}a_{12} \cdot b_{13} \cdot c_{11}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}a_{13} \cdot b_{11} \cdot 2a_{12}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}a_{13} \cdot b_{11} \cdot b_{12}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}a_{13} \cdot b_{11} \cdot c_{12}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}2a_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}b_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}2a_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}b_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}c_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}2a_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}b_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)

    A continuaciÃ³n agrupamos los tÃ©rminos:

    \(\bigl|\begin{matrix}B\end{matrix}\bigr|\) =
\(\bigl(\begin{matrix}2a_{13} \cdot a_{11} \cdot b_{12}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}2a_{11} \cdot a_{12} \cdot b_{13}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}2a_{12} \cdot a_{13} \cdot b_{11}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}2a_{11} \cdot a_{13} \cdot b_{12}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}2a_{12} \cdot a_{11} \cdot b_{13}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}2a_{13} \cdot a_{12} \cdot b_{11}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}b_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}b_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}b_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}b_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}b_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}b_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}a_{11} \cdot b_{12} \cdot c_{13}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}a_{12} \cdot b_{13} \cdot c_{11}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}a_{13} \cdot b_{11} \cdot c_{12}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}c_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)

    \(= 2 \cdot \bigl[\)
\(\bigl(\begin{matrix}a_{13} \cdot a_{11} \cdot b_{12}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}a_{11} \cdot a_{12} \cdot b_{13}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}a_{12} \cdot a_{13} \cdot b_{11}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}a_{11} \cdot a_{13} \cdot b_{12}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}a_{12} \cdot a_{11} \cdot b_{13}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}a_{13} \cdot a_{12} \cdot b_{11}\end{matrix}\bigr)\bigl]\)-
\(\bigl(\begin{matrix}b_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}b_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}b_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}b_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}b_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}b_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}a_{11} \cdot b_{12} \cdot c_{13}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}a_{12} \cdot b_{13} \cdot c_{11}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}a_{13} \cdot b_{11} \cdot c_{12}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}c_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)

    Operando queda:

    \(\bigl|\begin{matrix}B\end{matrix}\bigr|\) =
\(\bigl(\begin{matrix}a_{11} \cdot b_{12} \cdot c_{13}\end{matrix}\bigr)\)+\(\bigl(\begin{matrix}a_{12} \cdot b_{13} \cdot c_{11}\end{matrix}\bigr)\)+
\(\bigl(\begin{matrix}a_{13} \cdot b_{11} \cdot c_{12}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{11} \cdot b_{12} \cdot a_{13}\end{matrix}\bigr)\)-
\(\bigl(\begin{matrix}c_{12} \cdot b_{13} \cdot a_{11}\end{matrix}\bigr)\)-\(\bigl(\begin{matrix}c_{13} \cdot b_{11} \cdot a_{12}\end{matrix}\bigr)\)
= \(\bigl|\begin{matrix}A\end{matrix}\bigr|\)

El efecto es que las dos matrices tienen el mismo determinante.

    \subsubsection{A partir de la definiciÃ³n 4, proponer una estrategia para
triangularizar una matriz sin cambiar su determinante e implementar en
Python una definiciÃ³n alternativa del
determinante.}\label{a-partir-de-la-definiciuxf3n-4-proponer-una-estrategia-para-triangularizar-una-matriz-sin-cambiar-su-determinante-e-implementar-en-python-una-definiciuxf3n-alternativa-del-determinante.}

    La estrategia propuesta es el mÃ©todo de eliminaciÃ³n de Gauss con pivoteo
parcial. Consiste en convertir una matriz en una una matriz triangular
superior. Este mÃ©todo utiliza operaciones elementales, como intercambio
de filas y multiplicaciÃ³n de filas por escalares, para lograr este
objetivo.

    Para ilustrar el mÃ©todo de eliminaciÃ³n de Gauss con pivoteo parcial,
consideremos la siguiente matriz de dimensiÃ³n 3x3:

    \begin{equation}
A =
    \begin{pmatrix}
        1 & 2 & 3\\
        4 & 5 & 6\\
        7 & 8 & 9
    \end{pmatrix}
\end{equation}

    Paso 1: Pivoteo parcial para la primera columna:

    \begin{itemize}
\tightlist
\item
  Seleccionamos la primera columna como columna pivote y buscamos el
  elemento de mayor magnitud (valor absoluto) en esa columna. En este
  caso, el elemento \textbf{7} en la tercera fila es el mayor.
\item
  Intercambiamos la primera fila con la tercera fila para tener un
  elemento pivote-mayor (7) en la diagonal principal.
\end{itemize}

    \begin{equation}
A =
    \begin{pmatrix}
        7 & 8 & 9\\
        4 & 5 & 6\\
        1 & 2 & 3
    \end{pmatrix}
\end{equation}

    Paso2: EliminaciÃ³n hacia adelante en la primera columna:

    Realizamos operaciones elementales para hacer ceros debajo del pivote
(7) en la primera columna. - A la segunda fila le sumamos el resultado
de multiplicar la primera fila por el factor -\(\frac{4}{7}\) - A la
tercera fila le sumamos el resultado de multiplicar la primera fila por
el factor -\(\frac{1}{7}\)

    \begin{equation}
A =
    \begin{pmatrix}
        7 & 8 & 9\\
        4+7\left (-\frac{4}{7}\right)& 5+8\left (-\frac{4}{7}\right) & 6+9\left (-\frac{4}{7}\right)\\
        1+7\left (-\frac{1}{7}\right) & 2+8\left (-\frac{1}{7}\right) & 3+9\left (-\frac{1}{7}\right)
    \end{pmatrix}
\end{equation}

    \begin{equation}
A =
    \begin{pmatrix}
        7 & 8 & 9\\
        0 & 5 -\frac{32}{7} & 6 -\frac{36}{7}\\
        0 & 2 -\frac{8}{7} & 3 -\frac{9}{7}
    \end{pmatrix}
\end{equation}

    \begin{equation}
A =
    \begin{pmatrix}
        7 & 8 & 9\\
        0 & \frac{3}{7} & \frac{6}{7}\\
        0 & \frac{6}{7} & \frac{12}{7}
    \end{pmatrix}
\end{equation}

    Paso3: Pivoteo parcial para la segunda columna:

    \begin{itemize}
\tightlist
\item
  Seleccionamos la segunda columna como columna pivote y buscamos el
  elemento de mayor magnitud (valor absoluto) en esa columna a partir de
  la fila2. En este caso, el elemento \(\frac{6}{7}\) en la tercera fila
  es el mayor.
\item
  Intercambiamos la segunda fila con la tercera fila para tener un
  elemento pivote mayor
\end{itemize}

    \begin{equation}
A =
    \begin{pmatrix}
        7 & 8 & 9\\
        0 & \frac{6}{7} & \frac{12}{7}\\
        0 & \frac{3}{7} & \frac{6}{7}
    \end{pmatrix}
\end{equation}

    Paso4: EliminaciÃ³n hacia adelante en la segunda columna:

    Realizamos operaciones elementales anteriores para hacer ceros debajo
del pivote en la segunda columna. - A la tercera fila le sumamos el
resultado de multiplicar la segunda fila por el factor \(-\frac{3}{6}\)

    \begin{equation}
A =
    \begin{pmatrix}
        7 & 8 & 9\\
        0 & \frac{6}{7} & \frac{12}{7}\\
        0 & \frac{3}{7}+\frac{6}{7} \left (-\frac{3}{6}\right) & \frac{6}{7}+\frac{12}{7} \left (-\frac{3}{6}\right)
    \end{pmatrix}
\end{equation}

    \begin{equation}
A =
    \begin{pmatrix}
        7 & 8 & 9\\
        0 & \frac{6}{7} & \frac{12}{7}\\
        0 & \frac{3}{7}-\frac{3}{7} & \frac{6}{7}-\frac{6}{7}
    \end{pmatrix}
\end{equation}

    \begin{equation}
A =
    \begin{pmatrix}
        7 & 8 & 9\\
        0 & \frac{6}{7} & \frac{12}{7}\\
        0 & 0 & 0
    \end{pmatrix}
\end{equation}

    La matriz resultante es una matriz triangular superior, y se ha logrado
sin cambiar el determinante significativamente, salvo por el intercambio
de filas, que cambia el signo del determinante.

    \subsubsection{Implementar en Python la definiciÃ³n asÃ­
obtenida}\label{implementar-en-python-la-definiciuxf3n-asuxed-obtenida}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{94}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{from} \PY{n+nn}{fractions} \PY{k+kn}{import} \PY{n}{Fraction}

\PY{k}{def} \PY{n+nf}{calcular\PYZus{}determinante}\PY{p}{(}\PY{n}{matriz}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    FunciÃ³n que recibe una matriz (lista de listas) y calcula el determinante utilizando el mÃ©todo de pivoteo parcial.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} Convertir la matriz a tipo Fraction para mantener la precisiÃ³n}
    \PY{c+c1}{\PYZsh{} Convertir la matriz a tipo Fraction para mantener la precisiÃ³n}
    \PY{n}{matriz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vectorize}\PY{p}{(}\PY{n}{Fraction}\PY{p}{)}\PY{p}{(}\PY{n}{matriz}\PY{p}{)}

    \PY{n}{n} \PY{o}{=} \PY{n}{matriz}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{n}{signo} \PY{o}{=} \PY{l+m+mi}{1}
    \PY{n}{det} \PY{o}{=} \PY{l+m+mi}{1}

    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Buscar el mÃ¡ximo en la columna actual desde la fila i hasta el final}
        \PY{n}{max\PYZus{}valor\PYZus{}index} \PY{o}{=} \PY{n}{i} \PY{o}{+} \PY{n}{np}\PY{o}{.}\PY{n}{argmax}\PY{p}{(}\PY{n+nb}{abs}\PY{p}{(}\PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Intercambio de filas si el valor no es el primero}
        \PY{k}{if} \PY{n}{max\PYZus{}valor\PYZus{}index} \PY{o}{!=} \PY{n}{i}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Se cambia la fila i por la del valor mÃ¡ximo}
            \PY{n}{matriz}\PY{p}{[}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{max\PYZus{}valor\PYZus{}index}\PY{p}{]}\PY{p}{]} \PY{o}{=} \PY{n}{matriz}\PY{p}{[}\PY{p}{[}\PY{n}{max\PYZus{}valor\PYZus{}index}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{]}
            \PY{n}{signo} \PY{o}{*}\PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
            
        \PY{k}{if} \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{k}{return} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{matriz}
            
        \PY{c+c1}{\PYZsh{} EliminaciÃ³n por filas hacia adelante de manera vectorizada}
        \PY{n}{pivote} \PY{o}{=} \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{n}{i}\PY{p}{]}
        \PY{n}{factores} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{matriz}\PY{p}{[}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]} \PY{o}{/} \PY{n}{pivote}
        \PY{n}{matriz}\PY{p}{[}\PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{outer}\PY{p}{(}\PY{n}{factores}\PY{p}{,} \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Calcular el determinante como el producto de los elementos diagonales}
    \PY{n}{det} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{prod}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{matriz}\PY{p}{)}\PY{p}{)}

    \PY{k}{return} \PY{n+nb}{int}\PY{p}{(}\PY{n}{signo} \PY{o}{*} \PY{n}{det}\PY{p}{)}\PY{p}{,} \PY{n}{matriz}

\PY{k}{def} \PY{n+nf}{formato\PYZus{}fraccion}\PY{p}{(}\PY{n}{valor}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}FunciÃ³n para formatear una fracciÃ³n como texto \PYZsq{}num/den\PYZsq{}.\PYZdq{}\PYZdq{}\PYZdq{}}
    
    \PY{c+c1}{\PYZsh{}return f\PYZdq{}\PYZob{}valor.numerator\PYZcb{}/\PYZob{}valor.denominator\PYZcb{}\PYZdq{}}
    \PY{n}{fraccion} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{valor}\PY{o}{.}\PY{n}{numerator}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{/}\PY{l+s+si}{\PYZob{}}\PY{n}{valor}\PY{o}{.}\PY{n}{denominator}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{return} \PY{n}{fraccion}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{fraccion}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{4}\PY{p}{)}


\PY{c+c1}{\PYZsh{} Consideremos la siguiente matriz de dimensiÃ³n 3x3:}
\PY{n}{A} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{7}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{]}
\PY{n}{det\PYZus{}matriz}\PY{p}{,} \PY{n}{matriz\PYZus{}superior} \PY{o}{=} \PY{n}{calcular\PYZus{}determinante}\PY{p}{(}\PY{n}{A}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Convertir la matriz superior a formato de fracciones para impresiÃ³n}
\PY{n}{matriz\PYZus{}superior\PYZus{}fracciones} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vectorize}\PY{p}{(}\PY{n}{formato\PYZus{}fraccion}\PY{p}{)}\PY{p}{(}\PY{n}{matriz\PYZus{}superior}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Matriz triangular superior A:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{fila}\PY{p}{)} \PY{k}{for} \PY{n}{fila} \PY{o+ow}{in} \PY{n}{matriz\PYZus{}superior\PYZus{}fracciones}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Determinante calculado con algoritmo : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{det\PYZus{}matriz}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Determinante calculado con numpy     : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{round}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Consideremos la siguiente matriz de dimensiÃ³n 3x3:}
\PY{n}{B} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{]}\PY{p}{]}
\PY{n}{det\PYZus{}matriz}\PY{p}{,} \PY{n}{matriz\PYZus{}superior} \PY{o}{=} \PY{n}{calcular\PYZus{}determinante}\PY{p}{(}\PY{n}{B}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Convertir la matriz superior a formato de fracciones para impresiÃ³n}
\PY{n}{matriz\PYZus{}superior\PYZus{}fracciones} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vectorize}\PY{p}{(}\PY{n}{formato\PYZus{}fraccion}\PY{p}{)}\PY{p}{(}\PY{n}{matriz\PYZus{}superior}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Matriz triangular superior B:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{fila}\PY{p}{)} \PY{k}{for} \PY{n}{fila} \PY{o+ow}{in} \PY{n}{matriz\PYZus{}superior\PYZus{}fracciones}\PY{p}{)}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Determinante calculado con algoritmo : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{det\PYZus{}matriz}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Determinante calculado con numpy     : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n+nb}{round}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{B}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Matriz triangular superior A:
7/1     8/1     9/1
0/1     6/7     12/7
0/1     0/1     0/1
Determinante calculado con algoritmo :  0
Determinante calculado con numpy     :  0

Matriz triangular superior B:
6/1     8/1     9/1
0/1     -20/3     -9/2
0/1     0/1     37/10
Determinante calculado con algoritmo :  148
Determinante calculado con numpy     :  148
    \end{Verbatim}

    \textbf{\textbf{\emph{ConclusiÃ³n :}}}

    \emph{Los resultados obtenidos al calcular el determinante de las
matrices A y B utilizando el mÃ©todo de pivoteo parcial coinciden
exactamente con los obtenidos mediante la funciÃ³n np.linalg.det de
numpy. Esto valida la eficacia del mÃ©todo de triangularizaciÃ³n para
matrices, que permite calcular el determinante sin alterar su valor.
Este enfoque es especialmente Ãºtil cuando se requiere mantener la
precisiÃ³n numÃ©rica utilizando fracciones en lugar de nÃºmeros de punto
flotante.}

    \subsection{Ejercicio 3:
ComparaciÃ³n.}\label{ejercicio-3-comparaciuxf3n.}

\subsubsection{Obtener la complejidad computacional de cada una de estas
dos
implementaciones.}\label{obtener-la-complejidad-computacional-de-cada-una-de-estas-dos-implementaciones.}

    \begin{itemize}
\item
  Calcular complejidad computacional ejercicio: Determinante Matriz
  Combinada
\item
  FunciÃ³n \textbf{generate\_random\_parameters(inf\_lim, sup\_lim)}:\\
  Tiene complejidad computacional \textbf{O(nÂ²)} dado que las lÃ­neas que
  generan vectores y matrices (puntos 3, 4 y 5) tienen complejidades
  O(n) y O(nÂ²), respectivamente, y el resto de las lÃ­neas O(1), la
  complejidad total del cÃ³digo estÃ¡ dominada por la generaciÃ³n de la
  matriz 'A', es decir \textbf{O(nÂ²)}.
\item
  FunciÃ³n \textbf{calcular\_determinante\_combinado(lambda\_val, v,
  omega, A)}:\\
  La funciÃ³n tiene la siguiente estructura:

  \begin{itemize}
  \tightlist
  \item
    ConstrucciÃ³n de la matriz combinada: Tiene un costo de O(n), donde n
    es el tamaÃ±o del vector v y la matriz A (asumiendo que omega tiene
    la misma longitud que v).
  \item
    CÃ¡lculo del determinante de la matriz combinada: Utilizando
    np.linalg.det, el costo es \(O(nÂ³)\) en el peor de los casos para
    matrices cuadradas.
  \item
    CÃ¡lculo de los cofactores: Se realiza un ciclo for de tamaÃ±o n (el
    tamaÃ±o de omega). Dentro del ciclo, se calcula el determinante de
    una submatriz de tamaÃ±o (n-1) x (n-1), con un costo de
    \(O((n-1)Â³)\).
  \end{itemize}

  Por lo tanto, el costo de calcular los cofactores es
  \(O(n * (n-1)Â³)\), que es equivalente a \(O(nâ´)\). VerificaciÃ³n del
  determinante: Tiene un costo de O(n), ya que se realiza una suma de n
  tÃ©rminos.
\end{itemize}

\begin{quote}
\emph{"La complejidad computacional total del cÃ³digo es \(O(nâ´)\), lo
que significa que el tiempo de ejecuciÃ³n crece con la cuarta potencia
del tamaÃ±o de la matriz. Este es un tiempo de ejecuciÃ³n relativamente
alto, especialmente para matrices grandes."}
\end{quote}

    \begin{itemize}
\item
  Calcular complejidad computacional ejercicio: Determinante de una
  Matriz utilizando el mÃ©todo de pivoteo parcial
\item
  FunciÃ³n \textbf{calcular\_determinante(matriz)}:\\
  EstÃ¡ funciÃ³n realiza las siguientes operaciones:

  \begin{itemize}
  \tightlist
  \item
    VectorizaciÃ³n y conversiÃ³n a fracciones, se realiza una
    vectorizaciÃ³n de la matriz para convertirla en tipo
    \textbf{'fraction'}, la cual tiene una complejidad de O(nÂ²) en donde
    n indica el tamaÃ±o de la matriz.
  \item
    Iteraciones, el algoritmo itera sobre cada fila i de la matriz y en
    cada iteraciÃ³n:

    \begin{itemize}
    \tightlist
    \item
      Busca el mÃ¡ximo en la columna actual hasta el final, lo cual tiene
      una complejidad de O(n-i)
    \item
      Realiza un intercambio de filas si es necesario, lo cual es una
      operaciÃ³n O(n).
    \item
      Realiza una eliminaciÃ³n por filas hacia adelante, que implica
      operaciones vectorizadas que pueden ser aproximadamente O((n
      -i)Â²).
    \end{itemize}
  \item
    Calcular el determinante, despuÃ©s de triangular la matriz, se
    calcula el determinante como el producto de los elementos
    diagonales, lo cual es una operaciÃ³n O(n).
  \end{itemize}
\end{itemize}

\begin{quote}
\emph{"Dado que el algoritmo realiza operaciones principalmente sobre
una matriz cuadrada de tamaÃ±o nxn la complejidad total puede aproximarse
a O(n)Â³. Esto se debe a que las operaciones de bÃºsqueda del mÃ¡ximo y
eliminaciÃ³n por filas son operaciones cuadrÃ¡ticas dentro de un bucle n
veces."}
\end{quote}

    \subsubsection{\texorpdfstring{Generar matrices aleatoriamente en
dimensiÃ³n \(n â\) \{ 2, 3, Â· Â· , 9, 10 \} y comparar el tiempo de
ejecuciÃ³n de cada una de estas dos implementaciones con la funciÃ³n
numpy.linalg.det (la funciÃ³n determinante de la extensiÃ³n numÃ©rica de
Python al Ã¡lgebra lineal). IndicaciÃ³n: se puede utilizar la funciÃ³n
numpy.random.rand para generar los coeficientes aleatorios de sus
matrices.}{Generar matrices aleatoriamente en dimensiÃ³n n â \{ 2, 3, Â· Â· , 9, 10 \} y comparar el tiempo de ejecuciÃ³n de cada una de estas dos implementaciones con la funciÃ³n numpy.linalg.det (la funciÃ³n determinante de la extensiÃ³n numÃ©rica de Python al Ã¡lgebra lineal). IndicaciÃ³n: se puede utilizar la funciÃ³n numpy.random.rand para generar los coeficientes aleatorios de sus matrices.}}\label{generar-matrices-aleatoriamente-en-dimensiuxf3n-n-2-3-9-10-y-comparar-el-tiempo-de-ejecuciuxf3n-de-cada-una-de-estas-dos-implementaciones-con-la-funciuxf3n-numpy.linalg.det-la-funciuxf3n-determinante-de-la-extensiuxf3n-numuxe9rica-de-python-al-uxe1lgebra-lineal.-indicaciuxf3n-se-puede-utilizar-la-funciuxf3n-numpy.random.rand-para-generar-los-coeficientes-aleatorios-de-sus-matrices.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{95}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{timeit}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{96}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{det\PYZus{}laplace}\PY{p}{(}\PY{n}{matriz}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n+nb}{float}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Calcula el determinante de una matriz usando la regla de Laplace.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        matriz: Es la matriz n x n para la cual se calcula el determinante}

\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    float}
\PY{l+s+sd}{        El determinante de la matriz}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}

  \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{matriz}\PY{p}{)}
  
  \PY{k}{if} \PY{n}{n} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
    \PY{k}{return} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
  \PY{k}{elif} \PY{n}{n} \PY{o}{==} \PY{l+m+mi}{2}\PY{p}{:}
    \PY{k}{return} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
  \PY{k}{else}\PY{p}{:}
    \PY{n}{det} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
      \PY{n}{submatriz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{matriz}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{j}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
      \PY{n}{det} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{n}{j} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{*} \PY{n}{det\PYZus{}laplace}\PY{p}{(}\PY{n}{submatriz}\PY{p}{)}
    \PY{k}{return} \PY{n}{det}


\PY{k}{def} \PY{n+nf}{det\PYZus{}gauss\PYZus{}jordan}\PY{p}{(}\PY{n}{matriz}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n+nb}{float}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Calcula el determinante de una matriz usando el mÃ©todo de eliminaciÃ³n de Gauss\PYZhy{}Jordan.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        matriz: Es la matriz n x n para la cual se calcula el determinante}

\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    float}
\PY{l+s+sd}{        El determinante de la matriz}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}

  \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{matriz}\PY{p}{)}
  \PY{n}{det} \PY{o}{=} \PY{l+m+mi}{1}

  \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
    \PY{n}{pivot} \PY{o}{=} \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
    \PY{k}{if} \PY{n}{pivot} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
      \PY{k}{return} \PY{l+m+mi}{0}
    
    \PY{n}{det}  \PY{o}{*}\PY{o}{=} \PY{n}{pivot}
    \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{/}\PY{o}{=} \PY{n}{pivot}

    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{:}
      \PY{n}{factor} \PY{o}{=} \PY{n}{matriz}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
      \PY{n}{matriz}\PY{p}{[}\PY{n}{j}\PY{p}{]}  \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{factor} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{]}
  \PY{k}{return} \PY{n}{det}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{97}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{DimensiÃ³n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{| T.Laplace (ms)}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{17}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{| T.Gauss\PYZhy{}Jordan (ms)}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{22}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{| T.numpy.linalg.det (ms)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}}\PY{l+s+s2}{\PYZdq{}} \PY{o}{*} \PY{l+m+mi}{75}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Genera matrices aleatorias de tamaÃ±o n x n}
\PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
  \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{n}\PY{p}{)}

  \PY{c+c1}{\PYZsh{} Calcula el tiempo de ejecuciÃ³n para cada algoritmo}
  \PY{n}{laplace\PYZus{}time} \PY{o}{=} \PY{n}{timeit}\PY{o}{.}\PY{n}{timeit}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{det\PYZus{}laplace}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{,} \PY{n}{number}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{1000}
  \PY{n}{gauss\PYZus{}jordan\PYZus{}time} \PY{o}{=} \PY{n}{timeit}\PY{o}{.}\PY{n}{timeit}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{det\PYZus{}gauss\PYZus{}jordan}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{,} \PY{n}{number}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  \PY{o}{*} \PY{l+m+mi}{1000}
  \PY{n}{numpy\PYZus{}time} \PY{o}{=} \PY{n}{timeit}\PY{o}{.}\PY{n}{timeit}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{,} \PY{n}{number}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  \PY{o}{*} \PY{l+m+mi}{1000}

  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n+nb}{str}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ | }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{format}\PY{p}{(}\PY{n}{laplace\PYZus{}time}\PY{p}{,}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.6f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ | }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{format}\PY{p}{(}\PY{n}{gauss\PYZus{}jordan\PYZus{}time}\PY{p}{,}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.6f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{19}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ | }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{format}\PY{p}{(}\PY{n}{numpy\PYZus{}time}\PY{p}{,}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.6f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
  
  
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
DimensiÃ³n | T.Laplace (ms) | T.Gauss-Jordan (ms) | T.numpy.linalg.det (ms)
---------------------------------------------------------------------------
2         | 0.011021       | 0.040960            | 0.046467
3         | 0.114820       | 0.045672            | 0.042871
4         | 0.390938       | 0.067282            | 0.037821
5         | 2.464395       | 0.092947            | 0.058323
6         | 14.373713      | 0.117725            | 0.039186
7         | 92.178981      | 0.147262            | 0.038534
8         | 727.794312     | 0.178852            | 0.044963
9         | 6274.807835    | 0.233912            | 0.039860
10        | 63215.603985   | 0.211654            | 0.045628
    \end{Verbatim}

    \textbf{\textbf{\emph{ConclusiÃ³n}}:}

    \emph{"Como podemos observar el tiempo de ejecuciÃ³n en
\textbf{np.linalg.det} de \textbf{numpy} es mucho mÃ¡s eficiente que el
tiempo que tardan en ejecutarse los algoritmos planteados para calcular
el determinante con la funciÃ³n : det\_laplace y det\_gauss\_jordan."}

    \section{Ejercicios acerca del
gradiente}\label{ejercicios-acerca-del-gradiente}

\subsection{Ejercicio 4 : MÃ©todo descenso del
gradiente}\label{ejercicio-4-muxe9todo-descenso-del-gradiente}

Con el propÃ³sito de aproximar un mÃ­nimo local de una funciÃ³n real de
varias variables reales, el mÃ©todo de descenso de gradiente consiste en
iterar una marcha (positivamente) proporcional al (opuesto del)
gradiente desde un valor inicial, con la intuiciÃ³n de `seguir el agua'
hasta dar con el valle.

\subsubsection{\texorpdfstring{Implementar en Python un algoritmo de
descenso del gradiente (con un mÃ¡ximo de m = \(10âµ\) iteraciones) a
partir de los siguientes argumentos tomados en ese
orden:}{Implementar en Python un algoritmo de descenso del gradiente (con un mÃ¡ximo de m = 10âµ iteraciones) a partir de los siguientes argumentos tomados en ese orden:}}\label{implementar-en-python-un-algoritmo-de-descenso-del-gradiente-con-un-muxe1ximo-de-m-10-iteraciones-a-partir-de-los-siguientes-argumentos-tomados-en-ese-orden}

\begin{itemize}
\tightlist
\item
  la funciÃ³n f cuyo mÃ­nimo local se propone aproximar,\\
\item
  el valor inicial x desde el que empieza la marcha,\\
\item
  la razÃ³n geomÃ©trica o coeficiente de proporcionalidad y,\\
\item
  el parÃ¡metro de tolerancia z para finalizar cuando el gradiente de la
  funciÃ³n f caiga dentro de esa tolerancia.\\
\end{itemize}

IndicaciÃ³n: empezar por implementar el gradiente grad(f) de la funciÃ³n
f.\\

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k}{def} \PY{n+nf}{gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{h}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    AproximaciÃ³n del gradiente de f en el punto x usando diferencias finitas.}

\PY{l+s+sd}{    f: La funciÃ³n de la cual se va a calcular el gradiente.}
\PY{l+s+sd}{    x: El punto en el cual se va a calcular el gradiente.}
\PY{l+s+sd}{    h: Un pequeÃ±o incremento para calcular las diferencias finitas.}
\PY{l+s+sd}{    return: El gradiente de f en x.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \PY{n}{gradiente} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{n}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
        \PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{x1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{x1}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{h}
        \PY{n}{gradiente}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{x1}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{f}\PY{p}{(}\PY{n}{x0}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{h}
    \PY{k}{return} \PY{n}{gradiente}

\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Algoritmo de descenso de gradiente para encontrar el mÃ­nimo de una funciÃ³n f.}

\PY{l+s+sd}{    :param f: La funciÃ³n cuyo mÃ­nimo local se desea encontrar.}
\PY{l+s+sd}{    :param x0: El punto inicial desde donde comienza la bÃºsqueda.}
\PY{l+s+sd}{    :param tasa\PYZus{}aprendizaje: La razÃ³n geomÃ©trica o coeficiente de proporcionalidad.}
\PY{l+s+sd}{    :param tolerancia: El parÃ¡metro de tolerancia para finalizar cuando el gradiente de f estÃ© dentro de esa tolerancia.}
\PY{l+s+sd}{    :param max\PYZus{}iter: El nÃºmero mÃ¡ximo de iteraciones.}
\PY{l+s+sd}{    :return: El punto que minimiza la funciÃ³n f.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{x} \PY{o}{=} \PY{n}{x0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{grad} \PY{o}{=} \PY{n}{gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ConvergiÃ³ despuÃ©s de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{grad}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AlcanzÃ³ el mÃ¡ximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Ejemplo de uso:}
\PY{c+c1}{\PYZsh{} Definimos una funciÃ³n cuadrÃ¡tica simple para demostrar el descenso de gradiente.}
\PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}

\PY{c+c1}{\PYZsh{} Punto inicial}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{} RazÃ³n geomÃ©trica o coeficiente de proporcionalidad}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{c+c1}{\PYZsh{} Tolerancia}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}6}

\PY{c+c1}{\PYZsh{} Ejecutar el descenso de gradiente}
\PY{n}{punto\PYZus{}minimo} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Punto mÃ­nimo encontrado:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{punto\PYZus{}minimo}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
ConvergiÃ³ despuÃ©s de 0 iteraciones.
Punto mÃ­nimo encontrado: [10 10]
    \end{Verbatim}

    \subsubsection{\texorpdfstring{Calcular formalmente
\(\{ ğ¡ â R. ğ â²(ğ¡) = 0 \}\) para
\(ğ : ğ¡ â¦ 3ğ¡â´+4ğ¡Â³â12ğ¡Â²+7\).}{Calcular formalmente \textbackslash{}\{ ğ¡ â R. ğ â²(ğ¡) = 0 \textbackslash{}\} para ğ : ğ¡ â¦ 3ğ¡â´+4ğ¡Â³â12ğ¡Â²+7.}}\label{calcular-formalmente-ux1d461-r.-ux1d453-ux1d461-0-para-ux1d453-ux1d461-3ux1d4614ux1d46112ux1d4617.}

    Para resolver este problema, primero necesitamos calcular la derivada de
la funciÃ³n \(( f(t) = 3t^4 + 4t^3 - 12t^2 + 7 )\) y luego encontrar los
puntos donde la derivada es igual a cero. Estos puntos son los
candidatos para los mÃ­nimos y mÃ¡ximos locales de la funciÃ³n.

Vamos a proceder paso a paso:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calcular la derivada de ( f(t) ).
\item
  Encontrar los puntos donde la derivada es igual a cero.
\item
  Utilizar la funciÃ³n de descenso de gradiente modificada para encontrar
  estos puntos.
\end{enumerate}

\textbf{- Calcular la derivada de \(( f(t) )\)}

La derivada de \(( f(t) )\) es:

\([ f'(t) = \frac{d}{dt}(3t^4 + 4t^3 - 12t^2 + 7) = 12t^3 + 12t^2 - 24t ]\)

    \textbf{- Encontrar los puntos donde la derivada es igual a cero}

Queremos encontrar los puntos \(( t )\) donde \(( f'(t) = 0 )\). Esto se
traduce en resolver la ecuaciÃ³n:

\([ 12t^3 + 12t^2 - 24t = 0 ]\)

Podemos factorizar esta ecuaciÃ³n:

\([ 12t(t^2 + t - 2) = 0 ]\)

Esta factorizaciÃ³n da tres posibles soluciones:

\([ 12t = 0 \quad \text{o} \quad t^2 + t - 2 = 0 ]\)

Resolviendo estas ecuaciones:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(( 12t = 0 )\) nos da \(( t = 0 )\).
\item
  \(( t^2 + t - 2 = 0 )\) se puede resolver usando la fÃ³rmula
  cuadrÃ¡tica:
\end{enumerate}

\([ t = \frac{-1 \pm \sqrt{1^2 - 4 \cdot 1 \cdot (-2)}}{2 \cdot 1} = \frac{-1 \pm \sqrt{1 + 8}}{2} = \frac{-1 \pm 3}{2} ]\)

Esto nos da dos soluciones:

\([ t = 1 \quad \text{y} \quad t = -2 ]\)

Por lo tanto, los puntos donde \(( f'(t) = 0 )\) son
\(( t = 0 ), ( t = 1 )\), y \(( t = -2 )\).

    \textbf{- ImplementaciÃ³n en Python de la funciÃ³n descenso de gradiente
modificada para encontrar estos puntos}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} Derivada de la funciÃ³n f(t)}
\PY{k}{def} \PY{n+nf}{f\PYZus{}prima}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3} \PY{o}{+} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{l+m+mi}{24}\PY{o}{*}\PY{n}{t}

\PY{c+c1}{\PYZsh{} Descenso de gradiente adaptado para encontrar raÃ­ces de la derivada}
\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prima}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{gradiente} \PY{o}{=} \PY{n}{f\PYZus{}prima}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{gradiente}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ConvergiÃ³ despuÃ©s de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{gradiente}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AlcanzÃ³ el mÃ¡ximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}

\PY{c+c1}{\PYZsh{} ParÃ¡metros para el descenso de gradiente}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}6}

\PY{c+c1}{\PYZsh{} Puntos iniciales para encontrar las raÃ­ces de la derivada}
\PY{n}{puntos\PYZus{}iniciales} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Ejecutar el descenso de gradiente para cada punto inicial}
\PY{n}{raices} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{x0} \PY{o+ow}{in} \PY{n}{puntos\PYZus{}iniciales}\PY{p}{:}
    \PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prima}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
    \PY{n}{raices}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{raiz}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Todas las raÃ­ces:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{raices}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
ConvergiÃ³ despuÃ©s de 0 iteraciones.
RaÃ­z encontrada: 0
ConvergiÃ³ despuÃ©s de 0 iteraciones.
RaÃ­z encontrada: 1
ConvergiÃ³ despuÃ©s de 0 iteraciones.
RaÃ­z encontrada: -2
Todas las raÃ­ces: [0, 1, -2]
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{FunciÃ³n \texttt{f\_prima}}: Calcula la derivada de ( f(t) ).
\item
  \textbf{FunciÃ³n \texttt{descenso\_gradiente\_raices}}: Aplica el
  descenso de gradiente para encontrar las raÃ­ces de la derivada de la
  funciÃ³n.
\item
  \textbf{ParÃ¡metros para el descenso de gradiente}: Definimos la tasa
  de aprendizaje y la tolerancia.
\item
  \textbf{Puntos iniciales}: Usamos los puntos ( t = 0 ), ( t = 1 ), y (
  t = -2 ) como puntos de partida para verificar las raÃ­ces.
\item
  \textbf{Ejecutar el descenso de gradiente}: Para cada punto inicial,
  ejecutamos el descenso de gradiente y almacenamos los resultados.
\end{enumerate}

    \subsubsection{\texorpdfstring{Con una tolerancia \(z = 10â»Â¹Â²\) y un
valor inicial de \(x = 3\) aplicar su algoritmo con razÃ³n \(y = 10â»Â¹\),
\(10â»Â²\), \(10â»Â³\) luego hacer lo mismo con \(x = 0\). Interpretar el
resultado.}{Con una tolerancia z = 10â»Â¹Â² y un valor inicial de x = 3 aplicar su algoritmo con razÃ³n y = 10â»Â¹, 10â»Â², 10â»Â³ luego hacer lo mismo con x = 0. Interpretar el resultado.}}\label{con-una-tolerancia-z-10-y-un-valor-inicial-de-x-3-aplicar-su-algoritmo-con-razuxf3n-y-10-10-10-luego-hacer-lo-mismo-con-x-0.-interpretar-el-resultado.}

    Para abordar este ejercicio, vamos a aplicar el algoritmo de descenso de
gradiente a la funciÃ³n \(( f(t) = 3t^4 + 4t^3 - 12t^2 + 7 )\) utilizando
diferentes tasas de aprendizaje \((\text{learning rates})\) y dos puntos
iniciales: \(( x = 3 )\) y \(( x = 0 )\). Utilizaremos una tolerancia de
\(( z = 10^{-12} )\).

Primero, recordemos la derivada de la funciÃ³n:

\([ f'(t) = 12t^3 + 12t^2 - 24t. ]\)

    \textbf{2.1.3.1 ImplementaciÃ³n del Algoritmo}

Vamos a implementar el descenso de gradiente para encontrar las raÃ­ces
de \(( f'(t) )\) utilizando las tasas de aprendizaje dadas. Luego,
interpretaremos los resultados para \(( x = 3 )\) y \(( x = 0 )\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} Derivada de la funciÃ³n f(t)}
\PY{k}{def} \PY{n+nf}{f\PYZus{}prime}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3} \PY{o}{+} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{l+m+mi}{24}\PY{o}{*}\PY{n}{t}

\PY{c+c1}{\PYZsh{} Descenso de gradiente adaptado para encontrar raÃ­ces de la derivada}
\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{gradiente} \PY{o}{=} \PY{n}{f\PYZus{}prime}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{gradiente}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ConvergiÃ³ despuÃ©s de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{gradiente}
        
        \PY{c+c1}{\PYZsh{} VerificaciÃ³n de lÃ­mites para prevenir overflow}
        \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mf}{1e10}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de x = }\PY{l+s+si}{\PYZob{}}\PY{n}{x}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ se volviÃ³ demasiado grande en la iteraciÃ³n }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{k+kc}{None}
        
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AlcanzÃ³ el mÃ¡ximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    \textbf{2.1.3.2 AplicaciÃ³n del Algoritmo}

\begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(( x = 3 )\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-1} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}12}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{l+m+mi}{3}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{k}{if} \PY{n}{raiz} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No se encontrÃ³ una raÃ­z dentro de los lÃ­mites permitidos.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 3 con tasa de aprendizaje = 0.1
El valor de x = -87049951065956.78 se volviÃ³ demasiado grande en la iteraciÃ³n 2.
No se encontrÃ³ una raÃ­z dentro de los lÃ­mites permitidos.
    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

La tasa de aprendizaje \(( y = 10^{-1} )\) es demasiado alta. Esto hace
que las actualizaciones en x sean muy grandes, causando que los valores
se vuelvan extremadamente grandes en poco tiempo, lo que lleva a un
desbordamiento numÃ©rico. Este es un claro ejemplo de cÃ³mo una tasa de
aprendizaje demasiado grande puede desestabilizar el algoritmo de
descenso de gradiente.

\begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(( x = 3 )\)
\item
  \textbf{Tasa de aprendizaje} \(y=10^{-2}\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 3 con tasa de aprendizaje = 0.01
ConvergiÃ³ despuÃ©s de 31 iteraciones.
RaÃ­z encontrada: -1.9999999999999882

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

Con una tasa de aprendizaje de \(( y = 10^{-2} )\), el algoritmo es mÃ¡s
estable y converge rÃ¡pidamente a una raÃ­z cercana a \(-2\). Este valor
es uno de los puntos donde la derivada de la funciÃ³n original es cero,
lo que indica un mÃ­nimo o mÃ¡ximo local.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(( x = 3 )\)
\item
  \textbf{Tasa de aprendizaje} \(y=10^{-3}\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 3 con tasa de aprendizaje = 0.001
ConvergiÃ³ despuÃ©s de 831 iteraciones.
RaÃ­z encontrada: 1.0000000000000275

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

Con una tasa de aprendizaje aÃºn mÃ¡s pequeÃ±a, \(( y = 10^{-3} )\), el
algoritmo converge de manera mÃ¡s lenta (requiriendo \(831\)
iteraciones). Sin embargo, alcanza un punto cercano a \(1\), que es otro
punto donde la derivada de la funciÃ³n original es cero. Esto demuestra
que una tasa de aprendizaje mÃ¡s pequeÃ±a puede llevar a una mayor
precisiÃ³n, aunque a costa de mÃ¡s iteraciones.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(x = 0\)
\item
  \textbf{Tasa de aprendizaje} \(y = 10^{-2}\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 0 con tasa de aprendizaje = 0.1
ConvergiÃ³ despuÃ©s de 0 iteraciones.
RaÃ­z encontrada: 0

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

Al comenzar en \(( x = 0 )\), el valor inicial ya es un punto donde la
derivada de la funciÃ³n es cero. No se necesitan iteraciones adicionales
porque 0 es una raÃ­z de la derivada. Esto muestra que el algoritmo
detecta correctamente que ya estÃ¡ en un punto estacionario.

    \textbf{Tasa de aprendizaje} \(y=10^{-1}\)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 0 con tasa de aprendizaje = 0.01
ConvergiÃ³ despuÃ©s de 0 iteraciones.
RaÃ­z encontrada: 0

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

Igual que con \(( y = 10^{-1} )\), el algoritmo reconoce que el punto
inicial x = 0 ya es una raÃ­z de la derivada. No se requieren
actualizaciones adicionales.

    \textbf{Tasa de aprendizaje} \(y=10^{-3}\)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 0 con tasa de aprendizaje = 0.001
ConvergiÃ³ despuÃ©s de 0 iteraciones.
RaÃ­z encontrada: 0

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

Nuevamente, al comenzar en \(x = 0\), el valor inicial ya es una raÃ­z de
la derivada. La tasa de aprendizaje no afecta el resultado en este caso
porque no se necesitan iteraciones adicionales.

    \textbf{2.1.3.3 ConclusiÃ³n General}

\begin{itemize}
\tightlist
\item
  \textbf{Tasa de aprendizaje grande}: Puede llevar a desbordamientos
  numÃ©ricos o a oscilaciones alrededor de la raÃ­z, como se observa con
  \(( y = 10^{-1} )\) cuando el valor inicial es \(( x = 3 )\).
\item
  \textbf{Tasa de aprendizaje moderada}: Proporciona un equilibrio entre
  velocidad y estabilidad, permitiendo una convergencia rÃ¡pida y
  precisa, como se observa con \(( y = 10^{-2} )\) cuando el valor
  inicial es \(( x = 3 )\).
\item
  \textbf{Tasa de aprendizaje pequeÃ±a}: Garantiza una alta precisiÃ³n,
  aunque a costa de un mayor nÃºmero de iteraciones, como se observa con
  \(( y = 10^{-3} )\) cuando el valor inicial es \(( x = 3 )\).
\item
  \textbf{Valor inicial en la raÃ­z}: Si el valor inicial es ya una raÃ­z
  como \(( x = 0 )\), el algoritmo converge instantÃ¡neamente sin
  necesidad de iteraciones adicionales.
\end{itemize}

Estos resultados ilustran cÃ³mo la elecciÃ³n de la tasa de aprendizaje y
el punto inicial pueden influir significativamente en el comportamiento
y eficiencia del algoritmo de descenso de gradiente.

    \subsubsection{\texorpdfstring{{[}Repetir estos dos Ãºltimos apartados
con \(ğ : (ğ , ğ¡) â¦ ğ Â² + 3ğ ğ¡ + ğ¡Â³ + 1\) y los valores iniciales x =
{[}-1,1{]},
{[}0,0{]}.{]}}{{[}Repetir estos dos Ãºltimos apartados con ğ : (ğ , ğ¡) â¦ ğ Â² + 3ğ ğ¡ + ğ¡Â³ + 1 y los valores iniciales x = {[}-1,1{]}, {[}0,0{]}.{]}}}\label{repetir-estos-dos-uxfaltimos-apartados-con-ux1d453-ux1d460-ux1d461-ux1d460-3ux1d460ux1d461-ux1d461-1-y-los-valores-iniciales-x--11-00.}

    \textbf{2.1.4.1: Calcular formalmente
\({ (s, t) â R^{2} | f(s, t) = 0 }\)}

La funciÃ³n dada es: \([ f(s, t) = s^2 + 3st + t^3 + 1 ]\)

El gradiente de \(( f(s, t) )\) es:
\([ \nabla f(s, t) = \left( \frac{\partial f}{\partial s}, \frac{\partial f}{\partial t} \right) ]\)

Calculando las derivadas parciales:

\([ \frac{\partial f}{\partial s} = 2s + 3t ]\)
\([ \frac{\partial f}{\partial t} = 3s + 3t^2 ]\)

Queremos encontrar los puntos donde el gradiente es cero:

\([ 2s + 3t = 0 ]\)

\([ 3s + 3t^2 = 0 ]\)

Resolviendo estas ecuaciones simultÃ¡neamente:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  De la primera ecuaciÃ³n: \([ 2s + 3t = 0 \implies s = -\frac{3}{2}t ]\)
\item
  Sustituyendo \(( s = -\frac{3}{2}t )\) en la segunda ecuaciÃ³n:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \([ 3\left(-\frac{3}{2}t\right) + 3t^2 = 0 ]\)
\item
  \([ -\frac{9}{2}t + 3t^2 = 0 ]\)
\item
  \([ t(3t - \frac{9}{2}) = 0 ]\)
\item
  \([ t = 0 \text{ o } t = \frac{3}{2} ]\)
\end{itemize}

Para \(( t = 0 )\): \([ s = 0 ]\)

Para \(( t = \frac{3}{2} )\):
\([ s = -\frac{3}{2} \left(\frac{3}{2}\right) = -\frac{9}{4} ]\)

Entonces, los puntos crÃ­ticos son: \([ (0, 0) ]\)
\([ \left(-\frac{9}{4}, \frac{3}{2}\right) ]\)

    \textbf{2.1.4.2 Aplicar el algoritmo de descenso de gradiente}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} Gradiente de la funciÃ³n f(s, t)}
\PY{k}{def} \PY{n+nf}{gradiente\PYZus{}f}\PY{p}{(}\PY{n}{st}\PY{p}{)}\PY{p}{:}
    \PY{n}{s}\PY{p}{,} \PY{n}{t} \PY{o}{=} \PY{n}{st}
    \PY{n}{df\PYZus{}ds} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{s} \PY{o}{+} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{t}
    \PY{n}{df\PYZus{}dt} \PY{o}{=} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{s} \PY{o}{+} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{df\PYZus{}ds}\PY{p}{,} \PY{n}{df\PYZus{}dt}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Descenso de gradiente adaptado para encontrar raÃ­ces del gradiente}
\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{grad\PYZus{}f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{x0}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{float}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{grad} \PY{o}{=} \PY{n}{grad\PYZus{}f}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ConvergiÃ³ despuÃ©s de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{grad}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{AlcanzÃ³ el mÃ¡ximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial \([-1, 1]\)}
\item
  \textbf{Tasa de aprendizaje \(( y = 10^{-1} )\)}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}12}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.1
ConvergiÃ³ despuÃ©s de 302 iteraciones.
RaÃ­z encontrada: [-2.25  1.5 ]

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

La tasa de aprendizaje de \(0.1\) es moderada, permitiendo que el
algoritmo converja razonablemente rÃ¡pido a la raÃ­z \(([-2.25, 1.5])\).
Este punto es una de las soluciones donde el gradiente de la funciÃ³n es
cero. El nÃºmero de iteraciones es relativamente bajo, lo que indica una
buena convergencia con esta tasa de aprendizaje.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial \([-1, 1]\)}
\item
  \textbf{Tasa de aprendizaje \(( y = 10^{-2} )\)}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.01
ConvergiÃ³ despuÃ©s de 3139 iteraciones.
RaÃ­z encontrada: [-2.25  1.5 ]

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

Con una tasa de aprendizaje de \(0.01\), el algoritmo converge mÃ¡s
lentamente que con una tasa de 0.1, pero aÃºn llega a la misma raÃ­z
\(([-2.25, 1.5])\). El nÃºmero de iteraciones es significativamente mayor
debido a la menor tasa de aprendizaje, lo que resulta en pasos mÃ¡s
pequeÃ±os hacia la convergencia.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial \([-1, 1]\)}
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-3} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.001
ConvergiÃ³ despuÃ©s de 31558 iteraciones.
RaÃ­z encontrada: [-2.25  1.5 ]

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}: Con una tasa de aprendizaje aÃºn mÃ¡s pequeÃ±a de
\(0.001\), el algoritmo requiere muchas mÃ¡s iteraciones para converger a
la misma raÃ­z \(([-2.25, 1.5])\). Esto demuestra que una tasa de
aprendizaje muy baja resulta en una convergencia muy lenta, aunque sigue
siendo precisa.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(([0, 0])\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-1} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [0, 0] con tasa de aprendizaje = 0.1
ConvergiÃ³ despuÃ©s de 0 iteraciones.
RaÃ­z encontrada: [0. 0.]

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

El valor inicial \(([0, 0])\) ya es un punto donde el gradiente de la
funciÃ³n es cero. Por lo tanto, el algoritmo no necesita realizar ninguna
iteraciÃ³n adicional para encontrar la raÃ­z. Esto muestra que el punto
inicial ya es una soluciÃ³n, independientemente de la tasa de
aprendizaje.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(([0, 0])\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-2} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [0, 0] con tasa de aprendizaje = 0.01
ConvergiÃ³ despuÃ©s de 0 iteraciones.
RaÃ­z encontrada: [0. 0.]

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

Igual que con la tasa de \(0.1\), el algoritmo reconoce que el punto
inicial \([0, 0])\) ya es una raÃ­z de la funciÃ³n, por lo que no se
requieren iteraciones adicionales.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(([0, 0])\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-3} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} ParÃ¡metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RaÃ­z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [0, 0] con tasa de aprendizaje = 0.001
ConvergiÃ³ despuÃ©s de 0 iteraciones.
RaÃ­z encontrada: [0. 0.]

    \end{Verbatim}

    \textbf{InterpretaciÃ³n}:

Al igual que con las tasas de aprendizaje mayores, el punto inicial
\(([0, 0])\) ya es una raÃ­z, y el algoritmo no necesita realizar
iteraciones adicionales.

    \textbf{2.1.4.3 ConclusiÃ³n General}

\begin{itemize}
\item
  \textbf{Tasa de aprendizaje y valor inicial \([-1, 1]\)}:
\item
  Una tasa de aprendizaje mayor (0.1) permite una convergencia mÃ¡s
  rÃ¡pida con menos iteraciones.
\item
  Una tasa de aprendizaje moderada (0.01) resulta en una convergencia
  mÃ¡s lenta pero estable.
\item
  Una tasa de aprendizaje muy pequeÃ±a (0.001) lleva a una convergencia
  extremadamente lenta, aunque precisa.
\item
  Todos convergen a la misma raÃ­z ({[}-2.25, 1.5{]}), que es un punto
  crÃ­tico de la funciÃ³n.
\item
  \textbf{Tasa de aprendizaje y valor inicial \([0, 0]\)}:
\item
  El valor inicial ({[}0, 0{]}) ya es un punto crÃ­tico donde el
  gradiente es cero.
\item
  Independientemente de la tasa de aprendizaje, el algoritmo reconoce
  inmediatamente que estÃ¡ en la raÃ­z y no realiza iteraciones
  adicionales.
\item
  Esto muestra que cuando el punto inicial es ya un punto crÃ­tico, la
  tasa de aprendizaje no influye en el resultado.
\end{itemize}

Estos resultados ilustran cÃ³mo la elecciÃ³n de la tasa de aprendizaje
afecta la velocidad de convergencia del algoritmo de descenso de
gradiente y cÃ³mo los puntos crÃ­ticos iniciales pueden simplificar la
convergencia.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
