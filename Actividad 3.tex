\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{tocbibind}
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    
    \usepackage{tocbibind}  % Incluye el índice en el ToC
    \usepackage{fancyhdr}  % Para encabezados y pies de página personalizados

    % Configura los márgenes, la altura del encabezado y la separación del encabezado
    %\usepackage[margin=1cm,headheight=80pt,includeheadfoot]{geometry}

    %\pagestyle{fancy}
    %\fancyhf{}  % Limpia los encabezados y pies de página
    %\rhead{\includegraphics[width=2cm]{resources/header.jpg}}  % Encabezado derecho con imagen

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \begin{titlepage}
      % Encabezado de la Institución
      \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/viu.png}\\[2cm]
        \textbf{\Huge Actividad evaluada \#3}\\[1cm]
      \end{center}

      % Detalles del Trabajo
      \begin{center}
        \textbf{\Large 02MIAR}\\[0.5cm]
        \textbf{\large Matemáticas para la Inteligencia Artificial}\\[2cm]
        
        \textbf{\Large Profesor}\\[0.5cm]
        \textbf{\large Dr. Matthieu F.-W. Huber}\\[2cm]
        
        \textbf{\Large Grupo No. 11}\\[1cm]
        \textbf{\large Jonathan Mora}\\
        \textbf{\large Luis Jama Tello}\\
        \textbf{\large Blanca Santos Fernández}\\
        \textbf{\large Laura Betancourt Leal}\\[2cm]
        
        \textbf{\Large Junio 2024}
      \end{center}
  \end{titlepage}

% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=black,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    %\tableofcontents
    \pagenumbering{gobble} % Para desactivar numeración en las páginas anteriores
    \textbf{\Large Índice general}\\[0.5cm]
    \contentsline {section}{\numberline {1}Ejercicios acerca del determinante}{1}{section.1}
    \contentsline {subsection}{\numberline {1.1}Desarrollo de Laplace.}{1}{subsection.1.1}
    \contentsline {subsubsection}{\numberline {1.1.1}Deducir de la definición 4 el determinante en dimensión 0, 1 y 2.}{1}{subsubsection.1.1.1}
    \contentsline {paragraph}{Dimensión 0}{1}{section*.2}
    \contentsline {paragraph}{Dimensión 1}{1}{section*.3}
    \contentsline {paragraph}{Dimensión 2}{1}{section*.4}
    \contentsline {subsubsection}{\numberline {1.1.2}A partir de la definición 4, expresar el determinante de una matriz cuadrada recursivamente en función de determinantes la matrices cuadradas de dimensión inferior.}{2}{subsubsection.1.1.2}
    \contentsline {subsubsection}{\numberline {1.1.3}Implementar en Python la definición así obtenida.}{3}{subsubsection.1.1.3}
    \contentsline {subsection}{\numberline {1.2}Ejercicio 2 : Eliminación de Gauss--Jordan.}{6}{subsection.1.2}
    \contentsline {subsubsection}{\numberline {1.2.1}Deducir de la definición 4 el efecto que tiene en el determinante de una matriz sumar a una de sus columnas una combinación lineal de las demás.}{6}{subsubsection.1.2.1}
    \contentsline {subsubsection}{\numberline {1.2.2}A partir de la definición 4, proponer una estrategia para triangularizar una matriz sin cambiar su determinante e implementar en Python una definición alternativa del determinante. Indicación: descomponer similarmente al ejercicio anterior.}{6}{subsubsection.1.2.2}
    \contentsline {subsubsection}{\numberline {1.2.3}Implementar en Python la definición así obtenida}{6}{subsubsection.1.2.3}
    \contentsline {subsection}{\numberline {1.3}Ejercicio 3: Comparación.}{6}{subsection.1.3}
    \contentsline {subsubsection}{\numberline {1.3.1}Obtener la complejidad computacional de cada una de estas dos implementaciones.}{6}{subsubsection.1.3.1}
    \contentsline {subsubsection}{\numberline {1.3.2}Generar matrices aleatoriamente en dimensión \(n ∈\) \{ 2, 3, · · , 9, 10 \} y comparar el tiempo de ejecución de cada una de estas dos implementaciones con la función numpy.linalg.det (la función determinante de la extensión numérica de Python al álgebra lineal). Indicación: se puede utilizar la función numpy.random.rand para generar los coeficientes aleatorios de sus matrices.}{7}{subsubsection.1.3.2}
    \contentsline {section}{\numberline {2}Ejercicios acerca del gradiente}{9}{section.2}
    \contentsline {subsection}{\numberline {2.1}Ejercicio 4 : Método descenso del gradiente}{9}{subsection.2.1}
    \contentsline {subsubsection}{\numberline {2.1.1}Implementar en Python un algoritmo de descenso del gradiente (con un máximo de m = \(10⁵\) iteraciones) a partir de los siguientes argumentos tomados en ese orden:}{9}{subsubsection.2.1.1}
    \contentsline {subsubsection}{\numberline {2.1.2}Calcular formalmente \(\{ 𝑡 ∈ R. 𝑓 ′(𝑡) = 0 \}\) para \(𝑓 : 𝑡 ↦ 3𝑡⁴+4𝑡³−12𝑡²+7\).}{11}{subsubsection.2.1.2}
    \contentsline {subsubsection}{\numberline {2.1.3}Con una tolerancia \(z = 10⁻¹²\) y un valor inicial de \(x = 3\) aplicar su algoritmo con razón \(y = 10⁻¹\), \(10⁻²\), \(10⁻³\) luego hacer lo mismo con \(x = 0\). Interpretar el resultado.}{13}{subsubsection.2.1.3}
    \contentsline {subsubsection}{\numberline {2.1.4}{[}Repetir estos dos últimos apartados con \(𝑓 : (𝑠, 𝑡) ↦ 𝑠² + 3𝑠𝑡 + 𝑡³ + 1\) y los valores iniciales x = {[}-1,1{]}, {[}0,0{]}.{]}}{17}{subsubsection.2.1.4}
    \clearpage
    \pagenumbering{arabic} % Para activar la numeración en arábigo
    \setcounter{page}{1}   % Para reiniciar la numeración de páginas desde 1

    \newpage
    \section{Ejercicios acerca del
determinante}\label{ejercicios-acerca-del-determinante}

\subsection{Desarrollo de Laplace.}\label{desarrollo-de-laplace.}

\subsubsection{Deducir de la definición 4 el determinante en dimensión
0, 1 y
2.}\label{deducir-de-la-definiciuxf3n-4-el-determinante-en-dimensiuxf3n-0-1-y-2.}

    \paragraph{Dimensión 0}\label{dimensiuxf3n-0}

En dimensión 0, una matriz es simplemente un escalar, y su determinante
es el propio escalar:

\begin{equation}
det(a) = a
\end{equation}

\paragraph{Dimensión 1}\label{dimensiuxf3n-1}

Una matriz es un solo vector (o escalar). Para un vector
\(\mathbb{v_1}\) en \(\mathbb{R}\), el determinante es simplemente el
valor absoluto del vector, ya que la única función lineal antisimétrica
es la identidad.

\begin{equation}
\det([v_1]) = v_1
\end{equation}

\paragraph{Dimensión 2}\label{dimensiuxf3n-2}

En dimensión 2 tenemos una matriz 2x2. Sea \(\mathbb{A}\) una matriz de
\(\mathbb{R}^2\):

\begin{equation}
A =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\end{equation}

La función lineal antisimetrica que cumple la definición 4 seria:

\begin{equation}
det(A) =ad-cd
\end{equation}

La matriz identidad de una matriz 2x2 es:

\begin{equation}
A =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\end{equation}

Por lo que tenemos que el resultado del determinate sera dado por:

\begin{equation}
det(A) = 1 \cdot 1-0 \cdot 0 = 1
\end{equation}

    \subsubsection{A partir de la definición 4, expresar el determinante de
una matriz cuadrada recursivamente en función de determinantes la
matrices cuadradas de dimensión
inferior.}\label{a-partir-de-la-definiciuxf3n-4-expresar-el-determinante-de-una-matriz-cuadrada-recursivamente-en-funciuxf3n-de-determinantes-la-matrices-cuadradas-de-dimensiuxf3n-inferior.}

Indicación: para cada \(n ∈ N\), distribuir (por linealidad en las
columnas) sobre la descomposición

\begin{equation}
\begin{bmatrix}
\lambda & \omega \\
v & A
\end{bmatrix} = \begin{bmatrix}
\lambda \cdot 1 + 0 & \omega \\
\lambda \cdot 0 + v & A
\end{bmatrix}.
\end{equation}

de una matriz cuadrada de dimensión 𝑛 + 1, siendo \(n ∈ N\) y\\
- \(𝜆\) un coeficiente real,\\
- \(𝑣\) un vector de dimensión 𝑛 (una columna de 𝑛 coeficientes
reales),\\
- \(𝜔\) un covector de la misma dimensión (una fila de 𝑛
coeficientes),\\
- \(𝐴\) una matriz cuadrada de la misma dimensión (con 𝑛2
coeficientes),\\
luego proceder del mismo modo con\\
- los demás coeficientes de esa primera columna,\\
- con cada columna.\\

    \textbf{Resolución ejercicio :}\\
Diremos que n = 2 por lo que tendremos las siguientes igualdades :

\begin{equation}
A =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix}
\end{equation}

\begin{equation}
v =
\begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}
\end{equation}

\begin{equation}
w =
\begin{bmatrix}
w_1 &&
w_2
\end{bmatrix}
\end{equation}

Usaremos una matriz M de dimensión 3×3 como ejemplo concreto donde
estaran los valores \(𝜆, 𝑣, 𝜔\) y los valores de A

\begin{equation}
M =
\begin{bmatrix}
\lambda & w_1 & w_2 \\
v_1 & a_{11} & a_{12} \\
v_2 & a_{21} & a_{22} \\
\end{bmatrix}
\end{equation}

    \textbf{Paso 1: Expansión por Cofactores}

Para calcular el determinante de M, vamos a usar cofactores. Esto
significa que vamos a descomponer el determinante de la matriz grande en
términos de los determinantes de matrices más pequeñas.

\begin{equation}
det(M) =
\lambda \cdot
det
 \Biggl(
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix} 
 \Biggl)
-
w_1 \cdot
det
\Biggl(
\begin{bmatrix}
v_1 & a_{12} \\
v_2 & a_{22} \\
\end{bmatrix}
\Biggl)
+
w_2 \cdot 
det
\Biggl(
\begin{bmatrix}
v_1 & a_{11} \\
v_2 & a_{21} \\
\end{bmatrix} 
\Biggl)
\end{equation}

    Entonces tenemos 3 sub matrices :

\begin{equation}
M_1 =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix}
\end{equation}

\begin{equation}
M_2 =
\begin{bmatrix}
v_1 & a_{12} \\
v_2 & a_{22} \\
\end{bmatrix}
\end{equation}

\begin{equation}
M_3 =
\begin{bmatrix}
v_1 & a_{11} \\
v_2 & a_{21} \\
\end{bmatrix}
\end{equation}

    Si simplificamos la ecuación tendríamos que:

\begin{equation}
det(M) =
\lambda \cdot
det(A)
-
w_1 \cdot det(M_2)
+
w_2 \cdot det(M_2)
\end{equation}

    Por tanto tenemos que en n:

\begin{equation}
M =
\begin{pmatrix}
\lambda & \omega_1 & \omega_2 & \cdots & \omega_n \\
v_1 & a_{11} & a_{12} & \cdots & a_{1n} \\
v_2 & a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
v_n & a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}
\end{equation}

El determinante esta dado por:

\begin{equation}
det(M) =
\lambda \cdot
det(A)
-
w_1 \cdot det(M_1)
+
w_2 \cdot det(M_2) ... + (-1)^{1+i} \cdot w_i \cdot det(A_i)
\end{equation}

Que es similar a decir que:

\begin{equation}
\det\begin{pmatrix}
\lambda & \omega \\
v & A
\end{pmatrix}
= \lambda \cdot \det(A) 
+
\sum_{i=1}^n \omega_i \cdot (-1)^{1+i} \cdot \det(A_i)
\end{equation}

    \subsubsection{Implementar en Python la definición así
obtenida.}\label{implementar-en-python-la-definiciuxf3n-asuxed-obtenida.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k}{def} \PY{n+nf}{calcular\PYZus{}determinante\PYZus{}combinado}\PY{p}{(}\PY{n}{lambda\PYZus{}val}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Convertir las entradas a matrices numpy}
    \PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{v}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{omega} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{omega}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{A}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Construir la matriz combinada}
    \PY{n}{primera\PYZus{}fila} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{p}{[}\PY{n}{lambda\PYZus{}val}\PY{p}{]}\PY{p}{,} \PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    \PY{n}{resto\PYZus{}filas} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{v}\PY{p}{,} \PY{n}{A}\PY{p}{)}\PY{p}{)}
    \PY{n}{matriz\PYZus{}combinada} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{(}\PY{n}{primera\PYZus{}fila}\PY{p}{,} \PY{n}{resto\PYZus{}filas}\PY{p}{)}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Matriz compuesta:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{matriz\PYZus{}combinada}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Calcular el determinante de la matriz combinada}
    \PY{n}{determinante\PYZus{}combinado} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{matriz\PYZus{}combinada}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Verificación del determinante según la fórmula}
    \PY{n}{determinante\PYZus{}A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A}\PY{p}{)}
    \PY{n}{suma\PYZus{}terminos} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{cofactores} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Crear una submatriz A\PYZus{}i eliminando la primera fila y la columna i}
        \PY{n}{columna\PYZus{}a\PYZus{}eliminar} \PY{o}{=} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{;}
        \PY{n}{num\PYZus{}filas}\PY{p}{,} \PY{n}{num\PYZus{}columnas} \PY{o}{=} \PY{n}{matriz\PYZus{}combinada}\PY{o}{.}\PY{n}{shape}
        \PY{n}{A\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{matriz\PYZus{}combinada}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Eliminar la primera fila}
        \PY{n}{A\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{A\PYZus{}i}\PY{p}{,} \PY{n}{columna\PYZus{}a\PYZus{}eliminar}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Eliminar la columna i}
        \PY{c+c1}{\PYZsh{}print(f\PYZdq{}(A\PYZus{}\PYZob{}i+1\PYZcb{}\PYZdq{})}
        \PY{c+c1}{\PYZsh{}print(A\PYZus{}i)}
        \PY{c+c1}{\PYZsh{}print(f\PYZdq{}w\PYZus{}\PYZob{}i+1\PYZcb{}\PYZdq{}, omega[0][i])}
        \PY{n}{cofactor} \PY{o}{=} \PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{i}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A\PYZus{}i}\PY{p}{)}
        \PY{n}{cofactores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cofactor}\PY{p}{)}
        \PY{n}{suma\PYZus{}terminos} \PY{o}{+}\PY{o}{=} \PY{n}{cofactor}
        \PY{c+c1}{\PYZsh{}print(f\PYZdq{}Cofactor \PYZob{}i+1\PYZcb{}: omega[\PYZob{}i\PYZcb{}] * (\PYZhy{}1)\PYZca{}\PYZob{}1+i\PYZcb{} * det(A\PYZus{}\PYZob{}i+1\PYZcb{}) = \PYZob{}omega[0][i]\PYZcb{} * (\PYZhy{}1)\PYZca{}\PYZob{}1+i\PYZcb{} * \PYZob{}np.linalg.det(A\PYZus{}i)\PYZcb{} = \PYZob{}cofactor\PYZcb{}\PYZdq{})}
    
    \PY{n}{determinante\PYZus{}verificado} \PY{o}{=} \PY{n}{lambda\PYZus{}val} \PY{o}{*} \PY{n}{determinante\PYZus{}A} \PY{o}{+} \PY{n}{suma\PYZus{}terminos}
    
    \PY{c+c1}{\PYZsh{} Mostrar cada uno de los cofactores y sus resultados}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cofactores:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Mostrar la fórmula completa en números}
    \PY{n}{formula} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{lambda\PYZus{}val}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * det(A) + }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * (\PYZhy{}1)\PYZca{}}\PY{l+s+si}{\PYZob{}}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * det(A\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
    \PY{n}{formula\PYZus{}numerica} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{lambda\PYZus{}val}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * }\PY{l+s+si}{\PYZob{}}\PY{n}{determinante\PYZus{}A}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{n+nb}{str}\PY{p}{(}\PY{n}{cofactor}\PY{p}{)} \PY{k}{for} \PY{n}{cofactor} \PY{o+ow}{in} \PY{n}{cofactores}\PY{p}{]}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fórmula simbólica: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{formula}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Fórmula numérica: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{formula\PYZus{}numerica}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{matriz\PYZus{}combinada}\PY{p}{,} \PY{n}{determinante\PYZus{}combinado}\PY{p}{,} \PY{n}{determinante\PYZus{}verificado}

\PY{c+c1}{\PYZsh{} Ejemplo de uso}
\PY{k}{def} \PY{n+nf}{generate\PYZus{}random\PYZus{}parameters}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{)}\PY{p}{:}

    \PY{c+c1}{\PYZsh{} 1. Generar n como un número entero aleatorio entre 2 y 5}
    \PY{n}{n} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 2. Generar lambda como un número entero aleatorio entre \PYZhy{}10 y 10}
    \PY{n}{lambda\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 3. Generar un vector columna v de dimensión n con valores enteros}
    \PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{,} \PY{n}{n}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 4. Generar un covector omega de dimensión n con valores enteros}
    \PY{n}{omega} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{,} \PY{n}{n}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 5. Generar una matriz cuadrada A de dimensión n x n con valores enteros}
    \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{,} \PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{n}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A}



\PY{n}{n}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A} \PY{o}{=} \PY{n}{generate\PYZus{}random\PYZus{}parameters}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{11}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{n}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lambda:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{v:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{v}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{omega:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{omega}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{A}\PY{p}{)}

\PY{n}{matriz\PYZus{}combinada}\PY{p}{,} \PY{n}{determinante\PYZus{}combinado}\PY{p}{,} \PY{n}{determinante\PYZus{}verificado} \PY{o}{=} \PY{n}{calcular\PYZus{}determinante\PYZus{}combinado}\PY{p}{(}\PY{n}{lambda\PYZus{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Determinante:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{determinante\PYZus{}combinado}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Verificacion:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{determinante\PYZus{}verificado}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
n: 5
lambda: 0
v: [ 5 -9 -6  7  0]
omega: [-5  6 -6 11  1]
A:
[[  3   6  13   2  -9]
 [  1  -4   4   1  -5]
 [ -7   3   1   8   5]
 [ -2  14   5   1  -3]
 [ -2 -11   4  12  13]]
Matriz compuesta:
[[  0  -5   6  -6  11   1]
 [  5   3   6  13   2  -9]
 [ -9   1  -4   4   1  -5]
 [ -6  -7   3   1   8   5]
 [  7  -2  14   5   1  -3]
 [  0  -2 -11   4  12  13]]
Cofactores:
Fórmula simbólica:  0 * det(A) + -5 * (-1)\^{}1 * det(A\_1) + 6 * (-1)\^{}2 * det(A\_2)
+ -6 * (-1)\^{}3 * det(A\_3) + 11 * (-1)\^{}4 * det(A\_4) + 1 * (-1)\^{}5 * det(A\_5)
Fórmula numérica:  0 * 690.0000000000016 + 140600.00000000006 +
-46212.00000000012 + -131280.0000000005 + -336688.00000000093 +
10678.000000000053
Determinante: -362902.00000000076
Verificacion: -362902.00000000146
    \end{Verbatim}

    \subsection{Ejercicio 2 : Eliminación de
Gauss--Jordan.}\label{ejercicio-2-eliminaciuxf3n-de-gaussjordan.}

\subsubsection{Deducir de la definición 4 el efecto que tiene en el
determinante de una matriz sumar a una de sus columnas una combinación
lineal de las
demás.}\label{deducir-de-la-definiciuxf3n-4-el-efecto-que-tiene-en-el-determinante-de-una-matriz-sumar-a-una-de-sus-columnas-una-combinaciuxf3n-lineal-de-las-demuxe1s.}

    

    \subsubsection{A partir de la definición 4, proponer una estrategia para
triangularizar una matriz sin cambiar su determinante e implementar en
Python una definición alternativa del determinante. Indicación:
descomponer similarmente al ejercicio
anterior.}\label{a-partir-de-la-definiciuxf3n-4-proponer-una-estrategia-para-triangularizar-una-matriz-sin-cambiar-su-determinante-e-implementar-en-python-una-definiciuxf3n-alternativa-del-determinante.-indicaciuxf3n-descomponer-similarmente-al-ejercicio-anterior.}

    

    \subsubsection{Implementar en Python la definición así
obtenida}\label{implementar-en-python-la-definiciuxf3n-asuxed-obtenida}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

    \subsection{Ejercicio 3:
Comparación.}\label{ejercicio-3-comparaciuxf3n.}

\subsubsection{Obtener la complejidad computacional de cada una de estas
dos
implementaciones.}\label{obtener-la-complejidad-computacional-de-cada-una-de-estas-dos-implementaciones.}

    \begin{itemize}
\item
  Calcular complejidad computacional ejercicio: Determinante Matriz
  Combinada
\item
  Función \textbf{generate\_random\_parameters(inf\_lim, sup\_lim)}:\\
  Tiene complejidad computacional \textbf{O(n²)} dado que las líneas que
  generan vectores y matrices (puntos 3, 4 y 5) tienen complejidades
  O(n) y O(n²), respectivamente, y el resto de las líneas O(1), la
  complejidad total del código está dominada por la generación de la
  matriz 'A', es decir \textbf{O(n²)}.
\item
  Función \textbf{calcular\_determinante\_combinado(lambda\_val, v,
  omega, A)}:\\
  La función tiene la siguiente estructura:

  \begin{itemize}
  \tightlist
  \item
    Construcción de la matriz combinada: Tiene un costo de O(n), donde n
    es el tamaño del vector v y la matriz A (asumiendo que omega tiene
    la misma longitud que v).
  \item
    Cálculo del determinante de la matriz combinada: Utilizando
    np.linalg.det, el costo es \(O(n³)\) en el peor de los casos para
    matrices cuadradas.
  \item
    Cálculo de los cofactores: Se realiza un ciclo for de tamaño n (el
    tamaño de omega). Dentro del ciclo, se calcula el determinante de
    una submatriz de tamaño (n-1) x (n-1), con un costo de
    \(O((n-1)³)\).
  \end{itemize}

  Por lo tanto, el costo de calcular los cofactores es
  \(O(n * (n-1)³)\), que es equivalente a \(O(n⁴)\). Verificación del
  determinante: Tiene un costo de O(n), ya que se realiza una suma de n
  términos.
\end{itemize}

\begin{quote}
\emph{"La complejidad computacional total del código es \(O(n⁴)\), lo
que significa que el tiempo de ejecución crece con la cuarta potencia
del tamaño de la matriz. Este es un tiempo de ejecución relativamente
alto, especialmente para matrices grandes."}
\end{quote}

    \subsubsection{\texorpdfstring{Generar matrices aleatoriamente en
dimensión \(n ∈\) \{ 2, 3, · · , 9, 10 \} y comparar el tiempo de
ejecución de cada una de estas dos implementaciones con la función
numpy.linalg.det (la función determinante de la extensión numérica de
Python al álgebra lineal). Indicación: se puede utilizar la función
numpy.random.rand para generar los coeficientes aleatorios de sus
matrices.}{Generar matrices aleatoriamente en dimensión n ∈ \{ 2, 3, · · , 9, 10 \} y comparar el tiempo de ejecución de cada una de estas dos implementaciones con la función numpy.linalg.det (la función determinante de la extensión numérica de Python al álgebra lineal). Indicación: se puede utilizar la función numpy.random.rand para generar los coeficientes aleatorios de sus matrices.}}\label{generar-matrices-aleatoriamente-en-dimensiuxf3n-n-2-3-9-10-y-comparar-el-tiempo-de-ejecuciuxf3n-de-cada-una-de-estas-dos-implementaciones-con-la-funciuxf3n-numpy.linalg.det-la-funciuxf3n-determinante-de-la-extensiuxf3n-numuxe9rica-de-python-al-uxe1lgebra-lineal.-indicaciuxf3n-se-puede-utilizar-la-funciuxf3n-numpy.random.rand-para-generar-los-coeficientes-aleatorios-de-sus-matrices.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{timeit}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{det\PYZus{}laplace}\PY{p}{(}\PY{n}{matriz}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n+nb}{float}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Calcula el determinante de una matriz usando la regla de Laplace.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        matriz: Es la matriz n x n para la cual se calcula el determinante}

\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    float}
\PY{l+s+sd}{        El determinante de la matriz}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}

  \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{matriz}\PY{p}{)}
  
  \PY{k}{if} \PY{n}{n} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
    \PY{k}{return} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
  \PY{k}{elif} \PY{n}{n} \PY{o}{==} \PY{l+m+mi}{2}\PY{p}{:}
    \PY{k}{return} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
  \PY{k}{else}\PY{p}{:}
    \PY{n}{det} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
      \PY{n}{submatriz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{matriz}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{j}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
      \PY{n}{det} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{n}{j} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{*} \PY{n}{det\PYZus{}laplace}\PY{p}{(}\PY{n}{submatriz}\PY{p}{)}
    \PY{k}{return} \PY{n}{det}


\PY{k}{def} \PY{n+nf}{det\PYZus{}gauss\PYZus{}jordan}\PY{p}{(}\PY{n}{matriz}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n+nb}{float}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Calcula el determinante de una matriz usando el método de eliminación de Gauss\PYZhy{}Jordan.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        matriz: Es la matriz n x n para la cual se calcula el determinante}

\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    float}
\PY{l+s+sd}{        El determinante de la matriz}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}

  \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{matriz}\PY{p}{)}
  \PY{n}{det} \PY{o}{=} \PY{l+m+mi}{1}

  \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
    \PY{n}{pivot} \PY{o}{=} \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
    \PY{k}{if} \PY{n}{pivot} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
      \PY{k}{return} \PY{l+m+mi}{0}
    
    \PY{n}{det}  \PY{o}{*}\PY{o}{=} \PY{n}{pivot}
    \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{/}\PY{o}{=} \PY{n}{pivot}

    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{:}
      \PY{n}{factor} \PY{o}{=} \PY{n}{matriz}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
      \PY{n}{matriz}\PY{p}{[}\PY{n}{j}\PY{p}{]}  \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{factor} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{]}
  \PY{k}{return} \PY{n}{det}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dimensión}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{| T.Laplace (ms)}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{17}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{| T.Gauss\PYZhy{}Jordan (ms)}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{22}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{| T.numpy.linalg.det (ms)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}}\PY{l+s+s2}{\PYZdq{}} \PY{o}{*} \PY{l+m+mi}{75}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Genera matrices aleatorias de tamaño n x n}
\PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
  \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{n}\PY{p}{)}

  \PY{c+c1}{\PYZsh{} Calcula el tiempo de ejecución para cada algoritmo}
  \PY{n}{laplace\PYZus{}time} \PY{o}{=} \PY{n}{timeit}\PY{o}{.}\PY{n}{timeit}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{det\PYZus{}laplace}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{,} \PY{n}{number}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{1000}
  \PY{n}{gauss\PYZus{}jordan\PYZus{}time} \PY{o}{=} \PY{n}{timeit}\PY{o}{.}\PY{n}{timeit}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{det\PYZus{}gauss\PYZus{}jordan}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{,} \PY{n}{number}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  \PY{o}{*} \PY{l+m+mi}{1000}
  \PY{n}{numpy\PYZus{}time} \PY{o}{=} \PY{n}{timeit}\PY{o}{.}\PY{n}{timeit}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{,} \PY{n}{number}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  \PY{o}{*} \PY{l+m+mi}{1000}

  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n+nb}{str}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ | }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{format}\PY{p}{(}\PY{n}{laplace\PYZus{}time}\PY{p}{,}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.6f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ | }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{format}\PY{p}{(}\PY{n}{gauss\PYZus{}jordan\PYZus{}time}\PY{p}{,}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.6f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{19}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ | }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{format}\PY{p}{(}\PY{n}{numpy\PYZus{}time}\PY{p}{,}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.6f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
  
  
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Dimensión | T.Laplace (ms) | T.Gauss-Jordan (ms) | T.numpy.linalg.det (ms)
---------------------------------------------------------------------------
2         | 0.009608       | 0.046513            | 0.035703
3         | 0.410473       | 0.042401            | 0.024766
4         | 0.830461       | 0.063236            | 0.026046
5         | 2.359853       | 0.090583            | 0.040796
6         | 13.553588      | 0.130150            | 0.036440
7         | 80.875877      | 0.162143            | 0.046795
8         | 643.808716     | 0.210978            | 0.045501
9         | 5897.031685    | 0.237919            | 0.040963
10        | 58559.983033   | 0.291078            | 0.039791
    \end{Verbatim}

    \section{Ejercicios acerca del
gradiente}\label{ejercicios-acerca-del-gradiente}

\subsection{Ejercicio 4 : Método descenso del
gradiente}\label{ejercicio-4-muxe9todo-descenso-del-gradiente}

Con el propósito de aproximar un mínimo local de una función real de
varias variables reales, el método de descenso de gradiente consiste en
iterar una marcha (positivamente) proporcional al (opuesto del)
gradiente desde un valor inicial, con la intuición de `seguir el agua'
hasta dar con el valle.

\subsubsection{\texorpdfstring{Implementar en Python un algoritmo de
descenso del gradiente (con un máximo de m = \(10⁵\) iteraciones) a
partir de los siguientes argumentos tomados en ese
orden:}{Implementar en Python un algoritmo de descenso del gradiente (con un máximo de m = 10⁵ iteraciones) a partir de los siguientes argumentos tomados en ese orden:}}\label{implementar-en-python-un-algoritmo-de-descenso-del-gradiente-con-un-muxe1ximo-de-m-10-iteraciones-a-partir-de-los-siguientes-argumentos-tomados-en-ese-orden}

\begin{itemize}
\tightlist
\item
  la función f cuyo mínimo local se propone aproximar,\\
\item
  el valor inicial x desde el que empieza la marcha,\\
\item
  la razón geométrica o coeficiente de proporcionalidad y,\\
\item
  el parámetro de tolerancia z para finalizar cuando el gradiente de la
  función f caiga dentro de esa tolerancia.\\
\end{itemize}

Indicación: empezar por implementar el gradiente grad(f) de la función
f.\\

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k}{def} \PY{n+nf}{gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{h}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Aproximación del gradiente de f en el punto x usando diferencias finitas.}

\PY{l+s+sd}{    f: La función de la cual se va a calcular el gradiente.}
\PY{l+s+sd}{    x: El punto en el cual se va a calcular el gradiente.}
\PY{l+s+sd}{    h: Un pequeño incremento para calcular las diferencias finitas.}
\PY{l+s+sd}{    return: El gradiente de f en x.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \PY{n}{gradiente} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{n}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
        \PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{x1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{x1}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{h}
        \PY{n}{gradiente}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{x1}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{f}\PY{p}{(}\PY{n}{x0}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{h}
    \PY{k}{return} \PY{n}{gradiente}

\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Algoritmo de descenso de gradiente para encontrar el mínimo de una función f.}

\PY{l+s+sd}{    :param f: La función cuyo mínimo local se desea encontrar.}
\PY{l+s+sd}{    :param x0: El punto inicial desde donde comienza la búsqueda.}
\PY{l+s+sd}{    :param tasa\PYZus{}aprendizaje: La razón geométrica o coeficiente de proporcionalidad.}
\PY{l+s+sd}{    :param tolerancia: El parámetro de tolerancia para finalizar cuando el gradiente de f esté dentro de esa tolerancia.}
\PY{l+s+sd}{    :param max\PYZus{}iter: El número máximo de iteraciones.}
\PY{l+s+sd}{    :return: El punto que minimiza la función f.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{x} \PY{o}{=} \PY{n}{x0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{grad} \PY{o}{=} \PY{n}{gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergió después de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{grad}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcanzó el máximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Ejemplo de uso:}
\PY{c+c1}{\PYZsh{} Definimos una función cuadrática simple para demostrar el descenso de gradiente.}
\PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}

\PY{c+c1}{\PYZsh{} Punto inicial}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Razón geométrica o coeficiente de proporcionalidad}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{c+c1}{\PYZsh{} Tolerancia}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}6}

\PY{c+c1}{\PYZsh{} Ejecutar el descenso de gradiente}
\PY{n}{punto\PYZus{}minimo} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Punto mínimo encontrado:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{punto\PYZus{}minimo}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Convergió después de 0 iteraciones.
Punto mínimo encontrado: [10 10]
    \end{Verbatim}

    \subsubsection{\texorpdfstring{Calcular formalmente
\(\{ 𝑡 ∈ R. 𝑓 ′(𝑡) = 0 \}\) para
\(𝑓 : 𝑡 ↦ 3𝑡⁴+4𝑡³−12𝑡²+7\).}{Calcular formalmente \textbackslash{}\{ 𝑡 ∈ R. 𝑓 ′(𝑡) = 0 \textbackslash{}\} para 𝑓 : 𝑡 ↦ 3𝑡⁴+4𝑡³−12𝑡²+7.}}\label{calcular-formalmente-ux1d461-r.-ux1d453-ux1d461-0-para-ux1d453-ux1d461-3ux1d4614ux1d46112ux1d4617.}

    Para resolver este problema, primero necesitamos calcular la derivada de
la función \(( f(t) = 3t^4 + 4t^3 - 12t^2 + 7 )\) y luego encontrar los
puntos donde la derivada es igual a cero. Estos puntos son los
candidatos para los mínimos y máximos locales de la función.

Vamos a proceder paso a paso:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calcular la derivada de ( f(t) ).
\item
  Encontrar los puntos donde la derivada es igual a cero.
\item
  Utilizar la función de descenso de gradiente modificada para encontrar
  estos puntos.
\end{enumerate}

\textbf{- Calcular la derivada de \(( f(t) )\)}

La derivada de \(( f(t) )\) es:

\([ f'(t) = \frac{d}{dt}(3t^4 + 4t^3 - 12t^2 + 7) = 12t^3 + 12t^2 - 24t ]\)

    \textbf{- Encontrar los puntos donde la derivada es igual a cero}

Queremos encontrar los puntos \(( t )\) donde \(( f'(t) = 0 )\). Esto se
traduce en resolver la ecuación:

\([ 12t^3 + 12t^2 - 24t = 0 ]\)

Podemos factorizar esta ecuación:

\([ 12t(t^2 + t - 2) = 0 ]\)

Esta factorización da tres posibles soluciones:

\([ 12t = 0 \quad \text{o} \quad t^2 + t - 2 = 0 ]\)

Resolviendo estas ecuaciones:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(( 12t = 0 )\) nos da \(( t = 0 )\).
\item
  \(( t^2 + t - 2 = 0 )\) se puede resolver usando la fórmula
  cuadrática:
\end{enumerate}

\([ t = \frac{-1 \pm \sqrt{1^2 - 4 \cdot 1 \cdot (-2)}}{2 \cdot 1} = \frac{-1 \pm \sqrt{1 + 8}}{2} = \frac{-1 \pm 3}{2} ]\)

Esto nos da dos soluciones:

\([ t = 1 \quad \text{y} \quad t = -2 ]\)

Por lo tanto, los puntos donde \(( f'(t) = 0 )\) son
\(( t = 0 ), ( t = 1 )\), y \(( t = -2 )\).

    \textbf{- Implementación en Python de la función descenso de gradiente
modificada para encontrar estos puntos}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} Derivada de la función f(t)}
\PY{k}{def} \PY{n+nf}{f\PYZus{}prima}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3} \PY{o}{+} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{l+m+mi}{24}\PY{o}{*}\PY{n}{t}

\PY{c+c1}{\PYZsh{} Descenso de gradiente adaptado para encontrar raíces de la derivada}
\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prima}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{gradiente} \PY{o}{=} \PY{n}{f\PYZus{}prima}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{gradiente}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergió después de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{gradiente}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcanzó el máximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}

\PY{c+c1}{\PYZsh{} Parámetros para el descenso de gradiente}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}6}

\PY{c+c1}{\PYZsh{} Puntos iniciales para encontrar las raíces de la derivada}
\PY{n}{puntos\PYZus{}iniciales} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Ejecutar el descenso de gradiente para cada punto inicial}
\PY{n}{raices} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{x0} \PY{o+ow}{in} \PY{n}{puntos\PYZus{}iniciales}\PY{p}{:}
    \PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prima}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
    \PY{n}{raices}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{raiz}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Todas las raíces:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{raices}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Convergió después de 0 iteraciones.
Raíz encontrada: 0
Convergió después de 0 iteraciones.
Raíz encontrada: 1
Convergió después de 0 iteraciones.
Raíz encontrada: -2
Todas las raíces: [0, 1, -2]
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Función \texttt{f\_prima}}: Calcula la derivada de ( f(t) ).
\item
  \textbf{Función \texttt{descenso\_gradiente\_raices}}: Aplica el
  descenso de gradiente para encontrar las raíces de la derivada de la
  función.
\item
  \textbf{Parámetros para el descenso de gradiente}: Definimos la tasa
  de aprendizaje y la tolerancia.
\item
  \textbf{Puntos iniciales}: Usamos los puntos ( t = 0 ), ( t = 1 ), y (
  t = -2 ) como puntos de partida para verificar las raíces.
\item
  \textbf{Ejecutar el descenso de gradiente}: Para cada punto inicial,
  ejecutamos el descenso de gradiente y almacenamos los resultados.
\end{enumerate}

    \subsubsection{\texorpdfstring{Con una tolerancia \(z = 10⁻¹²\) y un
valor inicial de \(x = 3\) aplicar su algoritmo con razón \(y = 10⁻¹\),
\(10⁻²\), \(10⁻³\) luego hacer lo mismo con \(x = 0\). Interpretar el
resultado.}{Con una tolerancia z = 10⁻¹² y un valor inicial de x = 3 aplicar su algoritmo con razón y = 10⁻¹, 10⁻², 10⁻³ luego hacer lo mismo con x = 0. Interpretar el resultado.}}\label{con-una-tolerancia-z-10-y-un-valor-inicial-de-x-3-aplicar-su-algoritmo-con-razuxf3n-y-10-10-10-luego-hacer-lo-mismo-con-x-0.-interpretar-el-resultado.}

    Para abordar este ejercicio, vamos a aplicar el algoritmo de descenso de
gradiente a la función \(( f(t) = 3t^4 + 4t^3 - 12t^2 + 7 )\) utilizando
diferentes tasas de aprendizaje \((\text{learning rates})\) y dos puntos
iniciales: \(( x = 3 )\) y \(( x = 0 )\). Utilizaremos una tolerancia de
\(( z = 10^{-12} )\).

Primero, recordemos la derivada de la función:

\([ f'(t) = 12t^3 + 12t^2 - 24t. ]\)

    \textbf{2.1.3.1 Implementación del Algoritmo}

Vamos a implementar el descenso de gradiente para encontrar las raíces
de \(( f'(t) )\) utilizando las tasas de aprendizaje dadas. Luego,
interpretaremos los resultados para \(( x = 3 )\) y \(( x = 0 )\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} Derivada de la función f(t)}
\PY{k}{def} \PY{n+nf}{f\PYZus{}prime}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3} \PY{o}{+} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{l+m+mi}{24}\PY{o}{*}\PY{n}{t}

\PY{c+c1}{\PYZsh{} Descenso de gradiente adaptado para encontrar raíces de la derivada}
\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{gradiente} \PY{o}{=} \PY{n}{f\PYZus{}prime}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{gradiente}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergió después de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{gradiente}
        
        \PY{c+c1}{\PYZsh{} Verificación de límites para prevenir overflow}
        \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mf}{1e10}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de x = }\PY{l+s+si}{\PYZob{}}\PY{n}{x}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ se volvió demasiado grande en la iteración }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{k+kc}{None}
        
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcanzó el máximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    \textbf{2.1.3.2 Aplicación del Algoritmo}

\begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(( x = 3 )\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-1} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}12}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{l+m+mi}{3}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{k}{if} \PY{n}{raiz} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No se encontró una raíz dentro de los límites permitidos.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 3 con tasa de aprendizaje = 0.1
El valor de x = -87049951065956.78 se volvió demasiado grande en la iteración 2.
No se encontró una raíz dentro de los límites permitidos.
    \end{Verbatim}

    \textbf{Interpretación}:

La tasa de aprendizaje \(( y = 10^{-1} )\) es demasiado alta. Esto hace
que las actualizaciones en x sean muy grandes, causando que los valores
se vuelvan extremadamente grandes en poco tiempo, lo que lleva a un
desbordamiento numérico. Este es un claro ejemplo de cómo una tasa de
aprendizaje demasiado grande puede desestabilizar el algoritmo de
descenso de gradiente.

\begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(( x = 3 )\)
\item
  \textbf{Tasa de aprendizaje} \(y=10^{-2}\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 3 con tasa de aprendizaje = 0.01
Convergió después de 31 iteraciones.
Raíz encontrada: -1.9999999999999882

    \end{Verbatim}

    \textbf{Interpretación}:

Con una tasa de aprendizaje de \(( y = 10^{-2} )\), el algoritmo es más
estable y converge rápidamente a una raíz cercana a \(-2\). Este valor
es uno de los puntos donde la derivada de la función original es cero,
lo que indica un mínimo o máximo local.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(( x = 3 )\)
\item
  \textbf{Tasa de aprendizaje} \(y=10^{-3}\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 3 con tasa de aprendizaje = 0.001
Convergió después de 831 iteraciones.
Raíz encontrada: 1.0000000000000275

    \end{Verbatim}

    \textbf{Interpretación}:

Con una tasa de aprendizaje aún más pequeña, \(( y = 10^{-3} )\), el
algoritmo converge de manera más lenta (requiriendo \(831\)
iteraciones). Sin embargo, alcanza un punto cercano a \(1\), que es otro
punto donde la derivada de la función original es cero. Esto demuestra
que una tasa de aprendizaje más pequeña puede llevar a una mayor
precisión, aunque a costa de más iteraciones.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(x = 0\)
\item
  \textbf{Tasa de aprendizaje} \(y = 10^{-2}\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 0 con tasa de aprendizaje = 0.1
Convergió después de 0 iteraciones.
Raíz encontrada: 0

    \end{Verbatim}

    \textbf{Interpretación}:

Al comenzar en \(( x = 0 )\), el valor inicial ya es un punto donde la
derivada de la función es cero. No se necesitan iteraciones adicionales
porque 0 es una raíz de la derivada. Esto muestra que el algoritmo
detecta correctamente que ya está en un punto estacionario.

    \textbf{Tasa de aprendizaje} \(y=10^{-1}\)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 0 con tasa de aprendizaje = 0.01
Convergió después de 0 iteraciones.
Raíz encontrada: 0

    \end{Verbatim}

    \textbf{Interpretación}:

Igual que con \(( y = 10^{-1} )\), el algoritmo reconoce que el punto
inicial x = 0 ya es una raíz de la derivada. No se requieren
actualizaciones adicionales.

    \textbf{Tasa de aprendizaje} \(y=10^{-3}\)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 0 con tasa de aprendizaje = 0.001
Convergió después de 0 iteraciones.
Raíz encontrada: 0

    \end{Verbatim}

    \textbf{Interpretación}:

Nuevamente, al comenzar en \(x = 0\), el valor inicial ya es una raíz de
la derivada. La tasa de aprendizaje no afecta el resultado en este caso
porque no se necesitan iteraciones adicionales.

    \textbf{2.1.3.3 Conclusión General}

\begin{itemize}
\tightlist
\item
  \textbf{Tasa de aprendizaje grande}: Puede llevar a desbordamientos
  numéricos o a oscilaciones alrededor de la raíz, como se observa con
  \(( y = 10^{-1} )\) cuando el valor inicial es \(( x = 3 )\).
\item
  \textbf{Tasa de aprendizaje moderada}: Proporciona un equilibrio entre
  velocidad y estabilidad, permitiendo una convergencia rápida y
  precisa, como se observa con \(( y = 10^{-2} )\) cuando el valor
  inicial es \(( x = 3 )\).
\item
  \textbf{Tasa de aprendizaje pequeña}: Garantiza una alta precisión,
  aunque a costa de un mayor número de iteraciones, como se observa con
  \(( y = 10^{-3} )\) cuando el valor inicial es \(( x = 3 )\).
\item
  \textbf{Valor inicial en la raíz}: Si el valor inicial es ya una raíz
  como \(( x = 0 )\), el algoritmo converge instantáneamente sin
  necesidad de iteraciones adicionales.
\end{itemize}

Estos resultados ilustran cómo la elección de la tasa de aprendizaje y
el punto inicial pueden influir significativamente en el comportamiento
y eficiencia del algoritmo de descenso de gradiente.

    \subsubsection{\texorpdfstring{{[}Repetir estos dos últimos apartados
con \(𝑓 : (𝑠, 𝑡) ↦ 𝑠² + 3𝑠𝑡 + 𝑡³ + 1\) y los valores iniciales x =
{[}-1,1{]},
{[}0,0{]}.{]}}{{[}Repetir estos dos últimos apartados con 𝑓 : (𝑠, 𝑡) ↦ 𝑠² + 3𝑠𝑡 + 𝑡³ + 1 y los valores iniciales x = {[}-1,1{]}, {[}0,0{]}.{]}}}\label{repetir-estos-dos-uxfaltimos-apartados-con-ux1d453-ux1d460-ux1d461-ux1d460-3ux1d460ux1d461-ux1d461-1-y-los-valores-iniciales-x--11-00.}

    \textbf{2.1.4.1: Calcular formalmente
\({ (s, t) ∈ R^{2} | f(s, t) = 0 }\)}

La función dada es: \([ f(s, t) = s^2 + 3st + t^3 + 1 ]\)

El gradiente de \(( f(s, t) )\) es:
\([ \nabla f(s, t) = \left( \frac{\partial f}{\partial s}, \frac{\partial f}{\partial t} \right) ]\)

Calculando las derivadas parciales:

\([ \frac{\partial f}{\partial s} = 2s + 3t ]\)
\([ \frac{\partial f}{\partial t} = 3s + 3t^2 ]\)

Queremos encontrar los puntos donde el gradiente es cero:

\([ 2s + 3t = 0 ]\)

\([ 3s + 3t^2 = 0 ]\)

Resolviendo estas ecuaciones simultáneamente:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  De la primera ecuación: \([ 2s + 3t = 0 \implies s = -\frac{3}{2}t ]\)
\item
  Sustituyendo \(( s = -\frac{3}{2}t )\) en la segunda ecuación:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \([ 3\left(-\frac{3}{2}t\right) + 3t^2 = 0 ]\)
\item
  \([ -\frac{9}{2}t + 3t^2 = 0 ]\)
\item
  \([ t(3t - \frac{9}{2}) = 0 ]\)
\item
  \([ t = 0 \text{ o } t = \frac{3}{2} ]\)
\end{itemize}

Para \(( t = 0 )\): \([ s = 0 ]\)

Para \(( t = \frac{3}{2} )\):
\([ s = -\frac{3}{2} \left(\frac{3}{2}\right) = -\frac{9}{4} ]\)

Entonces, los puntos críticos son: \([ (0, 0) ]\)
\([ \left(-\frac{9}{4}, \frac{3}{2}\right) ]\)

    \textbf{2.1.4.2 Aplicar el algoritmo de descenso de gradiente}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} Gradiente de la función f(s, t)}
\PY{k}{def} \PY{n+nf}{gradiente\PYZus{}f}\PY{p}{(}\PY{n}{st}\PY{p}{)}\PY{p}{:}
    \PY{n}{s}\PY{p}{,} \PY{n}{t} \PY{o}{=} \PY{n}{st}
    \PY{n}{df\PYZus{}ds} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{s} \PY{o}{+} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{t}
    \PY{n}{df\PYZus{}dt} \PY{o}{=} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{s} \PY{o}{+} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{df\PYZus{}ds}\PY{p}{,} \PY{n}{df\PYZus{}dt}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Descenso de gradiente adaptado para encontrar raíces del gradiente}
\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{grad\PYZus{}f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{x0}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{float}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{grad} \PY{o}{=} \PY{n}{grad\PYZus{}f}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergió después de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{grad}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcanzó el máximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial \([-1, 1]\)}
\item
  \textbf{Tasa de aprendizaje \(( y = 10^{-1} )\)}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}12}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.1
Convergió después de 302 iteraciones.
Raíz encontrada: [-2.25  1.5 ]

    \end{Verbatim}

    \textbf{Interpretación}:

La tasa de aprendizaje de \(0.1\) es moderada, permitiendo que el
algoritmo converja razonablemente rápido a la raíz \(([-2.25, 1.5])\).
Este punto es una de las soluciones donde el gradiente de la función es
cero. El número de iteraciones es relativamente bajo, lo que indica una
buena convergencia con esta tasa de aprendizaje.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial \([-1, 1]\)}
\item
  \textbf{Tasa de aprendizaje \(( y = 10^{-2} )\)}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.01
Convergió después de 3139 iteraciones.
Raíz encontrada: [-2.25  1.5 ]

    \end{Verbatim}

    \textbf{Interpretación}:

Con una tasa de aprendizaje de \(0.01\), el algoritmo converge más
lentamente que con una tasa de 0.1, pero aún llega a la misma raíz
\(([-2.25, 1.5])\). El número de iteraciones es significativamente mayor
debido a la menor tasa de aprendizaje, lo que resulta en pasos más
pequeños hacia la convergencia.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial \([-1, 1]\)}
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-3} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.001
Convergió después de 31558 iteraciones.
Raíz encontrada: [-2.25  1.5 ]

    \end{Verbatim}

    \textbf{Interpretación}: Con una tasa de aprendizaje aún más pequeña de
\(0.001\), el algoritmo requiere muchas más iteraciones para converger a
la misma raíz \(([-2.25, 1.5])\). Esto demuestra que una tasa de
aprendizaje muy baja resulta en una convergencia muy lenta, aunque sigue
siendo precisa.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(([0, 0])\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-1} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [0, 0] con tasa de aprendizaje = 0.1
Convergió después de 0 iteraciones.
Raíz encontrada: [0. 0.]

    \end{Verbatim}

    \textbf{Interpretación}:

El valor inicial \(([0, 0])\) ya es un punto donde el gradiente de la
función es cero. Por lo tanto, el algoritmo no necesita realizar ninguna
iteración adicional para encontrar la raíz. Esto muestra que el punto
inicial ya es una solución, independientemente de la tasa de
aprendizaje.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(([0, 0])\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-2} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [0, 0] con tasa de aprendizaje = 0.01
Convergió después de 0 iteraciones.
Raíz encontrada: [0. 0.]

    \end{Verbatim}

    \textbf{Interpretación}:

Igual que con la tasa de \(0.1\), el algoritmo reconoce que el punto
inicial \([0, 0])\) ya es una raíz de la función, por lo que no se
requieren iteraciones adicionales.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(([0, 0])\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-3} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Parámetros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Raíz encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [0, 0] con tasa de aprendizaje = 0.001
Convergió después de 0 iteraciones.
Raíz encontrada: [0. 0.]

    \end{Verbatim}

    \textbf{Interpretación}:

Al igual que con las tasas de aprendizaje mayores, el punto inicial
\(([0, 0])\) ya es una raíz, y el algoritmo no necesita realizar
iteraciones adicionales.

    \textbf{2.1.4.3 Conclusión General}

\begin{itemize}
\item
  \textbf{Tasa de aprendizaje y valor inicial \([-1, 1]\)}:
\item
  Una tasa de aprendizaje mayor (0.1) permite una convergencia más
  rápida con menos iteraciones.
\item
  Una tasa de aprendizaje moderada (0.01) resulta en una convergencia
  más lenta pero estable.
\item
  Una tasa de aprendizaje muy pequeña (0.001) lleva a una convergencia
  extremadamente lenta, aunque precisa.
\item
  Todos convergen a la misma raíz ({[}-2.25, 1.5{]}), que es un punto
  crítico de la función.
\item
  \textbf{Tasa de aprendizaje y valor inicial \([0, 0]\)}:
\item
  El valor inicial ({[}0, 0{]}) ya es un punto crítico donde el
  gradiente es cero.
\item
  Independientemente de la tasa de aprendizaje, el algoritmo reconoce
  inmediatamente que está en la raíz y no realiza iteraciones
  adicionales.
\item
  Esto muestra que cuando el punto inicial es ya un punto crítico, la
  tasa de aprendizaje no influye en el resultado.
\end{itemize}

Estos resultados ilustran cómo la elección de la tasa de aprendizaje
afecta la velocidad de convergencia del algoritmo de descenso de
gradiente y cómo los puntos críticos iniciales pueden simplificar la
convergencia.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
