\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Keep aspect ratio if custom image width or height is specified
    \setkeys{Gin}{keepaspectratio}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{tocbibind}
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{soul}      % strikethrough (\st) support for pandoc >= 3.0.0
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \begin{titlepage}
      % Encabezado de la Instituci√≥n
      \begin{center}
        \includegraphics[width=0.5\textwidth]{resources/viu.png}\\[2cm]
        \textbf{\Huge Actividad evaluada \#3}\\[1cm]
      \end{center}

      % Detalles del Trabajo
      \begin{center}
        \textbf{\Large 02MIAR}\\[0.5cm]
        \textbf{\large Matem√°ticas para la Inteligencia Artificial}\\[2cm]
        
        \textbf{\Large Profesor}\\[0.5cm]
        \textbf{\large Dr. Matthieu F.-W. Huber}\\[2cm]
        
        \textbf{\Large Grupo No. 11}\\[1cm]
        \textbf{\large Jonathan Mora}\\
        \textbf{\large Luis Jama Tello}\\
        \textbf{\large Blanca Santos Fern√°ndez}\\
        \textbf{\large Laura Betancourt Leal}\\[2cm]
        
        \textbf{\Large Junio 2024}
      \end{center}
  \end{titlepage}

% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@ges}{\let\PY@bf=\textbf\let\PY@it=\textit}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=black,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    \pagenumbering{gobble} % Para desactivar numeraci√≥n en las p√°ginas anteriores
    \textbf{\Large √çndice general}\\[0.5cm]
    \contentsline {section}{\numberline {1}Ejercicios acerca del determinante}{1}{section.1}
    \contentsline {subsection}{\numberline {1.1}Desarrollo de Laplace.}{1}{subsection.1.1}
    \contentsline {subsubsection}{\numberline {1.1.1}Deducir de la definici√≥n 4 el determinante en dimensi√≥n 0, 1 y 2.}{1}{subsubsection.1.1.1}
    \contentsline {paragraph}{Dimensi√≥n 0}{1}{section*.2}
    \contentsline {paragraph}{Dimensi√≥n 1}{1}{section*.3}
    \contentsline {paragraph}{Dimensi√≥n 2}{1}{section*.4}
    \contentsline {subsubsection}{\numberline {1.1.2}A partir de la definici√≥n 4, expresar el determinante de una matriz cuadrada recursivamente en funci√≥n de determinantes la matrices cuadradas de dimensi√≥n inferior.}{2}{subsubsection.1.1.2}
    \contentsline {subsubsection}{\numberline {1.1.3}Implementar en Python la definici√≥n as√≠ obtenida.}{3}{subsubsection.1.1.3}
    \contentsline {subsection}{\numberline {1.2}Ejercicio 2 : Eliminaci√≥n de Gauss--Jordan.}{6}{subsection.1.2}
    \contentsline {subsubsection}{\numberline {1.2.1}Deducir de la definici√≥n 4 el efecto que tiene en el determinante de una matriz sumar a una de sus columnas una combinaci√≥n lineal de las dem√°s.}{6}{subsubsection.1.2.1}
    \contentsline {subsubsection}{\numberline {1.2.2}A partir de la definici√≥n 4, proponer una estrategia para triangularizar una matriz sin cambiar su determinante e implementar en Python una definici√≥n alternativa del determinante. Indicaci√≥n: descomponer similarmente al ejercicio anterior.}{6}{subsubsection.1.2.2}
    \contentsline {subsubsection}{\numberline {1.2.3}Implementar en Python la definici√≥n as√≠ obtenida}{6}{subsubsection.1.2.3}
    \contentsline {subsection}{\numberline {1.3}Ejercicio 3: Comparaci√≥n.}{6}{subsection.1.3}
    \contentsline {subsubsection}{\numberline {1.3.1}Obtener la complejidad computacional de cada una de estas dos implementaciones.}{6}{subsubsection.1.3.1}
    \contentsline {subsubsection}{\numberline {1.3.2}Generar matrices aleatoriamente en dimensi√≥n \(n ‚àà\) \{ 2, 3, ¬∑ ¬∑ , 9, 10 \} y comparar el tiempo de ejecuci√≥n de cada una de estas dos implementaciones con la funci√≥n numpy.linalg.det (la funci√≥n determinante de la extensi√≥n num√©rica de Python al √°lgebra lineal). Indicaci√≥n: se puede utilizar la funci√≥n numpy.random.rand para generar los coeficientes aleatorios de sus matrices.}{7}{subsubsection.1.3.2}
    \contentsline {section}{\numberline {2}Ejercicios acerca del gradiente}{9}{section.2}
    \contentsline {subsection}{\numberline {2.1}Ejercicio 4 : M√©todo descenso del gradiente}{9}{subsection.2.1}
    \contentsline {subsubsection}{\numberline {2.1.1}Implementar en Python un algoritmo de descenso del gradiente (con un m√°ximo de m = \(10‚Åµ\) iteraciones) a partir de los siguientes argumentos tomados en ese orden:}{9}{subsubsection.2.1.1}
    \contentsline {subsubsection}{\numberline {2.1.2}Calcular formalmente \(\{ ùë° ‚àà R. ùëì ‚Ä≤(ùë°) = 0 \}\) para \(ùëì : ùë° ‚Ü¶ 3ùë°‚Å¥+4ùë°¬≥‚àí12ùë°¬≤+7\).}{11}{subsubsection.2.1.2}
    \contentsline {subsubsection}{\numberline {2.1.3}Con una tolerancia \(z = 10‚Åª¬π¬≤\) y un valor inicial de \(x = 3\) aplicar su algoritmo con raz√≥n \(y = 10‚Åª¬π\), \(10‚Åª¬≤\), \(10‚Åª¬≥\) luego hacer lo mismo con \(x = 0\). Interpretar el resultado.}{13}{subsubsection.2.1.3}
    \contentsline {subsubsection}{\numberline {2.1.4}\hyperref [toc0_]{Repetir estos dos √∫ltimos apartados con ùëì : (ùë†, ùë°) ‚Ü¶ ùë†¬≤ + 3ùë†ùë° + ùë°¬≥ + 1 y los valores iniciales x = [-1,1], [0,0].}}{17}{subsubsection.2.1.4}
    \clearpage
    \pagenumbering{arabic} % Para activar la numeraci√≥n en ar√°bigo
    \setcounter{page}{1}   % Para reiniciar la numeraci√≥n de p√°ginas desde 1

    \newpage
    \section{Ejercicios acerca del
determinante}\label{ejercicios-acerca-del-determinante}

\subsection{Desarrollo de Laplace.}\label{desarrollo-de-laplace.}

\subsubsection{Deducir de la definici√≥n 4 el determinante en dimensi√≥n
0, 1 y
2.}\label{deducir-de-la-definiciuxf3n-4-el-determinante-en-dimensiuxf3n-0-1-y-2.}

    \paragraph{Dimensi√≥n 0}\label{dimensiuxf3n-0}

En dimensi√≥n 0, una matriz es simplemente un escalar, y su determinante
es el propio escalar:

\begin{equation}
det(a) = a
\end{equation}

\paragraph{Dimensi√≥n 1}\label{dimensiuxf3n-1}

Una matriz es un solo vector (o escalar). Para un vector
\(\mathbb{v_1}\) en \(\mathbb{R}\), el determinante es simplemente el
valor absoluto del vector, ya que la √∫nica funci√≥n lineal antisim√©trica
es la identidad.

\begin{equation}
\det([v_1]) = v_1
\end{equation}

\paragraph{Dimensi√≥n 2}\label{dimensiuxf3n-2}

En dimensi√≥n 2 tenemos una matriz 2x2. Sea \(\mathbb{A}\) una matriz de
\(\mathbb{R}^2\):

\begin{equation}
A =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
\end{equation}

La funci√≥n lineal antisimetrica que cumple la definici√≥n 4 seria:

\begin{equation}
det(A) =ad-cd
\end{equation}

La matriz identidad de una matriz 2x2 es:

\begin{equation}
A =
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\end{equation}

Por lo que tenemos que el resultado del determinate sera dado por:

\begin{equation}
det(A) = 1 \cdot 1-0 \cdot 0 = 1
\end{equation}

    \subsubsection{A partir de la definici√≥n 4, expresar el determinante de
una matriz cuadrada recursivamente en funci√≥n de determinantes la
matrices cuadradas de dimensi√≥n
inferior.}\label{a-partir-de-la-definiciuxf3n-4-expresar-el-determinante-de-una-matriz-cuadrada-recursivamente-en-funciuxf3n-de-determinantes-la-matrices-cuadradas-de-dimensiuxf3n-inferior.}

Indicaci√≥n: para cada \(n ‚àà N\), distribuir (por linealidad en las
columnas) sobre la descomposici√≥n

\begin{equation}
\begin{bmatrix}
\lambda & \omega \\
v & A
\end{bmatrix} = \begin{bmatrix}
\lambda \cdot 1 + 0 & \omega \\
\lambda \cdot 0 + v & A
\end{bmatrix}.
\end{equation}

de una matriz cuadrada de dimensi√≥n ùëõ + 1, siendo \(n ‚àà N\) y\\
- \(ùúÜ\) un coeficiente real,\\
- \(ùë£\) un vector de dimensi√≥n ùëõ (una columna de ùëõ coeficientes
reales),\\
- \(ùúî\) un covector de la misma dimensi√≥n (una fila de ùëõ
coeficientes),\\
- \(ùê¥\) una matriz cuadrada de la misma dimensi√≥n (con ùëõ2
coeficientes),\\
luego proceder del mismo modo con\\
- los dem√°s coeficientes de esa primera columna,\\
- con cada columna.\\

    \textbf{Resoluci√≥n ejercicio :}\\
Diremos que n = 2 por lo que tendremos las siguientes igualdades :

\begin{equation}
A =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix}
\end{equation}

\begin{equation}
v =
\begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}
\end{equation}

\begin{equation}
w =
\begin{bmatrix}
w_1 &&
w_2
\end{bmatrix}
\end{equation}

Usaremos una matriz M de dimensi√≥n 3√ó3 como ejemplo concreto donde
estaran los valores \(ùúÜ, ùë£, ùúî\) y los valores de A

\begin{equation}
M =
\begin{bmatrix}
\lambda & w_1 & w_2 \\
v_1 & a_{11} & a_{12} \\
v_2 & a_{21} & a_{22} \\
\end{bmatrix}
\end{equation}

    \textbf{Paso 1: Expansi√≥n por Cofactores}

Para calcular el determinante de M, vamos a usar cofactores. Esto
significa que vamos a descomponer el determinante de la matriz grande en
t√©rminos de los determinantes de matrices m√°s peque√±as.

\begin{equation}
det(M) =
\lambda \cdot
det
 \Biggl(
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix} 
 \Biggl)
-
w_1 \cdot
det
\Biggl(
\begin{bmatrix}
v_1 & a_{12} \\
v_2 & a_{22} \\
\end{bmatrix}
\Biggl)
+
w_2 \cdot 
det
\Biggl(
\begin{bmatrix}
v_1 & a_{11} \\
v_2 & a_{21} \\
\end{bmatrix} 
\Biggl)
\end{equation}

    Entonces tenemos 3 sub matrices :

\begin{equation}
M_1 =
\begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22} \\
\end{bmatrix}
\end{equation}

\begin{equation}
M_2 =
\begin{bmatrix}
v_1 & a_{12} \\
v_2 & a_{22} \\
\end{bmatrix}
\end{equation}

\begin{equation}
M_3 =
\begin{bmatrix}
v_1 & a_{11} \\
v_2 & a_{21} \\
\end{bmatrix}
\end{equation}

    Si simplificamos la ecuaci√≥n tendr√≠amos que:

\begin{equation}
det(M) =
\lambda \cdot
det(A)
-
w_1 \cdot det(M_2)
+
w_2 \cdot det(M_2)
\end{equation}

    Por tanto tenemos que en n:

\begin{equation}
M =
\begin{pmatrix}
\lambda & \omega_1 & \omega_2 & \cdots & \omega_n \\
v_1 & a_{11} & a_{12} & \cdots & a_{1n} \\
v_2 & a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
v_n & a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}
\end{equation}

El determinante esta dado por:

\begin{equation}
det(M) =
\lambda \cdot
det(A)
-
w_1 \cdot det(M_1)
+
w_2 \cdot det(M_2) ... + (-1)^{1+i} \cdot w_i \cdot det(A_i)
\end{equation}

Que es similar a decir que:

\begin{equation}
\det\begin{pmatrix}
\lambda & \omega \\
v & A
\end{pmatrix}
= \lambda \cdot \det(A) 
+
\sum_{i=1}^n \omega_i \cdot (-1)^{1+i} \cdot \det(A_i)
\end{equation}

    \subsubsection{Implementar en Python la definici√≥n as√≠
obtenida.}\label{implementar-en-python-la-definiciuxf3n-asuxed-obtenida.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k}{def} \PY{n+nf}{calcular\PYZus{}determinante\PYZus{}combinado}\PY{p}{(}\PY{n}{lambda\PYZus{}val}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Convertir las entradas a matrices numpy}
    \PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{v}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{omega} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{omega}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{A}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Construir la matriz combinada}
    \PY{n}{primera\PYZus{}fila} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{p}{[}\PY{n}{lambda\PYZus{}val}\PY{p}{]}\PY{p}{,} \PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
    \PY{n}{resto\PYZus{}filas} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{v}\PY{p}{,} \PY{n}{A}\PY{p}{)}\PY{p}{)}
    \PY{n}{matriz\PYZus{}combinada} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{(}\PY{n}{primera\PYZus{}fila}\PY{p}{,} \PY{n}{resto\PYZus{}filas}\PY{p}{)}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Matriz compuesta:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{matriz\PYZus{}combinada}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Calcular el determinante de la matriz combinada}
    \PY{n}{determinante\PYZus{}combinado} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{matriz\PYZus{}combinada}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Verificaci√≥n del determinante seg√∫n la f√≥rmula}
    \PY{n}{determinante\PYZus{}A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A}\PY{p}{)}
    \PY{n}{suma\PYZus{}terminos} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{cofactores} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Crear una submatriz A\PYZus{}i eliminando la primera fila y la columna i}
        \PY{n}{columna\PYZus{}a\PYZus{}eliminar} \PY{o}{=} \PY{n}{i} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{;}
        \PY{n}{num\PYZus{}filas}\PY{p}{,} \PY{n}{num\PYZus{}columnas} \PY{o}{=} \PY{n}{matriz\PYZus{}combinada}\PY{o}{.}\PY{n}{shape}
        \PY{n}{A\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{matriz\PYZus{}combinada}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Eliminar la primera fila}
        \PY{n}{A\PYZus{}i} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{A\PYZus{}i}\PY{p}{,} \PY{n}{columna\PYZus{}a\PYZus{}eliminar}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} Eliminar la columna i}
        \PY{c+c1}{\PYZsh{}print(f\PYZdq{}(A\PYZus{}\PYZob{}i+1\PYZcb{}\PYZdq{})}
        \PY{c+c1}{\PYZsh{}print(A\PYZus{}i)}
        \PY{c+c1}{\PYZsh{}print(f\PYZdq{}w\PYZus{}\PYZob{}i+1\PYZcb{}\PYZdq{}, omega[0][i])}
        \PY{n}{cofactor} \PY{o}{=} \PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{i}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A\PYZus{}i}\PY{p}{)}
        \PY{n}{cofactores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cofactor}\PY{p}{)}
        \PY{n}{suma\PYZus{}terminos} \PY{o}{+}\PY{o}{=} \PY{n}{cofactor}
        \PY{c+c1}{\PYZsh{}print(f\PYZdq{}Cofactor \PYZob{}i+1\PYZcb{}: omega[\PYZob{}i\PYZcb{}] * (\PYZhy{}1)\PYZca{}\PYZob{}1+i\PYZcb{} * det(A\PYZus{}\PYZob{}i+1\PYZcb{}) = \PYZob{}omega[0][i]\PYZcb{} * (\PYZhy{}1)\PYZca{}\PYZob{}1+i\PYZcb{} * \PYZob{}np.linalg.det(A\PYZus{}i)\PYZcb{} = \PYZob{}cofactor\PYZcb{}\PYZdq{})}
    
    \PY{n}{determinante\PYZus{}verificado} \PY{o}{=} \PY{n}{lambda\PYZus{}val} \PY{o}{*} \PY{n}{determinante\PYZus{}A} \PY{o}{+} \PY{n}{suma\PYZus{}terminos}
    
    \PY{c+c1}{\PYZsh{} Mostrar cada uno de los cofactores y sus resultados}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Cofactores:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} Mostrar la f√≥rmula completa en n√∫meros}
    \PY{n}{formula} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{lambda\PYZus{}val}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * det(A) + }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * (\PYZhy{}1)\PYZca{}}\PY{l+s+si}{\PYZob{}}\PY{l+m+mi}{1}\PY{o}{+}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * det(A\PYZus{}}\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{omega}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{]}\PY{p}{)}
    \PY{n}{formula\PYZus{}numerica} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{lambda\PYZus{}val}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ * }\PY{l+s+si}{\PYZob{}}\PY{n}{determinante\PYZus{}A}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ + }\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{p}{[}\PY{n+nb}{str}\PY{p}{(}\PY{n}{cofactor}\PY{p}{)} \PY{k}{for} \PY{n}{cofactor} \PY{o+ow}{in} \PY{n}{cofactores}\PY{p}{]}\PY{p}{)}
    
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F√≥rmula simb√≥lica: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{formula}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{F√≥rmula num√©rica: }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{formula\PYZus{}numerica}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{matriz\PYZus{}combinada}\PY{p}{,} \PY{n}{determinante\PYZus{}combinado}\PY{p}{,} \PY{n}{determinante\PYZus{}verificado}

\PY{c+c1}{\PYZsh{} Ejemplo de uso}
\PY{k}{def} \PY{n+nf}{generate\PYZus{}random\PYZus{}parameters}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{)}\PY{p}{:}

    \PY{c+c1}{\PYZsh{} 1. Generar n como un n√∫mero entero aleatorio entre 2 y 5}
    \PY{n}{n} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 2. Generar lambda como un n√∫mero entero aleatorio entre \PYZhy{}10 y 10}
    \PY{n}{lambda\PYZus{}} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 3. Generar un vector columna v de dimensi√≥n n con valores enteros}
    \PY{n}{v} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{,} \PY{n}{n}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 4. Generar un covector omega de dimensi√≥n n con valores enteros}
    \PY{n}{omega} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{,} \PY{n}{n}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{} 5. Generar una matriz cuadrada A de dimensi√≥n n x n con valores enteros}
    \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{inf\PYZus{}lim}\PY{p}{,} \PY{n}{sup\PYZus{}lim}\PY{p}{,} \PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{)}
    
    \PY{k}{return} \PY{n}{n}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A}



\PY{n}{n}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A} \PY{o}{=} \PY{n}{generate\PYZus{}random\PYZus{}parameters}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{11}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{n:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{n}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lambda:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{lambda\PYZus{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{v:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{v}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{omega:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{omega}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{A}\PY{p}{)}

\PY{n}{matriz\PYZus{}combinada}\PY{p}{,} \PY{n}{determinante\PYZus{}combinado}\PY{p}{,} \PY{n}{determinante\PYZus{}verificado} \PY{o}{=} \PY{n}{calcular\PYZus{}determinante\PYZus{}combinado}\PY{p}{(}\PY{n}{lambda\PYZus{}}\PY{p}{,} \PY{n}{v}\PY{p}{,} \PY{n}{omega}\PY{p}{,} \PY{n}{A}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Determinante:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{determinante\PYZus{}combinado}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Verificacion:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{determinante\PYZus{}verificado}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
n: 5
lambda: 0
v: [ 5 -9 -6  7  0]
omega: [-5  6 -6 11  1]
A:
[[  3   6  13   2  -9]
 [  1  -4   4   1  -5]
 [ -7   3   1   8   5]
 [ -2  14   5   1  -3]
 [ -2 -11   4  12  13]]
Matriz compuesta:
[[  0  -5   6  -6  11   1]
 [  5   3   6  13   2  -9]
 [ -9   1  -4   4   1  -5]
 [ -6  -7   3   1   8   5]
 [  7  -2  14   5   1  -3]
 [  0  -2 -11   4  12  13]]
Cofactores:
F√≥rmula simb√≥lica:  0 * det(A) + -5 * (-1)\^{}1 * det(A\_1) + 6 * (-1)\^{}2 * det(A\_2)
+ -6 * (-1)\^{}3 * det(A\_3) + 11 * (-1)\^{}4 * det(A\_4) + 1 * (-1)\^{}5 * det(A\_5)
F√≥rmula num√©rica:  0 * 690.0000000000016 + 140600.00000000006 +
-46212.00000000012 + -131280.0000000005 + -336688.00000000093 +
10678.000000000053
Determinante: -362902.00000000076
Verificacion: -362902.00000000146
    \end{Verbatim}

    \subsection{Ejercicio 2 : Eliminaci√≥n de
Gauss--Jordan.}\label{ejercicio-2-eliminaciuxf3n-de-gaussjordan.}

\subsubsection{Deducir de la definici√≥n 4 el efecto que tiene en el
determinante de una matriz sumar a una de sus columnas una combinaci√≥n
lineal de las
dem√°s.}\label{deducir-de-la-definiciuxf3n-4-el-efecto-que-tiene-en-el-determinante-de-una-matriz-sumar-a-una-de-sus-columnas-una-combinaciuxf3n-lineal-de-las-demuxe1s.}

    

    \subsubsection{A partir de la definici√≥n 4, proponer una estrategia para
triangularizar una matriz sin cambiar su determinante e implementar en
Python una definici√≥n alternativa del determinante. Indicaci√≥n:
descomponer similarmente al ejercicio
anterior.}\label{a-partir-de-la-definiciuxf3n-4-proponer-una-estrategia-para-triangularizar-una-matriz-sin-cambiar-su-determinante-e-implementar-en-python-una-definiciuxf3n-alternativa-del-determinante.-indicaciuxf3n-descomponer-similarmente-al-ejercicio-anterior.}

    

    \subsubsection{Implementar en Python la definici√≥n as√≠
obtenida}\label{implementar-en-python-la-definiciuxf3n-asuxed-obtenida}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

    \subsection{Ejercicio 3:
Comparaci√≥n.}\label{ejercicio-3-comparaciuxf3n.}

\subsubsection{Obtener la complejidad computacional de cada una de estas
dos
implementaciones.}\label{obtener-la-complejidad-computacional-de-cada-una-de-estas-dos-implementaciones.}

    \begin{itemize}
\item
  Calcular complejidad computacional ejercicio: Determinante Matriz
  Combinada
\item
  Funci√≥n \textbf{generate\_random\_parameters(inf\_lim, sup\_lim)}:\\
  Tiene complejidad computacional \textbf{O(n¬≤)} dado que las l√≠neas que
  generan vectores y matrices (puntos 3, 4 y 5) tienen complejidades
  O(n) y O(n¬≤), respectivamente, y el resto de las l√≠neas O(1), la
  complejidad total del c√≥digo est√° dominada por la generaci√≥n de la
  matriz 'A', es decir \textbf{O(n¬≤)}.
\item
  Funci√≥n \textbf{calcular\_determinante\_combinado(lambda\_val, v,
  omega, A)}:\\
  La funci√≥n tiene la siguiente estructura:

  \begin{itemize}
  \tightlist
  \item
    Construcci√≥n de la matriz combinada: Tiene un costo de O(n), donde n
    es el tama√±o del vector v y la matriz A (asumiendo que omega tiene
    la misma longitud que v).
  \item
    C√°lculo del determinante de la matriz combinada: Utilizando
    np.linalg.det, el costo es O(n\^{}3) en el peor de los casos para
    matrices cuadradas.
  \item
    C√°lculo de los cofactores: Se realiza un ciclo for de tama√±o n (el
    tama√±o de omega). Dentro del ciclo, se calcula el determinante de
    una submatriz de tama√±o (n-1) x (n-1), con un costo de
    O((n-1)\^{}3).
  \end{itemize}

  Por lo tanto, el costo de calcular los cofactores es O(n *
  (n-1)\^{}3), que es equivalente a O(n\^{}4). Verificaci√≥n del
  determinante: Tiene un costo de O(n), ya que se realiza una suma de n
  t√©rminos.
\end{itemize}

\begin{quote}
\emph{"La complejidad computacional total del c√≥digo es O(n\^{}4), lo
que significa que el tiempo de ejecuci√≥n crece con la cuarta potencia
del tama√±o de la matriz. Este es un tiempo de ejecuci√≥n relativamente
alto, especialmente para matrices grandes."}
\end{quote}

    \subsubsection{\texorpdfstring{Generar matrices aleatoriamente en
dimensi√≥n \(n ‚àà\) \{ 2, 3, ¬∑ ¬∑ , 9, 10 \} y comparar el tiempo de
ejecuci√≥n de cada una de estas dos implementaciones con la funci√≥n
numpy.linalg.det (la funci√≥n determinante de la extensi√≥n num√©rica de
Python al √°lgebra lineal). Indicaci√≥n: se puede utilizar la funci√≥n
numpy.random.rand para generar los coeficientes aleatorios de sus
matrices.}{Generar matrices aleatoriamente en dimensi√≥n n ‚àà \{ 2, 3, ¬∑ ¬∑ , 9, 10 \} y comparar el tiempo de ejecuci√≥n de cada una de estas dos implementaciones con la funci√≥n numpy.linalg.det (la funci√≥n determinante de la extensi√≥n num√©rica de Python al √°lgebra lineal). Indicaci√≥n: se puede utilizar la funci√≥n numpy.random.rand para generar los coeficientes aleatorios de sus matrices.}}\label{generar-matrices-aleatoriamente-en-dimensiuxf3n-n-2-3-9-10-y-comparar-el-tiempo-de-ejecuciuxf3n-de-cada-una-de-estas-dos-implementaciones-con-la-funciuxf3n-numpy.linalg.det-la-funciuxf3n-determinante-de-la-extensiuxf3n-numuxe9rica-de-python-al-uxe1lgebra-lineal.-indicaciuxf3n-se-puede-utilizar-la-funciuxf3n-numpy.random.rand-para-generar-los-coeficientes-aleatorios-de-sus-matrices.}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{timeit}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{det\PYZus{}laplace}\PY{p}{(}\PY{n}{matriz}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n+nb}{float}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Calcula el determinante de una matriz usando la regla de Laplace.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        matriz: Es la matriz n x n para la cual se calcula el determinante}

\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    float}
\PY{l+s+sd}{        El determinante de la matriz}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}

  \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{matriz}\PY{p}{)}
  
  \PY{k}{if} \PY{n}{n} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
    \PY{k}{return} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
  \PY{k}{elif} \PY{n}{n} \PY{o}{==} \PY{l+m+mi}{2}\PY{p}{:}
    \PY{k}{return} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
  \PY{k}{else}\PY{p}{:}
    \PY{n}{det} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
      \PY{n}{submatriz} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{matriz}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{j}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
      \PY{n}{det} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{n}{j} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{*} \PY{n}{det\PYZus{}laplace}\PY{p}{(}\PY{n}{submatriz}\PY{p}{)}
    \PY{k}{return} \PY{n}{det}


\PY{k}{def} \PY{n+nf}{det\PYZus{}gauss\PYZus{}jordan}\PY{p}{(}\PY{n}{matriz}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{ndarray}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n+nb}{float}\PY{p}{:}
\PY{+w}{  }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}Calcula el determinante de una matriz usando el m√©todo de eliminaci√≥n de Gauss\PYZhy{}Jordan.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Parameters}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{        matriz: Es la matriz n x n para la cual se calcula el determinante}

\PY{l+s+sd}{    Returns}
\PY{l+s+sd}{    \PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}\PYZhy{}}
\PY{l+s+sd}{    float}
\PY{l+s+sd}{        El determinante de la matriz}
\PY{l+s+sd}{  \PYZdq{}\PYZdq{}\PYZdq{}}

  \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{matriz}\PY{p}{)}
  \PY{n}{det} \PY{o}{=} \PY{l+m+mi}{1}

  \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
    \PY{n}{pivot} \PY{o}{=} \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
    \PY{k}{if} \PY{n}{pivot} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
      \PY{k}{return} \PY{l+m+mi}{0}
    
    \PY{n}{det}  \PY{o}{*}\PY{o}{=} \PY{n}{pivot}
    \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{/}\PY{o}{=} \PY{n}{pivot}

    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n}\PY{p}{)}\PY{p}{:}
      \PY{n}{factor} \PY{o}{=} \PY{n}{matriz}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{[}\PY{n}{i}\PY{p}{]}
      \PY{n}{matriz}\PY{p}{[}\PY{n}{j}\PY{p}{]}  \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{factor} \PY{o}{*} \PY{n}{matriz}\PY{p}{[}\PY{n}{i}\PY{p}{]}
  \PY{k}{return} \PY{n}{det}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Dimensi√≥n}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{| T.Laplace (ms)}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{17}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{| T.Gauss\PYZhy{}Jordan (ms)}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{22}\PY{p}{)} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{| T.numpy.linalg.det (ms)}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}}\PY{l+s+s2}{\PYZdq{}} \PY{o}{*} \PY{l+m+mi}{75}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Genera matrices aleatorias de tama√±o n x n}
\PY{k}{for} \PY{n}{n} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{11}\PY{p}{)}\PY{p}{:}
  \PY{n}{A} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{n}\PY{p}{)}

  \PY{c+c1}{\PYZsh{} Calcula el tiempo de ejecuci√≥n para cada algoritmo}
  \PY{n}{laplace\PYZus{}time} \PY{o}{=} \PY{n}{timeit}\PY{o}{.}\PY{n}{timeit}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{det\PYZus{}laplace}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{,} \PY{n}{number}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{1000}
  \PY{n}{gauss\PYZus{}jordan\PYZus{}time} \PY{o}{=} \PY{n}{timeit}\PY{o}{.}\PY{n}{timeit}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{det\PYZus{}gauss\PYZus{}jordan}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{,} \PY{n}{number}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  \PY{o}{*} \PY{l+m+mi}{1000}
  \PY{n}{numpy\PYZus{}time} \PY{o}{=} \PY{n}{timeit}\PY{o}{.}\PY{n}{timeit}\PY{p}{(}\PY{k}{lambda}\PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{det}\PY{p}{(}\PY{n}{A}\PY{p}{)}\PY{p}{,} \PY{n}{number}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}  \PY{o}{*} \PY{l+m+mi}{1000}

  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+si}{\PYZob{}}\PY{n+nb}{str}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ | }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{format}\PY{p}{(}\PY{n}{laplace\PYZus{}time}\PY{p}{,}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.6f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{14}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ | }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{format}\PY{p}{(}\PY{n}{gauss\PYZus{}jordan\PYZus{}time}\PY{p}{,}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.6f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{ljust}\PY{p}{(}\PY{l+m+mi}{19}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ | }\PY{l+s+si}{\PYZob{}}\PY{n+nb}{format}\PY{p}{(}\PY{n}{numpy\PYZus{}time}\PY{p}{,}\PY{+w}{ }\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.6f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
  
  
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Dimensi√≥n | T.Laplace (ms) | T.Gauss-Jordan (ms) | T.numpy.linalg.det (ms)
---------------------------------------------------------------------------
2         | 0.009608       | 0.046513            | 0.035703
3         | 0.410473       | 0.042401            | 0.024766
4         | 0.830461       | 0.063236            | 0.026046
5         | 2.359853       | 0.090583            | 0.040796
6         | 13.553588      | 0.130150            | 0.036440
7         | 80.875877      | 0.162143            | 0.046795
8         | 643.808716     | 0.210978            | 0.045501
9         | 5897.031685    | 0.237919            | 0.040963
10        | 58559.983033   | 0.291078            | 0.039791
    \end{Verbatim}

    \section{Ejercicios acerca del
gradiente}\label{ejercicios-acerca-del-gradiente}

\subsection{Ejercicio 4 : M√©todo descenso del
gradiente}\label{ejercicio-4-muxe9todo-descenso-del-gradiente}

Con el prop√≥sito de aproximar un m√≠nimo local de una funci√≥n real de
varias variables reales, el m√©todo de descenso de gradiente consiste en
iterar una marcha (positivamente) proporcional al (opuesto del)
gradiente desde un valor inicial, con la intuici√≥n de `seguir el agua'
hasta dar con el valle.

\subsubsection{\texorpdfstring{Implementar en Python un algoritmo de
descenso del gradiente (con un m√°ximo de m = \(10‚Åµ\) iteraciones) a
partir de los siguientes argumentos tomados en ese
orden:}{Implementar en Python un algoritmo de descenso del gradiente (con un m√°ximo de m = 10‚Åµ iteraciones) a partir de los siguientes argumentos tomados en ese orden:}}\label{implementar-en-python-un-algoritmo-de-descenso-del-gradiente-con-un-muxe1ximo-de-m-10-iteraciones-a-partir-de-los-siguientes-argumentos-tomados-en-ese-orden}

\begin{itemize}
\tightlist
\item
  la funci√≥n f cuyo m√≠nimo local se propone aproximar,\\
\item
  el valor inicial x desde el que empieza la marcha,\\
\item
  la raz√≥n geom√©trica o coeficiente de proporcionalidad y,\\
\item
  el par√°metro de tolerancia z para finalizar cuando el gradiente de la
  funci√≥n f caiga dentro de esa tolerancia.\\
\end{itemize}

Indicaci√≥n: empezar por implementar el gradiente grad(f) de la funci√≥n
f.\\

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{k}{def} \PY{n+nf}{gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{h}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Aproximaci√≥n del gradiente de f en el punto x usando diferencias finitas.}

\PY{l+s+sd}{    f: La funci√≥n de la cual se va a calcular el gradiente.}
\PY{l+s+sd}{    x: El punto en el cual se va a calcular el gradiente.}
\PY{l+s+sd}{    h: Un peque√±o incremento para calcular las diferencias finitas.}
\PY{l+s+sd}{    return: El gradiente de f en x.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \PY{n}{gradiente} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{n}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
        \PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{x1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{x1}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{n}{h}
        \PY{n}{gradiente}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{f}\PY{p}{(}\PY{n}{x1}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{f}\PY{p}{(}\PY{n}{x0}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{h}
    \PY{k}{return} \PY{n}{gradiente}

\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Algoritmo de descenso de gradiente para encontrar el m√≠nimo de una funci√≥n f.}

\PY{l+s+sd}{    :param f: La funci√≥n cuyo m√≠nimo local se desea encontrar.}
\PY{l+s+sd}{    :param x0: El punto inicial desde donde comienza la b√∫squeda.}
\PY{l+s+sd}{    :param tasa\PYZus{}aprendizaje: La raz√≥n geom√©trica o coeficiente de proporcionalidad.}
\PY{l+s+sd}{    :param tolerancia: El par√°metro de tolerancia para finalizar cuando el gradiente de f est√© dentro de esa tolerancia.}
\PY{l+s+sd}{    :param max\PYZus{}iter: El n√∫mero m√°ximo de iteraciones.}
\PY{l+s+sd}{    :return: El punto que minimiza la funci√≥n f.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{x} \PY{o}{=} \PY{n}{x0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{grad} \PY{o}{=} \PY{n}{gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergi√≥ despu√©s de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{grad}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcanz√≥ el m√°ximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Ejemplo de uso:}
\PY{c+c1}{\PYZsh{} Definimos una funci√≥n cuadr√°tica simple para demostrar el descenso de gradiente.}
\PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}

\PY{c+c1}{\PYZsh{} Punto inicial}
\PY{n}{x0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\PY{c+c1}{\PYZsh{} Raz√≥n geom√©trica o coeficiente de proporcionalidad}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}
\PY{c+c1}{\PYZsh{} Tolerancia}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}6}

\PY{c+c1}{\PYZsh{} Ejecutar el descenso de gradiente}
\PY{n}{punto\PYZus{}minimo} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente}\PY{p}{(}\PY{n}{f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Punto m√≠nimo encontrado:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{punto\PYZus{}minimo}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Convergi√≥ despu√©s de 0 iteraciones.
Punto m√≠nimo encontrado: [10 10]
    \end{Verbatim}

    \subsubsection{\texorpdfstring{Calcular formalmente
\(\{ ùë° ‚àà R. ùëì ‚Ä≤(ùë°) = 0 \}\) para
\(ùëì : ùë° ‚Ü¶ 3ùë°‚Å¥+4ùë°¬≥‚àí12ùë°¬≤+7\).}{Calcular formalmente \textbackslash{}\{ ùë° ‚àà R. ùëì ‚Ä≤(ùë°) = 0 \textbackslash{}\} para ùëì : ùë° ‚Ü¶ 3ùë°‚Å¥+4ùë°¬≥‚àí12ùë°¬≤+7.}}\label{calcular-formalmente-ux1d461-r.-ux1d453-ux1d461-0-para-ux1d453-ux1d461-3ux1d4614ux1d46112ux1d4617.}

    Para resolver este problema, primero necesitamos calcular la derivada de
la funci√≥n \(( f(t) = 3t^4 + 4t^3 - 12t^2 + 7 )\) y luego encontrar los
puntos donde la derivada es igual a cero. Estos puntos son los
candidatos para los m√≠nimos y m√°ximos locales de la funci√≥n.

Vamos a proceder paso a paso:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calcular la derivada de ( f(t) ).
\item
  Encontrar los puntos donde la derivada es igual a cero.
\item
  Utilizar la funci√≥n de descenso de gradiente modificada para encontrar
  estos puntos.
\end{enumerate}

\textbf{- Calcular la derivada de \(( f(t) )\)}

La derivada de \(( f(t) )\) es:

\([ f'(t) = \frac{d}{dt}(3t^4 + 4t^3 - 12t^2 + 7) = 12t^3 + 12t^2 - 24t ]\)

    \textbf{- Encontrar los puntos donde la derivada es igual a cero}

Queremos encontrar los puntos \(( t )\) donde \(( f'(t) = 0 )\). Esto se
traduce en resolver la ecuaci√≥n:

\([ 12t^3 + 12t^2 - 24t = 0 ]\)

Podemos factorizar esta ecuaci√≥n:

\([ 12t(t^2 + t - 2) = 0 ]\)

Esta factorizaci√≥n da tres posibles soluciones:

\([ 12t = 0 \quad \text{o} \quad t^2 + t - 2 = 0 ]\)

Resolviendo estas ecuaciones:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(( 12t = 0 )\) nos da \(( t = 0 )\).
\item
  \(( t^2 + t - 2 = 0 )\) se puede resolver usando la f√≥rmula
  cuadr√°tica:
\end{enumerate}

\([ t = \frac{-1 \pm \sqrt{1^2 - 4 \cdot 1 \cdot (-2)}}{2 \cdot 1} = \frac{-1 \pm \sqrt{1 + 8}}{2} = \frac{-1 \pm 3}{2} ]\)

Esto nos da dos soluciones:

\([ t = 1 \quad \text{y} \quad t = -2 ]\)

Por lo tanto, los puntos donde \(( f'(t) = 0 )\) son
\(( t = 0 ), ( t = 1 )\), y \(( t = -2 )\).

    \textbf{- Implementaci√≥n en Python de la funci√≥n descenso de gradiente
modificada para encontrar estos puntos}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} Derivada de la funci√≥n f(t)}
\PY{k}{def} \PY{n+nf}{f\PYZus{}prima}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3} \PY{o}{+} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{l+m+mi}{24}\PY{o}{*}\PY{n}{t}

\PY{c+c1}{\PYZsh{} Descenso de gradiente adaptado para encontrar ra√≠ces de la derivada}
\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prima}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{gradiente} \PY{o}{=} \PY{n}{f\PYZus{}prima}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{gradiente}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergi√≥ despu√©s de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{gradiente}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcanz√≥ el m√°ximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}

\PY{c+c1}{\PYZsh{} Par√°metros para el descenso de gradiente}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}6}

\PY{c+c1}{\PYZsh{} Puntos iniciales para encontrar las ra√≠ces de la derivada}
\PY{n}{puntos\PYZus{}iniciales} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{]}

\PY{c+c1}{\PYZsh{} Ejecutar el descenso de gradiente para cada punto inicial}
\PY{n}{raices} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{x0} \PY{o+ow}{in} \PY{n}{puntos\PYZus{}iniciales}\PY{p}{:}
    \PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prima}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
    \PY{n}{raices}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{raiz}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Todas las ra√≠ces:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{raices}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Convergi√≥ despu√©s de 0 iteraciones.
Ra√≠z encontrada: 0
Convergi√≥ despu√©s de 0 iteraciones.
Ra√≠z encontrada: 1
Convergi√≥ despu√©s de 0 iteraciones.
Ra√≠z encontrada: -2
Todas las ra√≠ces: [0, 1, -2]
    \end{Verbatim}

    \begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Funci√≥n \texttt{f\_prima}}: Calcula la derivada de ( f(t) ).
\item
  \textbf{Funci√≥n \texttt{descenso\_gradiente\_raices}}: Aplica el
  descenso de gradiente para encontrar las ra√≠ces de la derivada de la
  funci√≥n.
\item
  \textbf{Par√°metros para el descenso de gradiente}: Definimos la tasa
  de aprendizaje y la tolerancia.
\item
  \textbf{Puntos iniciales}: Usamos los puntos ( t = 0 ), ( t = 1 ), y (
  t = -2 ) como puntos de partida para verificar las ra√≠ces.
\item
  \textbf{Ejecutar el descenso de gradiente}: Para cada punto inicial,
  ejecutamos el descenso de gradiente y almacenamos los resultados.
\end{enumerate}

    \subsubsection{\texorpdfstring{Con una tolerancia \(z = 10‚Åª¬π¬≤\) y un
valor inicial de \(x = 3\) aplicar su algoritmo con raz√≥n \(y = 10‚Åª¬π\),
\(10‚Åª¬≤\), \(10‚Åª¬≥\) luego hacer lo mismo con \(x = 0\). Interpretar el
resultado.}{Con una tolerancia z = 10‚Åª¬π¬≤ y un valor inicial de x = 3 aplicar su algoritmo con raz√≥n y = 10‚Åª¬π, 10‚Åª¬≤, 10‚Åª¬≥ luego hacer lo mismo con x = 0. Interpretar el resultado.}}\label{con-una-tolerancia-z-10-y-un-valor-inicial-de-x-3-aplicar-su-algoritmo-con-razuxf3n-y-10-10-10-luego-hacer-lo-mismo-con-x-0.-interpretar-el-resultado.}

    Para abordar este ejercicio, vamos a aplicar el algoritmo de descenso de
gradiente a la funci√≥n \(( f(t) = 3t^4 + 4t^3 - 12t^2 + 7 )\) utilizando
diferentes tasas de aprendizaje \((\text{learning rates})\) y dos puntos
iniciales: \(( x = 3 )\) y \(( x = 0 )\). Utilizaremos una tolerancia de
\(( z = 10^{-12} )\).

Primero, recordemos la derivada de la funci√≥n:

\([ f'(t) = 12t^3 + 12t^2 - 24t. ]\)

    \textbf{2.1.3.1 Implementaci√≥n del Algoritmo}

Vamos a implementar el descenso de gradiente para encontrar las ra√≠ces
de \(( f'(t) )\) utilizando las tasas de aprendizaje dadas. Luego,
interpretaremos los resultados para \(( x = 3 )\) y \(( x = 0 )\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} Derivada de la funci√≥n f(t)}
\PY{k}{def} \PY{n+nf}{f\PYZus{}prime}\PY{p}{(}\PY{n}{t}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3} \PY{o}{+} \PY{l+m+mi}{12}\PY{o}{*}\PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{l+m+mi}{24}\PY{o}{*}\PY{n}{t}

\PY{c+c1}{\PYZsh{} Descenso de gradiente adaptado para encontrar ra√≠ces de la derivada}
\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x0}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{gradiente} \PY{o}{=} \PY{n}{f\PYZus{}prime}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{gradiente}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergi√≥ despu√©s de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{gradiente}
        
        \PY{c+c1}{\PYZsh{} Verificaci√≥n de l√≠mites para prevenir overflow}
        \PY{k}{if} \PY{n+nb}{abs}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mf}{1e10}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El valor de x = }\PY{l+s+si}{\PYZob{}}\PY{n}{x}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ se volvi√≥ demasiado grande en la iteraci√≥n }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{k+kc}{None}
        
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcanz√≥ el m√°ximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    \textbf{2.1.3.2 Aplicaci√≥n del Algoritmo}

\begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(( x = 3 )\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-1} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}12}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{l+m+mi}{3}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{k}{if} \PY{n}{raiz} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{No se encontr√≥ una ra√≠z dentro de los l√≠mites permitidos.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 3 con tasa de aprendizaje = 0.1
El valor de x = -87049951065956.78 se volvi√≥ demasiado grande en la iteraci√≥n 2.
No se encontr√≥ una ra√≠z dentro de los l√≠mites permitidos.
    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

La tasa de aprendizaje \(( y = 10^{-1} )\) es demasiado alta. Esto hace
que las actualizaciones en \$ x \$ sean muy grandes, causando que los
valores se vuelvan extremadamente grandes en poco tiempo, lo que lleva a
un desbordamiento num√©rico. Este es un claro ejemplo de c√≥mo una tasa de
aprendizaje demasiado grande puede desestabilizar el algoritmo de
descenso de gradiente.

\begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(( x = 3 )\)
\item
  \textbf{Tasa de aprendizaje} \(y=10^{-2}\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 3 con tasa de aprendizaje = 0.01
Convergi√≥ despu√©s de 31 iteraciones.
Ra√≠z encontrada: -1.9999999999999882

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

Con una tasa de aprendizaje de \(( y = 10^{-2} )\), el algoritmo es m√°s
estable y converge r√°pidamente a una ra√≠z cercana a \(-2\). Este valor
es uno de los puntos donde la derivada de la funci√≥n original es cero,
lo que indica un m√≠nimo o m√°ximo local.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(( x = 3 )\)
\item
  \textbf{Tasa de aprendizaje} \(y=10^{-3}\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 3 con tasa de aprendizaje = 0.001
Convergi√≥ despu√©s de 831 iteraciones.
Ra√≠z encontrada: 1.0000000000000275

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

Con una tasa de aprendizaje a√∫n m√°s peque√±a, \(( y = 10^{-3} )\), el
algoritmo converge de manera m√°s lenta (requiriendo \(831\)
iteraciones). Sin embargo, alcanza un punto cercano a \(1\), que es otro
punto donde la derivada de la funci√≥n original es cero. Esto demuestra
que una tasa de aprendizaje m√°s peque√±a puede llevar a una mayor
precisi√≥n, aunque a costa de m√°s iteraciones.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(x = 0\)
\item
  \textbf{Tasa de aprendizaje} \(y = 10^{-2}\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 0 con tasa de aprendizaje = 0.1
Convergi√≥ despu√©s de 0 iteraciones.
Ra√≠z encontrada: 0

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

Al comenzar en \(( x = 0 )\), el valor inicial ya es un punto donde la
derivada de la funci√≥n es cero. No se necesitan iteraciones adicionales
porque 0 es una ra√≠z de la derivada. Esto muestra que el algoritmo
detecta correctamente que ya est√° en un punto estacionario.

    \textbf{Tasa de aprendizaje} \(y=10^{-1}\)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 0 con tasa de aprendizaje = 0.01
Convergi√≥ despu√©s de 0 iteraciones.
Ra√≠z encontrada: 0

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

Igual que con \(( y = 10^{-1} )\), el algoritmo reconoce que el punto
inicial \$x = 0 \$ ya es una ra√≠z de la derivada. No se requieren
actualizaciones adicionales.

    \textbf{Tasa de aprendizaje} \(y=10^{-3}\)

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{f\PYZus{}prime}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = 0 con tasa de aprendizaje = 0.001
Convergi√≥ despu√©s de 0 iteraciones.
Ra√≠z encontrada: 0

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

Nuevamente, al comenzar en \(x = 0\), el valor inicial ya es una ra√≠z de
la derivada. La tasa de aprendizaje no afecta el resultado en este caso
porque no se necesitan iteraciones adicionales.

    \textbf{2.1.3.3 Conclusi√≥n General}

\begin{itemize}
\tightlist
\item
  \textbf{Tasa de aprendizaje grande}: Puede llevar a desbordamientos
  num√©ricos o a oscilaciones alrededor de la ra√≠z, como se observa con
  \(( y = 10^{-1} )\) cuando el valor inicial es \(( x = 3 )\).
\item
  \textbf{Tasa de aprendizaje moderada}: Proporciona un equilibrio entre
  velocidad y estabilidad, permitiendo una convergencia r√°pida y
  precisa, como se observa con \(( y = 10^{-2} )\) cuando el valor
  inicial es \(( x = 3 )\).
\item
  \textbf{Tasa de aprendizaje peque√±a}: Garantiza una alta precisi√≥n,
  aunque a costa de un mayor n√∫mero de iteraciones, como se observa con
  \(( y = 10^{-3} )\) cuando el valor inicial es \(( x = 3 )\).
\item
  \textbf{Valor inicial en la ra√≠z}: Si el valor inicial es ya una ra√≠z
  como \(( x = 0 )\), el algoritmo converge instant√°neamente sin
  necesidad de iteraciones adicionales.
\end{itemize}

Estos resultados ilustran c√≥mo la elecci√≥n de la tasa de aprendizaje y
el punto inicial pueden influir significativamente en el comportamiento
y eficiencia del algoritmo de descenso de gradiente.

    \subsubsection{\texorpdfstring{\hyperref[toc0_]{Repetir estos dos √∫ltimos apartados con ùëì : (ùë†, ùë°) ‚Ü¶ ùë†¬≤ + 3ùë†ùë° + ùë°¬≥ + 1 y los valores iniciales x = [-1,1], [0,0].}}{}}\label{repetir-estos-dos-uxfaltimos-apartados-con-ux1d453-ux1d460-ux1d461-ux1d460-3ux1d460ux1d461-ux1d461-1-y-los-valores-iniciales-x--11-00.}

    \textbf{2.1.4.1: Calcular formalmente
\({ (s, t) ‚àà R^{2} | f(s, t) = 0 }\)}

La funci√≥n dada es: \([ f(s, t) = s^2 + 3st + t^3 + 1 ]\)

El gradiente de \(( f(s, t) )\) es:
\([ \nabla f(s, t) = \left( \frac{\partial f}{\partial s}, \frac{\partial f}{\partial t} \right) ]\)

Calculando las derivadas parciales:

\([ \frac{\partial f}{\partial s} = 2s + 3t ]\)
\([ \frac{\partial f}{\partial t} = 3s + 3t^2 ]\)

Queremos encontrar los puntos donde el gradiente es cero:

\([ 2s + 3t = 0 ]\)

\([ 3s + 3t^2 = 0 ]\)

Resolviendo estas ecuaciones simult√°neamente:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  De la primera ecuaci√≥n: \([ 2s + 3t = 0 \implies s = -\frac{3}{2}t ]\)
\item
  Sustituyendo \(( s = -\frac{3}{2}t )\) en la segunda ecuaci√≥n:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \([ 3\left(-\frac{3}{2}t\right) + 3t^2 = 0 ]\)
\item
  \([ -\frac{9}{2}t + 3t^2 = 0 ]\)
\item
  \([ t(3t - \frac{9}{2}) = 0 ]\)
\item
  \([ t = 0 \text{ o } t = \frac{3}{2} ]\)
\end{itemize}

Para \(( t = 0 )\): \([ s = 0 ]\)

Para \(( t = \frac{3}{2} )\):
\([ s = -\frac{3}{2} \left(\frac{3}{2}\right) = -\frac{9}{4} ]\)

Entonces, los puntos cr√≠ticos son: \([ (0, 0) ]\)
\([ \left(-\frac{9}{4}, \frac{3}{2}\right) ]\)

    \textbf{2.1.4.2 Aplicar el algoritmo de descenso de gradiente}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}

\PY{c+c1}{\PYZsh{} Gradiente de la funci√≥n f(s, t)}
\PY{k}{def} \PY{n+nf}{gradiente\PYZus{}f}\PY{p}{(}\PY{n}{st}\PY{p}{)}\PY{p}{:}
    \PY{n}{s}\PY{p}{,} \PY{n}{t} \PY{o}{=} \PY{n}{st}
    \PY{n}{df\PYZus{}ds} \PY{o}{=} \PY{l+m+mi}{2} \PY{o}{*} \PY{n}{s} \PY{o}{+} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{t}
    \PY{n}{df\PYZus{}dt} \PY{o}{=} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{s} \PY{o}{+} \PY{l+m+mi}{3} \PY{o}{*} \PY{n}{t}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{df\PYZus{}ds}\PY{p}{,} \PY{n}{df\PYZus{}dt}\PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Descenso de gradiente adaptado para encontrar ra√≠ces del gradiente}
\PY{k}{def} \PY{n+nf}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{grad\PYZus{}f}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{x0}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb}{float}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{grad} \PY{o}{=} \PY{n}{grad\PYZus{}f}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{grad}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n}{tolerancia}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergi√≥ despu√©s de }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ iteraciones.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{x}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{*} \PY{n}{grad}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Alcanz√≥ el m√°ximo de iteraciones (}\PY{l+s+si}{\PYZob{}}\PY{n}{max\PYZus{}iter}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{) sin convergencia.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
\end{Verbatim}
\end{tcolorbox}

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial \([-1, 1]\)}
\item
  \textbf{Tasa de aprendizaje \(( y = 10^{-1} )\)}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tolerancia} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}12}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.1
Convergi√≥ despu√©s de 302 iteraciones.
Ra√≠z encontrada: [-2.25  1.5 ]

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

La tasa de aprendizaje de \(0.1\) es moderada, permitiendo que el
algoritmo converja razonablemente r√°pido a la ra√≠z \(([-2.25, 1.5])\).
Este punto es una de las soluciones donde el gradiente de la funci√≥n es
cero. El n√∫mero de iteraciones es relativamente bajo, lo que indica una
buena convergencia con esta tasa de aprendizaje.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial \([-1, 1]\)}
\item
  \textbf{Tasa de aprendizaje \(( y = 10^{-2} )\)}
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.01
Convergi√≥ despu√©s de 3139 iteraciones.
Ra√≠z encontrada: [-2.25  1.5 ]

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

Con una tasa de aprendizaje de \(0.01\), el algoritmo converge m√°s
lentamente que con una tasa de 0.1, pero a√∫n llega a la misma ra√≠z
\(([-2.25, 1.5])\). El n√∫mero de iteraciones es significativamente mayor
debido a la menor tasa de aprendizaje, lo que resulta en pasos m√°s
peque√±os hacia la convergencia.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial \([-1, 1]\)}
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-3} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [-1, 1] con tasa de aprendizaje = 0.001
Convergi√≥ despu√©s de 31558 iteraciones.
Ra√≠z encontrada: [-2.25  1.5 ]

    \end{Verbatim}

    \textbf{Interpretaci√≥n}: Con una tasa de aprendizaje a√∫n m√°s peque√±a de
\(0.001\), el algoritmo requiere muchas m√°s iteraciones (\(31,559\))
para converger a la misma ra√≠z \(([-2.25, 1.5])\). Esto demuestra que
una tasa de aprendizaje muy baja resulta en una convergencia muy lenta,
aunque sigue siendo precisa.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(([0, 0])\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-1} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{punto\PYZus{}inicial} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [0, 0] con tasa de aprendizaje = 0.1
Convergi√≥ despu√©s de 0 iteraciones.
Ra√≠z encontrada: [0. 0.]

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

El valor inicial \(([0, 0])\) ya es un punto donde el gradiente de la
funci√≥n es cero. Por lo tanto, el algoritmo no necesita realizar ninguna
iteraci√≥n adicional para encontrar la ra√≠z. Esto muestra que el punto
inicial ya es una soluci√≥n, independientemente de la tasa de
aprendizaje.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(([0, 0])\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-2} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.01}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [0, 0] con tasa de aprendizaje = 0.01
Convergi√≥ despu√©s de 0 iteraciones.
Ra√≠z encontrada: [0. 0.]

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

Igual que con la tasa de \(0.1\), el algoritmo reconoce que el punto
inicial \([0, 0])\) ya es una ra√≠z de la funci√≥n, por lo que no se
requieren iteraciones adicionales.

    \begin{itemize}
\tightlist
\item
  \textbf{Valor inicial} \(([0, 0])\)
\item
  \textbf{Tasa de aprendizaje} \(( y = 10^{-3} )\)
\end{itemize}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Par√°metros}
\PY{n}{tasa\PYZus{}aprendizaje} \PY{o}{=} \PY{l+m+mf}{0.001}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Comenzando en x = }\PY{l+s+si}{\PYZob{}}\PY{n}{punto\PYZus{}inicial}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ con tasa de aprendizaje = }\PY{l+s+si}{\PYZob{}}\PY{n}{tasa\PYZus{}aprendizaje}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{raiz} \PY{o}{=} \PY{n}{descenso\PYZus{}gradiente\PYZus{}raices}\PY{p}{(}\PY{n}{gradiente\PYZus{}f}\PY{p}{,} \PY{n}{punto\PYZus{}inicial}\PY{p}{,} \PY{n}{tasa\PYZus{}aprendizaje}\PY{p}{,} \PY{n}{tolerancia}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Ra√≠z encontrada: }\PY{l+s+si}{\PYZob{}}\PY{n}{raiz}\PY{l+s+si}{\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Comenzando en x = [0, 0] con tasa de aprendizaje = 0.001
Convergi√≥ despu√©s de 0 iteraciones.
Ra√≠z encontrada: [0. 0.]

    \end{Verbatim}

    \textbf{Interpretaci√≥n}:

Al igual que con las tasas de aprendizaje mayores, el punto inicial
\(([0, 0])\) ya es una ra√≠z, y el algoritmo no necesita realizar
iteraciones adicionales.

    \textbf{2.1.4.3 Conclusi√≥n General}

\begin{itemize}
\item
  \textbf{Tasa de aprendizaje y valor inicial \([-1, 1]\)}:
\item
  Una tasa de aprendizaje mayor (0.1) permite una convergencia m√°s
  r√°pida con menos iteraciones.
\item
  Una tasa de aprendizaje moderada (0.01) resulta en una convergencia
  m√°s lenta pero estable.
\item
  Una tasa de aprendizaje muy peque√±a (0.001) lleva a una convergencia
  extremadamente lenta, aunque precisa.
\item
  Todos convergen a la misma ra√≠z ({[}-2.25, 1.5{]}), que es un punto
  cr√≠tico de la funci√≥n.
\item
  \textbf{Tasa de aprendizaje y valor inicial \([0, 0]\)}:
\item
  El valor inicial ({[}0, 0{]}) ya es un punto cr√≠tico donde el
  gradiente es cero.
\item
  Independientemente de la tasa de aprendizaje, el algoritmo reconoce
  inmediatamente que est√° en la ra√≠z y no realiza iteraciones
  adicionales.
\item
  Esto muestra que cuando el punto inicial es ya un punto cr√≠tico, la
  tasa de aprendizaje no influye en el resultado.
\end{itemize}

Estos resultados ilustran c√≥mo la elecci√≥n de la tasa de aprendizaje
afecta la velocidad de convergencia del algoritmo de descenso de
gradiente y c√≥mo los puntos cr√≠ticos iniciales pueden simplificar la
convergencia.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
